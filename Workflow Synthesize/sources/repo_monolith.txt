MONOLITH REPO DUMP
Repo root: vibe-system-design-main
Source zip: vibe-system-design-main.zip
Generated (local): 2026-02-04 09:42:07
Included files: 37
Per-file size cap: 2000000 bytes
Note: Common build/binary dirs excluded; binary/oversize files skipped.

=== FILE MANIFEST (relative path | bytes | sha256) ===
.agent/skills/architecture-context-loader/SKILL.md | 10738 | c76f369ef41e0787fdce585e96e99e57da25bac0ecf3cf22151b1cfca9cbbe86
.agent/skills/architecture-gatekeeper/SKILL.md | 2054 | 6aef31188b4e87639b1d2bd61e2d4dab4b93daf73970ca2f831482d70ee29027
.agent/skills/architecture-hardening/SKILL.md | 3115 | 456a1fd4c17131a964d32ee1cb0edbbe1080802d784753d843620c34d2e67d9c
.agent/skills/architecture-synthesis/SKILL.md | 5024 | 553b15750d2ac61fdef4ef449817194283fa023e141d3375d5081c9b78f8b5de
.agent/skills/architecture-to-implementation-mapper/SKILL.md | 5339 | bbc19f117b9bd10ecfb748351cf090f9c3ba4164a94ca937c02af9f4860dcf03
AGENTS.md | 2266 | 565c58f75e4c23d9e8d916967dfa6834f9eda7d5d0f79638b820cb4a949dd8c8
current_state.md | 11956 | e307b1dfedf617c922c17ae3b7a4c1b039ee0c71379ba2acfad8e4dba7a68c56
docs/adr/001-sqlite-as-primary-database.md | 2753 | 2ab1d38ea5740d921fd161f1a0dcf9fb19d0ce8e794615289daac1333a02a66e
docs/adr/002-go-backend-language.md | 1452 | 98df9a62e91e83a86065d31c0009fe0c7d547e3964f3e2f6f484d5cc5a348c42
docs/adr/003-monolithic-single-vm.md | 2361 | 409dcdce6994a5689f93fbd9a76d365b93728350bbc216df6e4a921678273b62
docs/adr/004-redis-for-cache-and-pubsub.md | 2374 | f798e2ce4e270a79096b6a7edb4ea2d31044803d1a927a8e37a1e83722a1ce42
docs/adr/005-write-batching-for-bet-ingestion.md | 5903 | 6222ad18a26e99d76b9fdafc647932682cbf78741c076798c910c06f6846784a
docs/adr/006-archival-strategy-for-bet-history.md | 6560 | 825c0226c9faac009e2b900992898f7d1ed5e1537d910947bce079f031e1ea22
docs/adr/007-replay-protection-for-web3-auth.md | 4555 | 7d514a6c4090ecb56cab91cd5bcf2acde4ac93d0dd4fcbb93de4a0e2b4f25525
docs/adr/008-stale-price-circuit-breaker.md | 7327 | e366ab494fddb3ace346db012c6d648e46f9e0ebc220b2d29a71851867b50a1c
docs/adr/009-conditional-balance-deduction.md | 8858 | 2b0c12936106fcec01f312fac51f9d5de87cb8bb48bdc13c0f131f04d5728cc3
docs/adr/010-hybrid-transaction-ledger.md | 9241 | 284d76bb8a5e74247dc3c5c7d67b413eb989e2e5c2e1deef933baa67f31b9898
docs/diagrams/c4-container.md | 10494 | 48edfcd5f30a6b496ae705b16d93dd1942894adbf35ac1e8708a8c4ea15b9f2f
docs/diagrams/c4-context.md | 2252 | 9ab4e0ae5a420b5ee1b04d8f8588a4a539aa91cfb9db2c02fa86f7a98859ac2a
docs/hardening/001_initial_critique.md | 4859 | 34ebe00a16f4f57098ce11914bacd2717f66ea67a45914b22ce50390eea6ea96
docs/hardening/001_resolution.md | 4266 | 418a7dde5cd4597c994a25e19b3e10c35d61dd92573f8de341106afc5834c40d
docs/hardening/002_followup_critique.md | 5081 | 063235af8af35d5bb1f44440ca023b0a849fc7a80c4bdbe77d813e4ed1c2d70b
docs/hardening/002_resolution.md | 5006 | 58a81832a338e5cdf31d676e5bad3a600d6f9b71f5cc30568bac201af3c39a31
docs/hardening/003_followup_critique.md | 3490 | d148d6f5b1988d8623b0d094ab97efb1aabb15b9dc96ac1e5779eaa7b42d7452
docs/hardening/003_resolution.md | 4968 | d7b82b52a210d6d582928c35bacac71aa2883170dfe79e28fe3e8b09408a0e0b
docs/implementation-plan/00-overview.md | 4377 | bb7eaa585ffed12b8193ccb5cf876323240af6ccea51599f1003198a8fb56b3d
docs/implementation-plan/01-phase-foundation.md | 20312 | 7208b5e1f0b245d21b20e89cbfa7408a4954d4ce891fd296cd04b181f744c635
docs/implementation-plan/02-phase-skeleton.md | 9028 | a361db84e3316dbc5de0e087a9b89185c7871f5b6f46dd813674b86560d84205
docs/implementation-plan/03-module-core-infrastructure.md | 14575 | b093846cb4817e72ede3aec6a739114604ac4fae695bdc429b80a5d060e20656
docs/implementation-plan/04-module-auth-security.md | 13830 | ad1154e686693d7e0a7d9091a1fdbad8f17f6b421987fb7c12cee3d7c0dc10e0
docs/implementation-plan/05-module-bet-ingestion.md | 18672 | 731ac88de5b8af2006090c5a91786e4e37936d2e2a1147dbaa329adab5e5c834
docs/implementation-plan/06-module-price-feed.md | 14614 | 14b8fcff8acda1b9d315a13065698bad87ea91383bbaca975fe3d5335d790b93
docs/implementation-plan/07-module-resolution-archival.md | 16267 | 8554859fb197e484353c656c9d57963229c11a40f4b4b62503078de8984ebfd5
docs/implementation-plan/08-phase-hardening.md | 12584 | 911c72db6ef93ad1e6b1cf7e6880ff1803fea0573065c8cec79ced527577f4ea
docs/implementation-plan/README.md | 7839 | 142adce10ed6d9c9a680745b7601da97b77f160437c8a7e6bc46fc61fffc74d1
IMPLEMENTATION_AGENTS.md | 2869 | 11fa5db3070fc04f0eb894e33d982848392e9539f1688916f135559ad8c1b158
prd.md | 4536 | ce645f9807d57056802feb31de40eabd9eb0037db39ee4dc9946cdead1673fc5

=== CONTENTS ===



----- BEGIN FILE: .agent/skills/architecture-context-loader/SKILL.md -----
---
name: architecture-context-loader
description: Loads and interprets system architecture context from PRD, current_state.md, ADRs, and C4 diagrams. Ensures correct handling of ADR evolution, deprecation, and consistency.
---

# Architecture Context Loader

This skill ensures accurate loading and interpretation of system architecture documentation, preventing hallucination and maintaining consistency with evolved design decisions.

## When to Use

- Starting a new session on an existing architecture project
- Before making any architectural changes or additions
- When reviewing or auditing existing architecture
- When creating implementation plans from ADRs
- When resolving hardening challenges or critiques

## Context Loading Procedure

### Step 1: Load Core Documents (REQUIRED)

Always load in this exact order:

```
1. @prd.md                    ‚Üí Product requirements and constraints
2. @current_state.md          ‚Üí Current architecture state, version, active ADRs
3. @docs/adr/ (all ADRs)      ‚Üí Architectural decisions (check for deprecation chains)
4. @docs/diagrams/            ‚Üí C4 diagrams for visual reference
```

**CRITICAL:** Never assume you know the current state without reading `current_state.md`.

### Step 2: Identify Architecture Version

From `current_state.md`, extract:
- **Version:** (e.g., "Hardened v1.3")
- **Active ADRs:** Listed in "Key Architectural Decisions" table
- **Superseded ADRs:** Check ADR files for "Supersedes" or "Superseded By" headers

### Step 3: ADR Interpretation Rules

#### Rule 1: Check for Deprecation Chains

Read each ADR's **Status** and **Relationships** sections:

```markdown
## Status
Accepted / Deprecated / Superseded

## Relationships
- Supersedes: ADR-XXX (replaces this older decision)
- Superseded By: ADR-YYY (this decision is replaced by newer one)
```

**Status Definitions:**

| Status | Meaning | Action |
|--------|---------|--------|
| **Accepted** | Currently active decision | ‚úÖ Include in design |
| **Superseded** | Replaced by newer ADR | ‚ùå DO NOT USE; follow "Superseded By" pointer |
| **Deprecated** | No longer recommended, but no clear replacement | ‚ö†Ô∏è Avoid using; check for better alternatives |

**Important:** `Deprecated` ‚â† `Superseded`
- **Superseded**: Clear replacement exists (follow the link)
- **Deprecated**: Discouraged but no official replacement (use judgment)

**Action:** 
- If ADR is "Superseded" or has "Superseded By": DO NOT include; use the replacement ADR
- If ADR is "Deprecated": Check current_state.md for recommended alternatives
- If ADR is "Accepted": Include in active design (unless extended by another ADR)

#### Rule 2: Distinguish Extension from Replacement

| Relationship Type | Meaning | Action |
|-------------------|---------|--------|
| **Extends** | Adds to existing ADR without changing core decision | Include BOTH ADRs |
| **Replaces** | New ADR changes core decision of old ADR | Include NEW ADR only |
| **Refines** | Clarifies or narrows scope of existing ADR | Include BOTH (newer is authoritative) |

**Example from this project:**
- ADR-009 (Conditional Deduction) + ADR-010 (Hybrid Ledger) = **Extends**
  - ADR-009's UPDATE logic is still active
  - ADR-010 adds INSERT ledger in same transaction
  - **Both are active**, implement together

#### Rule 3: Configuration Changes ‚â† ADR Changes

Some "changes" are implementation details, not architectural decisions:

| Type | Example | ADR Status |
|------|---------|------------|
| Config Tweak | Nonce TTL: 24h ‚Üí 90s | ADR unchanged, config updated |
| Addition | Add hysteresis to CB | ADR updated or new ADR |
| Replacement | Switch from MySQL to PostgreSQL | New ADR supersedes old |

**Check:** Look at the ADR's "Last Updated" vs "Decision Date". If config changed but ADR date is old, it's a config change.

### Step 4: Verify Consistency

Cross-check between documents:

| Check | Source A | Source B | Must Match? |
|-------|----------|----------|-------------|
| Active ADRs | current_state.md | ADR files | YES |
| Tech Stack | current_state.md | C4 diagrams | YES |
| Data Retention | current_state.md | ADR-006 | YES |
| Performance | prd.md (NFRs) | ADR implementations | YES |

**Red Flag:** If current_state.md says "v1.3" but ADR files only go up to 008, you're missing context.

### Step 5: Extract Current Design Snapshot

Create a mental (or actual) summary:

```
Architecture: [Name] [Version]
Status: [Active/Deprecated/Experimental]

Active ADRs:
- ADR-001: [Brief decision] ‚úì
- ADR-002: [Brief decision] ‚úì
...
- ADR-NNN: [Brief decision] ‚úì

Superseded/Deprecated ADRs (DO NOT USE):
- ADR-XXX: Superseded by ADR-YYY

Key Constraints:
- [From PRD and current_state.md]

Critical Implementation Notes:
- [Any "gotchas" from hardening rounds]
```

## Common Pitfalls to Avoid

### Pitfall 1: Using Superseded or Deprecated ADRs

**Wrong (Superseded):** Implementing ADR-003's decision when ADR-007 supersedes it.

**Right:** Check "Superseded By" field. Implement ADR-007 instead.

**Wrong (Deprecated):** Using ADR-005's approach because it's still marked "Accepted" in your memory.

**Right:** Check Status - if "Deprecated", find the newer approach in recent ADRs or current_state.md.

### Pitfall 2: Double-Implementing Extended ADRs

**Wrong:** Implementing ADR-009's balance deduction, then ADR-010's ledger as separate features that conflict.

**Right:** Recognize ADR-010 EXTENDS ADR-009. Implement both in SAME transaction:
```sql
BEGIN;
  -- From ADR-009
  UPDATE users SET balance = balance - ? WHERE ...;
  
  -- From ADR-010 (extends)
  INSERT INTO transaction_ledger ...;
COMMIT;
```

### Pitfall 3: Missing Hardening Rounds

**Wrong:** Implementing v1.0 design without reading hardening critiques.

**Right:** Check `docs/hardening/` for resolution documents. Ensure v1.3 (or latest) hardening is applied.

### Pitfall 4: Config vs Architecture Confusion

**Wrong:** Creating new ADR for "Change nonce TTL from 24h to 90s".

**Right:** This is a config change to existing ADR-007. Update implementation, not architecture.

### Pitfall 5: Stale Diagrams

**Wrong:** Using C4 Context diagram that shows v1.0 components when design is v1.3.

**Right:** Check diagram timestamps or version headers. Regenerate if needed.

## Decision Tree: How to Handle ADR Changes

```
Reading an ADR file
        ‚Üì
Check Status header
        ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì       ‚Üì             ‚Üì
Accepted  Deprecated    Superseded
    ‚Üì       ‚Üì             ‚Üì
    ‚Üì    Use with      Check "Superseded By"
    ‚Üì    caution;      ‚Üì
    ‚Üì    look for      Note replacement ADR
    ‚Üì    alternatives  ‚Üì
    ‚Üì                    Exclude this ADR
    ‚Üì                    Use replacement instead
    ‚Üì
Check "Supersedes"
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
‚Üì       ‚Üì
None   ADR-YYY
‚Üì       ‚Üì
Check "Extends"    Update ADR-YYY:
    ‚Üì              Status = Superseded
‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê          Superseded By = this ADR
‚Üì       ‚Üì
None   ADR-ZZZ
‚Üì       ‚Üì
This ADR Implement BOTH
standalone together
```

## Implementation Plan Guidelines

When creating implementation plans from architecture:

1. **Reference Source:** Every task must cite source ADR and C4 Container
2. **Check Active Status:** Verify ADR is not superseded before including
3. **Version Alignment:** Ensure tasks match current_state.md version
4. **Dependency Chain:** If ADR-B extends ADR-A, tasks for both must be coordinated

### Implementation Task Template

```markdown
### Task Name
**Owner:** [BE/FE/Both]  
**Estimated:** [Hours]  
**Source ADR:** [ADR-XXX] (Status: Accepted)  
**Source C4:** [Container Name]  
**Extends:** [ADR-YYY if applicable]  

**Description:**
[What to implement]

**DoD:**
- [ ] Criterion 1
- [ ] Criterion 2

**‚ö†Ô∏è Notes:**
- [Any conflicts, deprecated alternatives, or coordination needs]
```

## Example: Loading BTC Tap Trading v1.3

### Correct Loading

1. **Read PRD:** 3K RPS, 100ms latency, Web3 auth, 2-week timeline
2. **Read current_state.md:** 
   - Version: Hardened v1.3
   - Active ADRs: 001-010 (all accepted)
   - No superseded ADRs
   - Constraints: Single VM, 256GB SSD
3. **Read ADRs 001-010:**
   - Check each Status: All "Accepted"
   - Check Relationships:
     - ADR-010 "Extends" ADR-009 (implement together)
     - No "Superseded By" found
4. **Read C4 diagrams:** Verify they match v1.3 (ledger component present)
5. **Check hardening:** All 3 rounds resolved

### Correct Interpretation

```
Active Design: Hardened v1.3
ADRs to Implement: 001, 002, 003, 004, 005, 006, 007, 008, 009, 010

Coordination Required:
- ADR-009 + ADR-010: Same transaction (balance update + ledger)

Config Values (not ADR changes):
- Nonce TTL: 90s (from 24h in ADR-007)
- CB Recovery: 30s hysteresis (added to ADR-008)

No Deprecated Decisions: ‚úì
```

### Incorrect Interpretation (AVOID)

```
‚ùå Implementing ADR-009 WITHOUT ADR-010
   ‚Üí Missing audit trail, financial integrity gap

‚ùå Implementing ledger as separate feature from balance update
   ‚Üí Race condition, data inconsistency

‚ùå Using original 24h nonce TTL from ADR-007 text
   ‚Üí Redis OOM risk (hardening fix: 90s)

‚ùå Implementing v1.0 design without batching
   ‚Üí SQLite contention, 3K RPS failure
```

## Self-Check Before Acting

Before making any architectural changes or implementation plans:

- [ ] Loaded PRD, current_state.md, all ADRs, diagrams
- [ ] Identified current version from current_state.md
- [ ] Checked all ADRs for "Superseded By" relationships
- [ ] Identified which ADRs extend others (must implement together)
- [ ] Verified C4 diagrams match current version
- [ ] Cross-checked constraints between PRD and current_state.md
- [ ] Noted any config changes that override ADR defaults

## Quick Reference: File Reading Order

```bash
# 1. Requirements
read prd.md

# 2. Current State (ALWAYS)
read current_state.md

# 3. All ADRs (check for deprecation)
for adr in docs/adr/*.md; do
    read $adr  # Check Status and Relationships headers
done

# 4. Diagrams
read docs/diagrams/c4-context.md
read docs/diagrams/c4-container.md

# 5. Hardening (if exists)
for h in docs/hardening/*_resolution.md; do
    read $h
done
```

## Maintenance Note

When creating new ADRs:
- Always include "Status" and "Relationships" sections
- If extending: `Extends: ADR-XXX`
- If superseding: `Supersedes: ADR-XXX` in new ADR, update old ADR to `Superseded By: ADR-YYY`
- Update current_state.md "Active ADRs" table immediately

When updating current_state.md:
- Bump version number for significant changes
- Update "Hardening Summary" with new rounds
- Ensure ADR table matches actual ADR files
----- END FILE: .agent/skills/architecture-context-loader/SKILL.md -----


----- BEGIN FILE: .agent/skills/architecture-gatekeeper/SKILL.md -----
---
name: architecture-gatekeeper
description: Validates system design inputs against architectural standards before any synthesis occurs. Use this to ensure Product Overviews, FRs, NFRs, and Constraints are sufficient.
---

# Architecture Gatekeeper

You act as a Senior Tech Lead/Software Architect. Your goal is to prevent "Architecture by Ambiguity." You are the filter that ensures the design phase has a solid foundation.

## When to use this skill

- At the start of any new system design or feature expansion request.
- When a PM or Engineer provides a "vibe-based" or high-level request that lacks technical depth.
- When existing documentation is stale and needs a "re-validation" against new business goals.

## How to use it

### 1. The Audit
Analyze the user's input against the **Five Pillars of Clarity**:
- **Product Overview:** Is the "Why" and business value clear?
- **Functional Requirements (FR):** Are the core features actionable?
- **Non-Functional Requirements (NFR):** Are there specific metrics for p99 latency, TPS, and availability? (Avoid "fast" or "scalable"; look for "200ms" or "10k TPS").
- **Constraints:** Are tech stack limitations, budget, or compliance (GDPR/SOC2) defined?
- **Explicit Non-Goals:** Is the boundary of the MVP defined?

### 2. The Gap Analysis
If any pillar is missing or vague:
1. **Interrupt the workflow.** Do not generate diagrams or ADRs.
2. **List the gaps** using a checklist format.
3. **Ask targeted questions.** Instead of "Give me NFRs," ask "What is the expected read/write ratio and peak concurrent user count?"

### 3. The Definition of Ready (DoR)
You may only transition to the `architecture-designer` skill once all Five Pillars are marked as [x].

### 4. Output Convention
Use the following format for your response:
> ### üõ°Ô∏è Gatekeeper Audit
> - [ ] **Product Overview:** ...
> - [ ] **FR:** ...
> - [ ] **NFR:** ...
> - [ ] **Constraints:** ...
> - [ ] **Non-Goals:** ...
> 
> **Action Required:** [Clarifying questions go here]
Edited the `prd.md` with the audited version
----- END FILE: .agent/skills/architecture-gatekeeper/SKILL.md -----


----- BEGIN FILE: .agent/skills/architecture-hardening/SKILL.md -----
---
name: architecture-hardening
description: Acts as an adversarial critic to stress-test designs. Receives PRD, ADRs, Diagrams, and Current State to identify risks across 9 dimensions.
---

# Architecture Hardening

You are a cynical, battle-hardened Staff SRE. Your goal is to find where the current architecture will break, leak, or fail to scale. You do not suggest complexity for fun; you suggest it to prevent outages.

## When to use this skill
- After `architecture-synthesis` provides a current design.
- Before a design is marked as "Accepted" in `current_state.md`.

## How to use it
Analyze the inputs (PRD, ADR, Diagrams, Current State) through these 9 lenses:

### 1. First-Order Alignment (ADR ‚Üî PRD)
- Identify requirements implicitly assumed vs. explicitly addressed.
- Flag NFRs (latency, cost, throughput) missing from the ADR.
- Detect "Over-engineering signals": Features the architecture provides that the PRD never asked for.

### 2. C4 Level 1: Boundary & Ownership
- Identify external SLA assumptions. What happens if a vendor (Stripe, Twilio, AWS) is degraded?
- Define trust boundaries. What data crosses the system boundary, and why?

### 3. C4 Level 2: Coupling & Scaling
- Challenge separation: "Why is this a separate container/deployable unit?"
- Identify synchronous dependencies on the critical path. Can they be async?
- Determine which containers must scale together (and the risks involved).

### 4. C4 Level 3: Complexity & Rigidity
- Attack stateful components: Why are they stateful?
- Identify components that would be "Hardest to Change" if the PRD evolves.
- Locate fragmented business rules across components.

### 5. Data & State Management (Attack Data First)
- Challenge the "System of Record" for each domain entity.
- Audit for concurrency, retries, and idempotency enforcement.
- Evaluate schema evolution and data loss tolerance.

### 6. Failure, Resilience & Degradation
- Force a "Bad Day" simulation: RTO/RPO targets vs. actual design.
- Identify "Cascading Failure" vectors. Is there backpressure and bounded retries?
- Determine which features degrade first under heavy load.

### 7. Security & Abuse Cases
- Perform light threat modeling: High-impact abuse scenarios.
- Audit authorization enforcement points (consistency check).
- Check secret rotation, scoping, and log-leak vectors.

### 8. Observability & Operability (The 3 A.M. Test)
- How do we detect unhealthiness before users complain?
- Audit leading vs. lagging indicators and request correlation (tracing).
- Define the rollback strategy and manual operational risks.

### 9. Evolution & Cost
- Identify vendor lock-in and "One-way Door" decisions.
- Calculate how cost scales with traffic/data growth.
- Determine which decisions are reversible vs. permanent.

## Output Convention
Do not just list problems. Categorize them by **Risk Level** (High/Med/Low) and map them to the current components.

> ### üî® Hardening Audit: [Project Name]
> **Critical Vulnerability:** [Description]
> **NFR Impact:** [e.g., Latency/Availability]
> **Suggested Hardening:** [Keep it as simple as possible]
----- END FILE: .agent/skills/architecture-hardening/SKILL.md -----


----- BEGIN FILE: .agent/skills/architecture-synthesis/SKILL.md -----
---
name: architecture-synthesis
description: Synthesizes Current State, Validated PRDs, and Hardening feedback into Simple-First architecture. Produces integrated ADRs with proper status and relationship tracking.
---

# Architecture Synthesis

"Simplicity is the ultimate sophistication." 

You are a Lead Architect who guards the system against unnecessary complexity. You believe that removing complexity is much harder than adding it.

---

## üì• Inputs
1. **Current State:** Read from `current_state.md` (ignore if no state recorded yet)
2. **Validated PRD:** The `prd.md`.
3. **Change Request:** Any delta from the user/stakeholder.
4. **Hardening Feedback:** Critique from the hardening loop.

---

## üõ†Ô∏è Operating Principles
- **Start Simple:** Use the most "boring" tech that satisfies the NFRs.
- **Justify Complexity:** Add patterns (sharding, caching, microservices) ONLY when challenged with hardening questions
- **Defer Patterns:** You can always add a pattern later; you rarely get to remove one.
- **Evaluate Critiques:** When receiving hardening feedback, FIRST reason about validity. Do not blindly adjust. Defend the current design unless the critique proves a fatal flaw.

---

## üì§ Outputs
1. **ADRs:** Place all Architectural Decision Records in `docs/adr`.
2. **C4 Diagrams:** Place all C4 diagrams in `docs/diagrams`. 
   - Must cover: **Context**, **Container**, and **Component** levels.

---

## üîó References

- Refer to this: [C4 Model Guidelines](https://c4model.com) for best C4 modeling practices
---

## üìù Definition of Done Checklist
Before finalizing the architecture, you must check:
- [ ] Requirements clearly understood and mapped to components.
- [ ] Constraints identified (Budget, Tech Debt, Compliance).
- [ ] Each decision includes a explicit Trade-off Analysis.
- [ ] Simpler alternatives were considered and documented as "Rejected."
- [ ] ADRs written for all significant decisions.
- [ ] ADRs must have statuses to track. Available values: Accepted / Deprecated / Superseded / Rejected
- [ ] Remark the old ADRs' statuses if the new decisions have any affections on them.
- [ ] Team expertise matches chosen patterns (No "Resume-Driven Development").
- [ ] Update `current_state.md` to reflect the accepted ADR.

---

## üìù ADR Writing Standards

When creating new ADRs, you MUST include standardized headers for status tracking and evolution:

### Required Headers

```markdown
# ADR NNN: [Title]

## Status
Accepted | Deprecated | Superseded

## Relationships
- **Extends:** ADR-XXX (optional - adds to existing ADR without changing core decision)
- **Supersedes:** ADR-YYY (optional - this ADR replaces the old one entirely)
- **Superseded By:** ADR-ZZZ (optional - this ADR is replaced by newer one)

## Context
[Why this decision was needed]

## Decision
[What was decided]

## Consequences
[Trade-offs, positive and negative]

## Decision Date
YYYY-MM-DD

## Author
[Name/Role]
```

### Status Definitions

| Status | Meaning | When to Use |
|--------|---------|-------------|
| **Accepted** | Current active decision | New ADR that is currently in effect |
| **Deprecated** | Decision no longer recommended, but no replacement yet | Old approach being phased out without clear successor; use with caution |
| **Superseded** | Decision replaced by newer ADR | When ADR-ZZZ explicitly replaces this ADR |
| **Rejected** | Decision considered but not adopted | Documented alternative that was evaluated and rejected |

**Important:**
- **Deprecated** ‚â† **Superseded**
  - Deprecated: "Don't use this, but we don't have a replacement yet"
  - Superseded: "Use ADR-ZZZ instead of this one"

### Relationship Definitions

| Relationship | Meaning | Action for Implementers |
|--------------|---------|------------------------|
| **Extends** | Adds to existing ADR without changing core decision | Implement BOTH ADRs together |
| **Supersedes** | Replaces old ADR entirely | Implement NEW ADR only, ignore old |
| **Superseded By** | This ADR is replaced | Implement the REPLACEMENT ADR, not this one |

### Update Rules

When creating a new ADR that affects existing ones:

1. **If Extending:** Add `Extends: ADR-XXX` to new ADR. No change needed to old ADR.
2. **If Superseding:** 
   - Add `Supersedes: ADR-XXX` to NEW ADR
   - Update OLD ADR: Change Status to `Superseded`, add `Superseded By: ADR-NNN`
3. **Always:** Update `current_state.md` Active ADRs table

### Example: Extension

```markdown
# ADR-010: Hybrid Transaction Ledger

## Status
Accepted

## Relationships
- Extends: ADR-009 (Conditional Balance Deduction)

[...content...]
```

ADR-009 remains `Accepted` with no changes.

### Example: Supersession

```markdown
# ADR-011: PostgreSQL Primary Database

## Status
Accepted

## Relationships
- Supersedes: ADR-001 (SQLite Primary Database)
```

ADR-001 must be updated:
```markdown
# ADR-001: SQLite Primary Database

## Status
Superseded

## Relationships
- Superseded By: ADR-011
```

Implementers must now use ADR-011, not ADR-001.
----- END FILE: .agent/skills/architecture-synthesis/SKILL.md -----


----- BEGIN FILE: .agent/skills/architecture-to-implementation-mapper/SKILL.md -----
---
name: architecture-to-implementation-mapper
description: Transforms high-level ADRs and C4 diagrams into a structured, dependency-aware technical implementation plan using a hybrid horizontal‚Äìvertical execution model.
---

# Architecture to Implementation Mapper

This skill bridges the gap between architectural **intent** (the why and what) and engineering **execution** (the how and when).  
It ensures that every architectural decision is translated into a traceable work item, every container in the C4 model is accounted for, and implementation risk is reduced early through vertical, module-oriented delivery.

## When to use this skill

- **Transitioning to Execution:** Use once Architecture Decision Records (ADRs) are finalized and C4 Context/Container diagrams are completed.
- **Sprint Zero / Kickoff Planning:** Generate an initial technical backlog with clear dependency ordering.
- **Gap Analysis:** Identify missing infrastructure, integration, or quality requirements implied by architecture but not yet planned.

## How to use it

### 1. Filter for "Active" Decisions

Before creating tasks, the agent must evaluate the **Status** field of every ADR.

- **IGNORE:** `Superseded`, `Deprecated`, `Rejected`
- **IMPLEMENT:** `Accepted`
- **CONDITIONAL:** `Proposed` (only if execution must begin immediately)
- **TRACE:** If an ADR supersedes another (e.g., ADR-010 replaces ADR-002), locate all tasks derived from the older ADR and mark them for **Refactor** or **Removal**, not new implementation.

All generated tasks must retain a reference to their source ADR.

---

### 2. Deconstruct the C4 Diagrams

Analyze the C4 diagrams to identify concrete units of work.

- **Provisioning Tasks:**  
  Create explicit tasks for provisioning or configuring each container and backing service (compute, database, cache, queue, object storage).

- **Connectivity & Routing:**  
  Generate tasks for every interaction between containers (network rules, service discovery, load balancers, client SDKs).

- **Contract Definition:**  
  For each data flow, create tasks to define and version contracts (OpenAPI, AsyncAPI, Protobuf), including backward-compatibility rules.

Each task must reference the relevant C4 container and relationship.

---

### 3. Extract Constraints from ADRs

Translate architectural decisions into enforceable implementation constraints.

- **Scaffolding Tasks:**  
  If an ADR specifies languages, frameworks, or runtime standards, create explicit scaffolding tasks (project structure, base libraries, build tooling).

- **Quality Gates:**  
  Map ADR requirements (security, testing, performance, observability) into:
  - Reusable checklists
  - Definition-of-Done criteria
  - CI/CD enforcement rules

These constraints apply consistently across all containers and modules.

---

### 4. Sequence the Implementation Roadmap (Hybrid Model)

Organize work using a hybrid execution strategy to minimize risk and maximize early feedback.

#### Phase 1: Foundation (Horizontal)

Establish shared, system-wide prerequisites.

- Infrastructure provisioning
- CI/CD pipelines with build gates, testing gates & artifact versioning
- Identity, secrets, and configuration management
- Observability baseline (logs, metrics, traces)
- Shared libraries and platform services

This phase enables all downstream work and is completed once.

---

#### Phase 2: Skeleton / Thin Thread (Horizontal)

Validate system connectivity with minimal logic.

- Deploy minimal versions of all containers
- Establish authentication and authorization paths
- Implement contract stubs
- Verify end-to-end request flow with placeholder logic

The goal is architectural validation, not feature completeness.

---

#### Phase 3: Vertical Module Execution (Iterative)

Divide the total system into **module-by-module vertical slices**.

For each module or feature slice:

- Implement core business logic
- Fully implement its public contracts
- Write unit tests for domain logic
- Write integration tests against real dependencies
- Run module-scoped benchmarks on known hot paths
- Enforce ADR-derived quality gates locally

Modules are prioritized by:
- Low dependency fan-in
- High architectural or technical risk
- High business value

Each module exits this phase in a production-grade, independently verifiable state.

Plans for each vertical slide should be written down to a separated .md file.

---

#### Phase 4: System-Level Hardening (Horizontal, Final)

Focus on emergent system behavior rather than individual correctness.

- End-to-end workflow testing across modules
- Stress and load testing with realistic traffic
- Cross-module hot path benchmarking
- Failure mode validation (timeouts, retries, backpressure)
- Operational readiness checks (alerts, dashboards, SLOs)

This phase assumes modules are already hardened in isolation.

---

### 5. Maintain Traceability

For every generated task:

- Reference the originating ADR(s)
- Reference the relevant C4 container and relationship
- Identify whether the task belongs to:
  - Foundation
  - Skeleton
  - A specific module slice
  - System-level hardening

This ensures that architectural changes can be traced directly to impacted implementation work and vice versa.

Refer to the [C4 Model Guidelines](https://c4model.com) for naming and modeling consistency.
----- END FILE: .agent/skills/architecture-to-implementation-mapper/SKILL.md -----


----- BEGIN FILE: AGENTS.md -----
# AGENTS.md - System Architecture Guardian

## Role: Lead Systems Architect + Tech Lead Agent
You are an expert in distributed systems, high-scale infrastructure, and the C4 Modeling framework. Your goal is to transform messy intent into hardened, peer-reviewed architectural documentation.

---

## Phase 1: The Gatekeeper (Input Validation)
**DO NOT** proceed to design or diagramming until the following "Contract" is satisfied. If inputs are missing, list them as checkboxes and ask for clarification.

### Input Requirements:
1. **Product Overview:** Business context and the "Why."
2. **Functional Requirements (FR):** Core capabilities.
3. **Non-Functional Requirements (NFR):** Specific targets for Latency (p95/p99), Throughput (TPS), Availability (SLAs), and Data Retention.
4. **Constraints:** Budget, existing tech stack, security/compliance, and timeline.
5. **Explicit Non-Goals:** What we are intentionally NOT building to avoid scope creep.

---

## Phase 2: Architectural Synthesis
Analyze the requirements, design the system

- **Output**: ADR (Architecture Decision Record) with C4 Mermaid charts for modeling
- **Focus:** Justify the "Why" and document the "Consequences" (the technical debt or trade-offs introduced).
---

## Phase 3: The Hardening Loop (Stress Test)
Before finalizing, you must perform an internal "Adversarial Review":
- **Failure Mode Analysis:** Simulate a regional outage or a downstream API failure.
- **Bottleneck Identification:** Identify the most likely component to fail under 10x load.
- **Vibe Alignment:** Ensure the solution fits our "Golden Paths

## Phase 4: Implementation Planning
Analyze the ADRs and C4 diagrams to create a structured technical implementation plan.
- **Output**: Implementation Plan with C4 Mermaid charts for modeling
- **Focus:** Justify the "Why" and document the "Consequences" (the technical debt or trade-offs introduced).
---

## Output Standards & Interactions
- **Tone:** Concise, direct, and peer-to-peer (Tech Lead level).
- **Format:** Markdown + Mermaid.
- **Context Management:** After every major decision, update `current_state.md` to reflect the new proposed source of truth.
- **References:** Always cite existing patterns or previous ADRs when making new recommendations.
----- END FILE: AGENTS.md -----


----- BEGIN FILE: current_state.md -----
# Current System State

## Project: BTC Tap Trading Application (HARDENED v1.3)

### Architecture Overview

**Deployment Model**: Monolithic single-VM deployment with hardening  
**VM Specs**: 4-core CPU @ 2.5 GHz, 8 GB RAM, 256 GB SSD  
**SLA Target**: 99.5% uptime (scheduled maintenance acceptable)  
**Peak Load**: 3,000 RPS (handled via write batching)  
**Latency Target**: ‚â§100ms end-to-end (optimistic + batched)

### Hardening Summary

#### Round 001 (Catastrophic Risks)
| Risk | Mitigation | ADR |
|------|------------|-----|
| SQLite write contention at 3K RPS | In-memory write batching | ADR-005 |
| Disk exhaustion (25 days to death) | Weekly partitioning + S3 archival | ADR-006 |
| Replay attacks on Web3 auth | Timestamp + Nonce validation | ADR-007 |
| Stale price feed | Automatic circuit breaker | ADR-008 |

#### Round 002 (Operational Risks)
| Risk | Mitigation | ADR/Note |
|------|------------|----------|
| Double-spend race condition | Conditional SQL balance deduction | ADR-009 |
| Redis nonce memory bloat | TTL: 24h ‚Üí 90s (config change) | Config |
| Archival silent failure | Dead man's switch metric | Ops |
| Circuit breaker flapping | 30s hysteresis on recovery | Config |
| Optimistic UI trust gap | "Pending" state until WS confirm | UX Spec |
| Read-write contention | SQLite read replica for resolution | Diagram update |
| Thundering herd on reconnect | Client jitter + nginx conn limits | Diagram update |

#### Round 003 (Financial Auditability)
| Risk | Mitigation | ADR |
|------|------------|-----|
| Mutable balance (no audit trail) | Hybrid Transaction Ledger | ADR-010 |
| Dispute resolution | Query ledger by user/ref/time | ADR-010 |
| Data corruption undetected | Daily integrity check (SUM validation) | ADR-010 |

### Technology Stack

| Layer | Technology | Version |
|-------|------------|---------|
| Frontend | React or Vue | Latest LTS |
| API Server | Go (Echo/Fiber) | 1.21+ |
| Bet Batcher | Go (channel-based) | - |
| Signature Validator | Go | - |
| Circuit Breaker | Go middleware | - |
| WebSocket Server | Go | 1.21+ |
| Price Health Monitor | Go | 1.21+ |
| Bet Resolution Worker | Go | 1.21+ |
| Weekly Archiver | Go + bash + zstd | - |
| Primary Database | SQLite (bets + ledger, WAL) | 3.x |
| Historical Database | SQLite (bets + ledger, read-only) | 3.x |
| Cache/PubSub | Redis | 7.x |
| Reverse Proxy | Nginx | Latest |
| Cold Storage | AWS S3 (STANDARD_IA) | - |
| Process Supervision | systemd | - |

### Data Retention

| Data Type | Retention | Storage |
|-----------|-----------|---------|
| Current week bets | Current week | SQLite (hot, local) |
| Recent history | 4 weeks | SQLite (read-only, local) |
| Historical bets | Forever | S3 STANDARD_IA |
| Market price data | 7 days | Redis (TTL) |
| User sessions | 24 hours | Redis (TTL) |
| Used nonces | 90 seconds | Redis (TTL) |

### Storage Budget (256GB SSD)

| Component | Size | Notes |
|-----------|------|-------|
| SQLite current week | ~18 GB | Bets + transaction_ledger |
| SQLite historical (4 weeks) | ~72 GB | Queryable locally |
| Redis (maxmemory) | ~512 MB | Configured |
| Application binaries/logs | ~5 GB | Generous estimate |
| **Total Used** | **~95 GB** | **~160 GB headroom** |

### Services Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Single VM (4c/8GB/256GB)                         ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Nginx     ‚îÇ  ‚îÇ           Application Layer (Go binaries)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   :80/443   ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ  ‚îÇ  API    ‚îÇ ‚îÇ   WS    ‚îÇ ‚îÇ  Price  ‚îÇ ‚îÇ  Arch  ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îÇ  ‚îÇ Server  ‚îÇ ‚îÇ Server  ‚îÇ ‚îÇ Ingest  ‚îÇ ‚îÇ  iver  ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇstatic‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚î§:8080   ‚îÇ ‚îÇ  :8081  ‚îÇ ‚îÇ         ‚îÇ ‚îÇ        ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇfiles ‚îÇ    ‚îÇ  ‚îÇ  ‚îÇ         ‚îÇ ‚îÇ         ‚îÇ ‚îÇ         ‚îÇ ‚îÇ        ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ  ‚îÇ‚îÇBatcher‚îÇ‚îÇ ‚îÇ‚îÇSession‚îÇ‚îÇ ‚îÇ‚îÇHealth ‚îÇ‚îÇ ‚îÇ‚îÇWeekly‚îÇ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  rate limit ‚îÇ  ‚îÇ  ‚îÇ‚îÇChannel‚îÇ‚îÇ ‚îÇ‚îÇManager‚îÇ‚îÇ ‚îÇ‚îÇMonitor‚îÇ‚îÇ ‚îÇ‚îÇS3    ‚îÇ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  conn limit ‚îÇ  ‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ                   ‚îÇ       ‚îÇ                        ‚îÇ         ‚îÇ      ‚îÇ   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                         Data Layer                                ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  SQLite RW     ‚îÇ  ‚îÇ  SQLite RO     ‚îÇ  ‚îÇ  Redis 7         ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (current week)‚îÇ  ‚îÇ  (historical)  ‚îÇ  ‚îÇ  - Sessions      ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  bets_2025_w06 ‚îÇ  ‚îÇ  bets_2025_w*  ‚îÇ  ‚îÇ  - Nonces        ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  WAL mode      ‚îÇ  ‚îÇ  Attached read ‚îÇ  ‚îÇ  - Prices        ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  - Pub/Sub       ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ   S3 Cold Storage   ‚îÇ
                         ‚îÇ   STANDARD_IA       ‚îÇ
                         ‚îÇ   bets_*.db.zst     ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Write Batching Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HTTP Handler ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Bet Channel  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Batcher    ‚îÇ
‚îÇ  (Goroutine)  ‚îÇ 202  ‚îÇ (10K buffer) ‚îÇ      ‚îÇ  (1 writer)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚ñ≤                                             ‚îÇ
     ‚îÇ WebSocket confirm                           ‚îÇ Batch insert
     ‚îÇ                                             ‚ñº
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                             ‚îÇ SQLite (WAL) ‚îÇ
                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Architectural Decisions

| ADR | Decision | Status | Purpose |
|-----|----------|--------|---------|
| 001 | SQLite as Primary Database | Accepted | Zero ops overhead |
| 002 | Go as Backend Language | Accepted | Concurrency + type safety |
| 003 | Monolithic Single-VM Deployment | Accepted | Team velocity |
| 004 | Redis for Cache and Pub/Sub | Accepted | Speed + decoupling |
| 005 | Write Batching for Bet Ingestion | Accepted (Hardening) | Survive 3K RPS |
| 006 | Archival Strategy for Bet History | Accepted (Hardening) | Infinite retention on 256GB |
| 007 | Replay Protection for Web3 Auth | Accepted (Hardening) | Security |
| 008 | Stale Price Circuit Breaker | Accepted (Hardening) | Fairness |
| 009 | Conditional Balance Deduction | Accepted (Hardening 002) | Double-spend prevention |
| 010 | Hybrid Transaction Ledger | Accepted (Hardening 003) | Financial auditability |

### Constraints & Limits

- **No horizontal scaling** (by design, for simplicity)
- **Single-writer SQLite** (batched to mitigate)
- **Scheduled maintenance** acceptable per SLA
- **Weekly archival required** for sustainability

### Security Controls

| Control | Implementation |
|---------|----------------|
| Replay protection | Timestamp (30s window) + Nonce (Redis SET NX, 90s TTL) |
| Signature validation | EIP-191 personal_sign recovery |
| Rate limiting | Redis-based per wallet |
| Input validation | Server-side constraint enforcement |
| Double-spend prevention | Conditional SQL UPDATE: `balance >= amount` |
| Audit trail | Hybrid ledger: `transaction_ledger` table (append-only) |
| Dispute resolution | Query ledger by user_id, ref_type, ref_id, time |

### Observability

| Metric | Source |
|--------|--------|
| Request rate/latency | API Server Prometheus |
| Error rate | Application logs (structured JSON) |
| Bet resolution outcomes | Worker metrics |
| Price feed health | Health Monitor status |
| Disk usage | Node exporter |
| Circuit breaker state | Health endpoint |
| Archive health | `last_successful_archive_timestamp` gauge |

### Accepted Risks (Post-Hardening)

1. **VM failure** = total outage (mitigated by hourly backups to S3)
2. **In-memory batch loss** = max 100ms of unconfirmed bets (acceptable for trading)
3. **S3 dependency** for historical queries (acceptable, async restore available)

### Deployment Checklist

- [ ] SQLite configured with WAL mode
- [ ] Redis configured with AOF + RDB persistence
- [ ] Nginx configured with rate limiting and connection limits
- [ ] S3 bucket created with STANDARD_IA lifecycle
- [ ] Weekly archive cron job installed
- [ ] Circuit breaker thresholds configured (enter: 10s, recover: 30s stable)
- [ ] Health endpoint monitoring configured
- [ ] Client-side reconnect jitter implemented
- [ ] **Nonce TTL set to 90 seconds** (not 24h)
- [ ] **Dead man's switch metric** `last_successful_archive_timestamp` alerting
- [ ] **UX**: Pending state until WebSocket confirmation
- [ ] **Startup**: Redis balance cache rebuild from SQLite
- [ ] **Database**: Create `transaction_ledger` table with indexes
- [ ] **Cron**: Daily integrity check (`users.balance == SUM(ledger.amount)`)
- [ ] **Archiver**: Include `transaction_ledger` in weekly partitions |

### Last Updated
2026-02-03 (Hardened v1.3)
----- END FILE: current_state.md -----


----- BEGIN FILE: docs/adr/001-sqlite-as-primary-database.md -----
# ADR 001: SQLite as Primary Database

## Status
Accepted

## Context
We need a database solution that:
- Supports 3,000 RPS peak load
- Provides atomic transactions for bet placement (prevent double-spend)
- Fits in 8GB RAM on a single VM
- Can be deployed and operational within 2 weeks by 1 backend engineer
- Retains bet history indefinitely (forever retention requirement)

## Decision
Use **SQLite** in WAL (Write-Ahead Logging) mode as the primary database.

## Consequences

### Positive
- **Zero operational complexity**: No separate process to manage, backup, or monitor
- **Single-file portability**: Easy backups, migrations, and local development
- **ACID compliance**: Native support for atomic transactions critical for betting correctness
- **Sufficient performance**: SQLite with WAL can handle 10,000+ writes/sec on SSD; our 3K RPS is read-heavy
- **Tiny resource footprint**: Fits comfortably in 8GB RAM constraint
- **Team velocity**: 1 senior backend engineer can be productive immediately

### Negative
- **No horizontal scaling**: Cannot shard across multiple VMs later without migration
- **Single-writer bottleneck**: All writes serialized through one connection pool
- **Manual backup strategy**: Need to implement automated backups (sqlite3 .backup or file-level)
- **No replication**: No hot standby; 99.5% SLA relies on VM reliability + quick restore from backup

### Trade-off Analysis
| Alternative | Why Rejected |
|-------------|--------------|
| PostgreSQL | Requires separate process, more config, connection pooling complexity; overkill for single VM |
| MySQL | Same as PostgreSQL; adds unnecessary operational burden |
| MongoDB | No ACID transactions across documents; harder to guarantee betting correctness |

## Mitigation Strategies

1. **WAL Mode**: Enables concurrent reads during writes, improves concurrency
2. **Connection Pooling**: Use `sql.DB` with appropriate `SetMaxOpenConns()` (suggest 10-20)
3. **Read Replicas (Future)**: If needed, can add read-only replicas via SQLite's backup API
4. **Backup Strategy**: Hourly incremental backups using `sqlite3 .backup`, daily full backups to S3
5. **Monitoring**: Alert on disk usage (256GB constraint), query slow log

## Compliance with NFRs

| NFR | Compliance |
|-----|------------|
| 3K RPS | ‚úÖ SQLite handles 10K+ writes/sec on SSD; our workload is read-heavy |
| ‚â§100ms latency | ‚úÖ In-process database eliminates network round-trip |
| Atomic bet placement | ‚úÖ ACID transactions with `BEGIN IMMEDIATE` |
| 99.5% uptime | ‚ö†Ô∏è Acceptable risk given constraint; mitigated by backups |
| Forever retention | ‚úÖ Single-file growth managed; archive old bets to cold storage if needed |

## Decision Date
2026-02-03

## Author
System Architect
----- END FILE: docs/adr/001-sqlite-as-primary-database.md -----


----- BEGIN FILE: docs/adr/002-go-backend-language.md -----
# ADR 002: Go as Backend Language

## Status
Accepted

## Context
We have 1 senior backend engineer and 2 weeks to deliver a production system handling 3K RPS with strict correctness requirements around concurrent bet placement.

## Decision
Use **Go** for all backend services (API, WebSocket, Workers).

## Consequences

### Positive
- **Concurrency primitives**: Goroutines + channels ideal for WebSocket handling and background workers
- **Type safety**: Compile-time checks reduce runtime errors in financial logic
- **Static binary**: Single binary deployment simplifies ops on single VM
- **Performance**: Efficient memory usage fits 8GB constraint; handles 3K RPS easily
- **SQLite ecosystem**: Excellent `modernc.org/sqlite` and `mattn/go-sqlite3` drivers
- **Team fit**: Senior backend engineer likely already productive in Go

### Negative
- **Verbosity**: More boilerplate than Python/Node.js for simple CRUD
- **Frontend gap**: No code sharing with frontend (unlike Node.js), but team is separate anyway

### Trade-off Analysis
| Alternative | Why Rejected |
|-------------|--------------|
| Node.js | Event-loop can block on CPU-heavy bet resolution; type safety requires TypeScript overhead |
| Python (FastAPI) | GIL limits true concurrency; async/await complexity for WebSocket broadcasting |
| Rust | Steeper learning curve, slower development velocity; 2-week timeline risk |

## Decision Date
2026-02-03

## Author
System Architect
----- END FILE: docs/adr/002-go-backend-language.md -----


----- BEGIN FILE: docs/adr/003-monolithic-single-vm.md -----
# ADR 003: Monolithic Single-VM Deployment

## Status
Accepted

## Context
Hard constraints: 2-week timeline, single VM (4c/8GB/256GB), 2-person team. Must support 3K RPS and 99.5% uptime.

## Decision
Deploy as **monolithic architecture** on a single VM with process supervision (systemd).

## Consequences

### Positive
- **Minimal ops overhead**: No container orchestration, service mesh, or load balancers
- **Fast deployment**: `scp` binary + `systemctl restart` is the deploy pipeline
- **Simple debugging**: All logs in one place, single machine to SSH into
- **Resource efficiency**: No Docker/Kubernetes overhead consuming RAM
- **Team velocity**: Both engineers can focus on features, not infrastructure

### Negative
- **No horizontal scaling**: Cannot add VMs to handle traffic spikes
- **Single point of failure**: VM failure = total outage
- **Maintenance windows**: Deploys require brief downtime (acceptable per 99.5% SLA)
- **Blast radius**: Bug in one component affects all

### Trade-off Analysis
| Alternative | Why Rejected |
|-------------|--------------|
| Docker Compose | Adds complexity without benefit on single VM; resource overhead |
| Kubernetes | Massive overkill; violates 2-week constraint; requires ops expertise |
| Microservices | Network latency between services; operational complexity; team too small |
| Serverless (Lambda/Cloud Functions) | Cold start latency violates 100ms requirement; vendor lock-in |

## Process Architecture

```
Single VM
‚îú‚îÄ‚îÄ nginx (reverse proxy, static files)
‚îú‚îÄ‚îÄ api-server (Go binary, port 8080)
‚îú‚îÄ‚îÄ ws-server (Go binary, port 8081)
‚îú‚îÄ‚îÄ price-ingestion (Go binary)
‚îú‚îÄ‚îÄ bet-resolver (Go binary, cron-triggered or daemon)
‚îú‚îÄ‚îÄ redis-server (port 6379)
‚îî‚îÄ‚îÄ sqlite.db (single file on SSD)
```

## Deployment Flow

```bash
# Build on CI
go build -o tap-trading-api ./cmd/api
go build -o tap-trading-ws ./cmd/ws

# Deploy via SSH
scp tap-trading-* user@vm:/opt/app/
ssh user@vm "sudo systemctl restart tap-trading-*"
```

## Monitoring

- **Process supervision**: systemd auto-restarts crashed services
- **Health checks**: HTTP `/health` endpoint for each service
- **Log aggregation**: journald + optional forwarding to external log service
- **Metrics**: Prometheus exporter embedded in API server

## Decision Date
2026-02-03

## Author
System Architect
----- END FILE: docs/adr/003-monolithic-single-vm.md -----


----- BEGIN FILE: docs/adr/004-redis-for-cache-and-pubsub.md -----
# ADR 004: Redis for Cache, Sessions, and Pub/Sub

## Status
Accepted

## Context
The system needs:
1. Session storage for Web3-authenticated users
2. Rate limiting on bet placement API
3. Real-time price distribution to multiple WebSocket server instances (if ever scaled)
4. 7-day price history retention for bet resolution
5. Hot data caching to reduce SQLite load

## Decision
Use **Redis** for caching, session storage, rate limiting, and price pub/sub.

## Consequences

### Positive
- **Pub/Sub**: Clean decoupling between price ingestion and WebSocket broadcasting
- **Speed**: Sub-millisecond operations for session validation and rate limiting
- **Data structures**: Built-in sorted sets perfect for time-series price history
- **TTL support**: Automatic expiration of 7-day price data
- **Lightweight**: Fits in 8GB alongside SQLite and Go services

### Negative
- **Additional process**: One more thing to monitor and restart
- **Memory limit**: Price data must be pruned (TTL configured to 7 days)
- **Durability**: Redis AOF/RDB adds disk I/O; acceptable for ephemeral data

## Data Stored in Redis

| Key Pattern | Type | TTL | Purpose |
|-------------|------|-----|---------|
| `session:{wallet}` | String | 24h | Web3 auth session |
| `rate_limit:{wallet}` | String | 1m | Bet placement rate limiting |
| `price:btc:usd` | String | 1h | Latest price cache |
| `prices:btc:usd:ts` | Sorted Set | 7d | Time-series price history for resolution |
| `cache:grid:{params}` | String | 5m | Computed reward rates grid |
| `ws:clients` | Set | - | Connected client tracking (optional) |

## Pub/Sub Flow

```
Price Ingestion Service
    ‚Üì (PUBLISH prices:btc:usd $price)
Redis Pub/Sub
    ‚Üì
WebSocket Server (SUBSCRIBE)
    ‚Üì (Broadcast)
Connected Clients
```

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| In-memory only | Lost on restart; no shared state between services |
| SQLite for sessions | Write amplification; slower than Redis for high-frequency reads |
| Kafka | Overkill for single VM; massive resource consumption |
| NATS | Good alternative, but Redis already needed for caching; minimize moving parts |

## Redis Configuration

```conf
# redis.conf for 8GB VM
maxmemory 512mb
maxmemory-policy allkeys-lru
appendonly yes
appendfsync everysec
```

## Decision Date
2026-02-03

## Author
System Architect
----- END FILE: docs/adr/004-redis-for-cache-and-pubsub.md -----


----- BEGIN FILE: docs/adr/005-write-batching-for-bet-ingestion.md -----
# ADR 005: Write Batching for Bet Ingestion

## Status
Accepted (Hardening Response)

## Context
Hardening Audit #001 identified that SQLite's single-writer limitation will fail at 3K RPS with individual row inserts. Each transaction requires fsync to WAL, causing queue buildup and latency spikes.

## Problem Statement
- SQLite allows only ONE writer at a time (even in WAL mode)
- 3,000 concurrent taps = 3,000 transactions queuing
- Each fsync to SSD takes ~0.5-2ms
- Queue depth √ó fsync time = unbounded latency growth
- `SQLITE_BUSY` errors under load violate correctness NFRs

## Decision
Implement **in-memory write batching** with channel-based aggregation.

## Solution Design

### Architecture Change

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     API Server (Go)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ HTTP Handler ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Bet Channel ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Batch Flusher   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Goroutine)‚îÇ    ‚îÇ (Buffered   ‚îÇ    ‚îÇ (Single Writer) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   10,000)   ‚îÇ    ‚îÇ                 ‚îÇ  ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ Flush triggers: ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ ‚Ä¢ 100 items OR  ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ ‚Ä¢ 100ms timeout ‚îÇ  ‚îÇ
‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                 ‚îÇ            ‚îÇ
‚îÇ                                                 ‚ñº            ‚îÇ
‚îÇ                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                                        ‚îÇ SQLite (WAL)    ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ BEGIN IMMEDIATE ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ INSERT x N      ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ COMMIT          ‚îÇ  ‚îÇ
‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation Details

```go
// Batch configuration
const (
    BatchSize     = 100      // Flush after N bets
    BatchTimeout  = 100 * time.Millisecond
    ChannelBuffer = 10000    // Backpressure threshold
)

type BetBatch struct {
    Bets []Bet
    Done chan error  // Per-bet completion signaling
}

type BetFlusher struct {
    channel chan PendingBet
    db      *sql.DB
}

func (f *BetFlusher) Start() {
    ticker := time.NewTicker(BatchTimeout)
    batch := make([]PendingBet, 0, BatchSize)
    
    for {
        select {
        case bet := <-f.channel:
            batch = append(batch, bet)
            if len(batch) >= BatchSize {
                f.flush(batch)
                batch = batch[:0]
                ticker.Reset(BatchTimeout)
            }
            
        case <-ticker.C:
            if len(batch) > 0 {
                f.flush(batch)
                batch = batch[:0]
            }
        }
    }
}

func (f *BetFlusher) flush(bets []PendingBet) {
    tx, _ := f.db.BeginTx(context.Background(), &sql.TxOptions{
        Isolation: sql.LevelSerializable,
    })
    
    stmt, _ := tx.Prepare("INSERT INTO bets (user_id, cell_id, amount, ...) VALUES (?, ?, ?, ...)")
    
    for _, bet := range bets {
        // Insert bet + deduct balance in single statement
        stmt.Exec(bet.UserID, bet.CellID, bet.Amount)
    }
    
    err := tx.Commit()
    
    // Signal completion to each waiting goroutine
    for _, bet := range bets {
        bet.Done <- err
    }
}
```

### Response Flow

1. **Synchronous Validation** (HTTP handler):
   - Validate signature + nonce (anti-replay)
   - Check balance in Redis (fast)
   - Check cell availability (Redis cache)
   - Validate timing rules

2. **Async Persistence** (channel):
   - Submit to batch channel
   - Wait on `Done` channel with timeout
   - Return 202 Accepted immediately (optimistic)

3. **WebSocket Confirmation**:
   - Client receives `bet:confirmed` or `bet:failed` event
   - Grid updates optimistically, confirmed async

## Throughput Math

| Scenario | Before | After |
|----------|--------|-------|
| Transaction count | 3,000/sec | 30/sec (100x batch) |
| fsync per second | 3,000 | 30 |
| Avg latency @ 3K RPS | 500-2000ms (queuing) | 10-50ms (batched) |
| SQLite busy errors | High | Zero |

## Risk: Data Loss on Crash

| Window | Exposure | Mitigation |
|--------|----------|------------|
| In-channel (unflushed) | Max 100ms √ó 3K = 300 bets | Acceptable for trading app; document in ToS |
| Client knows | WebSocket confirms persistence | Retry mechanism for unconfirmed bets |

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| Individual transactions | Fails at 3K RPS per hardening audit |
| PostgreSQL | Violates 2-week constraint, adds ops burden |
| Async queue (RabbitMQ) | Extra process, overkill for single VM |
| In-memory only | Loses durability guarantee |

## NFR Compliance

| NFR | Before | After |
|-----|--------|-------|
| 3K RPS | ‚ùå Fails | ‚úÖ 30 batched TX/sec |
| ‚â§100ms latency | ‚ùå Spikes | ‚úÖ Batched + optimistic |
| Atomic bets | ‚úÖ | ‚úÖ (per-batch atomic) |
| Correctness | ‚ùå SQLITE_BUSY | ‚úÖ Ordered channel |

## Decision Date
2026-02-03

## Author
System Architect (Hardening Response)
----- END FILE: docs/adr/005-write-batching-for-bet-ingestion.md -----


----- BEGIN FILE: docs/adr/006-archival-strategy-for-bet-history.md -----
# ADR 006: Archival Strategy for Bet History

## Status
Accepted (Hardening Response)

## Context
Hardening Audit #001 calculated that "Forever" retention on 256GB SSD will exhaust disk in ~25 days:
- 600 bets/sec √ó 86400 √ó 200 bytes = ~10 GB/day
- 256GB √∑ 10GB/day = 25.6 days

## Decision
Implement **time-based database partitioning** with automated archival to cold storage.

## Solution Design

### Partitioning Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SQLite Partition Layout                    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ  bets_2025_w05  ‚îÇ    ‚îÇ  bets_2025_w06  ‚îÇ  (Cold - S3)     ‚îÇ
‚îÇ  ‚îÇ  ~2GB (zstd)    ‚îÇ    ‚îÇ  ~2GB (zstd)    ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ  bets_2025_w07  ‚îÇ    ‚îÇ  bets_current   ‚îÇ  (Hot - Local)   ‚îÇ
‚îÇ  ‚îÇ  (read-only)    ‚îÇ    ‚îÇ  (read-write)   ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ         ‚ñ≤                      ‚ñ≤                              ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ                              ‚îÇ
‚îÇ    Historical queries      Live betting                     ‚îÇ
‚îÇ    (ATTACH DATABASE)       (INSERT)                          ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Weekly Rotation

| Phase | Action | Schedule |
|-------|--------|----------|
| **Active** | Current week bets go to `bets_yyyy_ww.db` | Continuous |
| **Freeze** | Close write, run `VACUUM INTO` | Sunday 00:00 UTC |
| **Compress** | `zstd bets_yyyy_ww.db` | Sunday 00:30 UTC |
| **Upload** | `aws s3 cp` to cold storage | Sunday 01:00 UTC |
| **Attach** | `ATTACH DATABASE` for reads | Sunday 01:30 UTC |
| **Purge** | Delete local compressed file after 4 weeks | 28 days later |

### Local Hot Storage Budget

| Retention | Size | Notes |
|-----------|------|-------|
| Current week (read-write) | ~10 GB | Active betting |
| Previous 4 weeks (read-only) | ~40 GB | Recent history queries |
| **Total Hot** | **~50 GB** | Leaves 200GB headroom |

### Query Routing

```go
type PartitionManager struct {
    currentDB *sql.DB
    attached  map[string]*sql.DB  // week -> read-only connection
}

func (pm *PartitionManager) QueryBets(ctx context.Context, userID string, from, to time.Time) ([]Bet, error) {
    // Determine which partitions to query
    weeks := getWeeksBetween(from, to)
    
    var results []Bet
    for _, week := range weeks {
        if week == currentWeek {
            // Query active DB
            results = append(results, pm.query(pm.currentDB, userID, from, to))
        } else {
            // Query attached historical DB
            if db, ok := pm.attached[week]; ok {
                results = append(results, pm.query(db, userID, from, to))
            } else {
                // Return "data archived" error or trigger async restore
                return nil, ErrArchived
            }
        }
    }
    return results, nil
}
```

### Automated Archival Script

```bash
#!/bin/bash
# /opt/app/scripts/archive-weekly.sh

WEEK=$(date -d 'last week' +%Y_w%U)
DB_FILE="/data/bets_${WEEK}.db"
ARCHIVE_FILE="/tmp/bets_${WEEK}.db.zst"

# 1. Freeze: VACUUM INTO (creates optimized copy)
sqlite3 /data/bets_current.db "VACUUM INTO '${DB_FILE}'"

# 2. Compress (zstd for speed + ratio)
zstd -19 -T4 -o "${ARCHIVE_FILE}" "${DB_FILE}"

# 3. Upload to S3
aws s3 cp "${ARCHIVE_FILE}" s3://tap-trading-archives/bets/ \
    --storage-class STANDARD_IA

# 4. Verify upload
aws s3 ls s3://tap-trading-archives/bets/bets_${WEEK}.db.zst

# 5. Update metadata
sqlite3 /data/metadata.db "INSERT INTO archives (week, s3_path, size) VALUES ('${WEEK}', 's3://...', $(stat -c%s ${ARCHIVE_FILE}))"

# 6. Attach new partition for local reads (optional, based on retention policy)
# 7. Purge old local files (>4 weeks)
find /data/bets_*.db -mtime +28 -delete
```

### Auto-Vacuum Configuration

```sql
-- Enable incremental auto-vacuum on each partition
PRAGMA auto_vacuum = INCREMENTAL;
PRAGMA journal_mode = WAL;

-- After deletions (if any), reclaim space without full VACUUM
PRAGMA incremental_vacuum(1000);
```

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| Single DB forever | Disk death in 25 days |
| DELETE + VACUUM | Locks database for hours; violates uptime |
| TimescaleDB | Extra process, complexity, not "boring" tech |
| Partition by day | 365 files/year = too many ATTACH operations |
| Partition by month | 4-5 weeks of hot data still ~50GB, acceptable |

## Operational Runbook

### Restoring Archived Data (User Dispute)

```bash
# 1. Identify week from timestamp
WEEK="2025_w05"

# 2. Download from S3
aws s3 cp s3://tap-trading-archives/bets/bets_${WEEK}.db.zst /tmp/

# 3. Decompress
unzstd /tmp/bets_${WEEK}.db.zst -o /data/restore/

# 4. Attach and query
sqlite3 /data/bets_current.db "ATTACH '/data/restore/bets_${WEEK}.db' AS h; SELECT * FROM h.bets WHERE user_id = '...';"
```

### Disk Alert Thresholds

| Threshold | Action |
|-----------|--------|
| 70% (180GB) | Warning: Accelerate archival |
| 85% (218GB) | Critical: Stop new bets, emergency archive |
| 95% (243GB) | Emergency: Purge oldest local partitions |

## NFR Compliance

| NFR | Compliance |
|-----|------------|
| Bet history forever | ‚úÖ Archived to S3 with STANDARD_IA |
| 99.5% uptime | ‚úÖ No full VACUUM; incremental only |
| Query performance | ‚úÖ Current week fast; historical via ATTACH |
| 256GB constraint | ‚úÖ ~50GB hot, ~200GB headroom |

## Decision Date
2026-02-03

## Author
System Architect (Hardening Response)
----- END FILE: docs/adr/006-archival-strategy-for-bet-history.md -----


----- BEGIN FILE: docs/adr/007-replay-protection-for-web3-auth.md -----
# ADR 007: Replay Protection for Web3 Auth

## Status
Accepted (Hardening Response)

## Context
Hardening Audit #001 identified vulnerability: Without replay protection, attackers can capture valid bet payloads and replay them to drain user funds or grief the system.

## Decision
Implement **timestamp + nonce** validation with Redis-backed replay cache.

## Solution Design

### Signature Payload Schema

```typescript
// Client constructs message for signing
interface BetSignaturePayload {
    action: "place_bet";
    cell_id: string;           // Grid cell identifier
    amount: string;            // Bet amount (wei-like string)
    nonce: string;             // UUID v4, unique per request
    timestamp: number;         // Unix timestamp (seconds)
    chain_id: number;          // Prevent cross-chain replay
    contract_version: string;  // "1" for protocol versioning
}

// EIP-191 compatible message format
const message = `Tap Trading Bet
Action: place_bet
Cell: ${cell_id}
Amount: ${amount}
Nonce: ${nonce}
Timestamp: ${timestamp}
Chain: ${chain_id}
Version: ${contract_version}`;

// Sign with personal_sign (EIP-191)
const signature = await wallet.signMessage(message);
```

### Server Validation Flow

```go
type SignatureValidator struct {
    redis *redis.Client
    clock func() time.Time
}

func (v *SignatureValidator) ValidateBet(ctx context.Context, req BetRequest) error {
    // 1. Timestamp freshness (prevent old signature reuse)
    now := v.clock().Unix()
    if abs(now - req.Timestamp) > 30 {
        return ErrSignatureExpired
    }
    
    // 2. Nonce uniqueness check (Redis SET NX with TTL)
    nonceKey := fmt.Sprintf("nonce:%s", req.Nonce)
    ok, err := v.redis.SetNX(ctx, nonceKey, req.UserID, 24*time.Hour).Result()
    if err != nil || !ok {
        return ErrReplayDetected  // Nonce already used
    }
    
    // 3. Recover signer address from signature
    recoveredAddr, err := recoverAddress(req.Message, req.Signature)
    if err != nil {
        return ErrInvalidSignature
    }
    
    // 4. Verify signer matches claimed user
    if !strings.EqualFold(recoveredAddr, req.UserID) {
        return ErrUnauthorized
    }
    
    // 5. Verify payload integrity (hash matches)
    expectedMessage := constructMessage(req)
    if req.Message != expectedMessage {
        return ErrTamperedPayload
    }
    
    return nil
}
```

### Redis Key Strategy

| Key Pattern | Type | TTL | Purpose |
|-------------|------|-----|---------|
| `nonce:{uuid}` | String | 24h | Replay protection (SET NX) |
| `nonce_count:{date}` | Counter | 7d | Rate limiting abuse detection |

### Client Implementation

```typescript
class BetClient {
    async placeBet(cellId: string, amount: string): Promise<void> {
        const payload = {
            action: "place_bet",
            cell_id: cellId,
            amount: amount,
            nonce: crypto.randomUUID(),
            timestamp: Math.floor(Date.now() / 1000),
            chain_id: await wallet.getChainId(),
            contract_version: "1"
        };
        
        const message = this.formatMessage(payload);
        const signature = await wallet.signMessage(message);
        
        // Optimistic UI update
        this.grid.markPending(cellId);
        
        try {
            await api.post('/bets', {
                ...payload,
                message,
                signature
            });
        } catch (e) {
            this.grid.revertPending(cellId);
            throw e;
        }
    }
}
```

## Attack Scenarios Mitigated

| Attack | Mitigation |
|--------|------------|
| Replay old bet | Timestamp expires after 30s |
| Replay same nonce | Redis SET NX fails on duplicate |
| Cross-chain replay | Chain ID in signed payload |
| Signature malleability | EIP-191 prefix standardization |
| Clock skew | 30s window accommodates minor drift |

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| Timestamp only | Clock skew issues; no nonce = replay within window |
| Nonce only | Requires persistent nonce storage (infinite growth) |
| Session tokens | Violates "Web3 native" design; adds complexity |
| On-chain verification | Gas costs, latency, violates off-chain custody |

## NFR Compliance

| NFR | Compliance |
|-----|------------|
| Replay attack prevention | ‚úÖ Timestamp + Nonce + Redis |
| ‚â§100ms latency | ‚úÖ Redis check is sub-millisecond |
| Stateless validation | ‚úÖ No session storage needed |

## Decision Date
2026-02-03

## Author
System Architect (Hardening Response)
----- END FILE: docs/adr/007-replay-protection-for-web3-auth.md -----


----- BEGIN FILE: docs/adr/008-stale-price-circuit-breaker.md -----
# ADR 008: Stale Price Circuit Breaker

## Status
Accepted (Hardening Response)

## Context
Hardening Audit #001 asked: "What happens if the price feed stops?" Users could bet on stale prices, or resolutions could use incorrect data.

## Decision
Implement **automatic circuit breaker** that suspends betting when price feed is stale.

## Solution Design

### Health Check Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Price Health Monitor                       ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ Price Ingestion ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Redis (pub/sub)‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ   Service       ‚îÇ     ‚îÇ  + Last Updated ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  Timestamp      ‚îÇ                 ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                   ‚îÇ                           ‚îÇ
‚îÇ                                   ‚ñº                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ   API Server    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Circuit Breaker‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (Bet Handler)  ‚îÇ     ‚îÇ   (Middleware)  ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚ñ≤                                                    ‚îÇ
‚îÇ          ‚îÇ REJECT if unhealthy                                 ‚îÇ
‚îÇ          ‚ñº                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ  ‚îÇ   Health Status ‚îÇ  [HEALTHY / DEGRADED / CRITICAL]         ‚îÇ
‚îÇ  ‚îÇ   /healthz      ‚îÇ                                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation

```go
const (
    PriceIntervalExpected = 1 * time.Second  // Expect 1 tick/sec
    StaleThreshold        = 3 * time.Second  // Degraded after 3s
    CriticalThreshold     = 10 * time.Second // Stop betting after 10s
)

type PriceHealthMonitor struct {
    redis       *redis.Client
    lastUpdated atomic.Int64  // Unix nano timestamp
    status      atomic.Value  // HealthStatus
}

type HealthStatus int

const (
    Healthy HealthStatus = iota
    Degraded            // Price delayed but recent
    Critical            // Price stale, betting suspended
)

func (m *PriceHealthMonitor) Start() {
    // Subscribe to price updates
    pubsub := m.redis.Subscribe(context.Background(), "prices:btc:usd")
    
    go func() {
        for msg := range pubsub.Channel() {
            m.lastUpdated.Store(time.Now().UnixNano())
            m.status.Store(Healthy)
        }
    }()
    
    // Background health checker
    ticker := time.NewTicker(1 * time.Second)
    go func() {
        for range ticker.C {
            m.evaluateHealth()
        }
    }()
}

func (m *PriceHealthMonitor) evaluateHealth() {
    last := time.Unix(0, m.lastUpdated.Load())
    age := time.Since(last)
    
    switch {
    case age > CriticalThreshold:
        m.status.Store(Critical)
        log.Error().Dur("stale_for", age).Msg("PRICE_FEED_CRITICAL")
        
    case age > StaleThreshold:
        m.status.Store(Degraded)
        log.Warn().Dur("stale_for", age).Msg("PRICE_FEED_DEGRADED")
        
    default:
        m.status.Store(Healthy)
    }
}

func (m *PriceHealthMonitor) IsHealthy() bool {
    return m.status.Load().(HealthStatus) != Critical
}

func (m *PriceHealthMonitor) GetStatus() HealthStatus {
    return m.status.Load().(HealthStatus)
}
```

### API Integration

```go
func BetMiddleware(health *PriceHealthMonitor) echo.MiddlewareFunc {
    return func(next echo.HandlerFunc) echo.HandlerFunc {
        return func(c echo.Context) error {
            if !health.IsHealthy() {
                return c.JSON(503, map[string]string{
                    "error": "PRICE_FEED_STALE",
                    "message": "Betting temporarily suspended due to stale price data",
                    "retry_after": "10",
                })
            }
            return next(c)
        }
    }
}

// Apply to bet placement routes only
api.POST("/bets", placeBetHandler, BetMiddleware(healthMonitor))
```

### Frontend Behavior

```typescript
// WebSocket status handler
ws.on('system:status', (status: SystemStatus) => {
    if (status.priceHealth === 'CRITICAL') {
        grid.disableInteractions();
        showBanner('Betting suspended - price feed issue detected');
    } else if (status.priceHealth === 'DEGRADED') {
        showWarning('Price feed delayed - trades at your own risk');
    } else {
        grid.enableInteractions();
        hideBanner();
    }
});
```

### Resolution Safety

Bet resolution also validates price freshness:

```go
func (r *BetResolver) Resolve(ctx context.Context, bet Bet) error {
    // Never resolve with potentially stale data
    if !r.priceMonitor.IsHealthy() {
        return ErrResolutionPaused
    }
    
    // Verify price history covers the resolution window
    prices := r.getPriceHistory(bet.TargetTimeStart, bet.TargetTimeEnd)
    if len(prices) == 0 {
        return ErrInsufficientPriceData
    }
    
    // Proceed with resolution...
}
```

### Alerting

| Condition | Severity | Action |
|-----------|----------|--------|
| DEGRADED > 30s | Warning | PagerDuty low-priority |
| CRITICAL > 10s | Critical | PagerDuty high-priority, auto-suspend betting |
| Recovery | Info | Resume betting, log incident |

## Health Endpoint

```json
GET /healthz
{
  "status": "healthy",
  "checks": {
    "price_feed": {
      "status": "healthy",
      "last_update": "2026-02-03T06:50:10Z",
      "stale_for_ms": 120
    },
    "database": {
      "status": "healthy",
      "connections": 15
    },
    "redis": {
      "status": "healthy",
      "latency_ms": 0.5
    }
  }
}
```

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| No circuit breaker | Users bet on stale prices = fairness violation |
| Only log warnings | Not proactive; ops may miss alert |
| Automatic failover to backup feed | Adds complexity; 2-week constraint |
| Full exchange integration | Out of scope; we consume feeds |

## NFR Compliance

| NFR | Compliance |
|-----|------------|
| Correctness | ‚úÖ Never resolve with stale data |
| Fairness | ‚úÖ Users can't exploit price delays |
| Observability | ‚úÖ Health endpoint + metrics |

## Decision Date
2026-02-03

## Author
System Architect (Hardening Response)
----- END FILE: docs/adr/008-stale-price-circuit-breaker.md -----


----- BEGIN FILE: docs/adr/009-conditional-balance-deduction.md -----
# ADR 009: Conditional Balance Deduction for Double-Spend Prevention

## Status
Accepted (Hardening Response 002)

## Context
Hardening Audit #002 identified a critical race condition: Redis balance check + SQLite batch deduction creates a window for double-spending.

## The Race Condition

```
User Balance: $10

Request A                    Request B
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Check Redis ($10 >= $10)    Check Redis ($10 >= $10)
    ‚Üì OK                         ‚Üì OK
Enter Batch                  Enter Batch
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    ‚Üì
            Batcher Flushes
                    ‚Üì
    INSERT Bet A (deduct $10)  
    INSERT Bet B (deduct $10)  ‚Üê Overdraft! Balance = -$10
```

## Decision
Use **conditional SQL UPDATE** as the single source of truth for balance deduction.

## Solution Design

### Database Schema Change

```sql
-- Users table with optimistic locking
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    wallet_address TEXT UNIQUE NOT NULL,
    balance INTEGER NOT NULL DEFAULT 0,  -- Stored in cents/smallest unit
    version INTEGER NOT NULL DEFAULT 1    -- Optimistic locking
);

-- Bets table tracks deduction outcome
CREATE TABLE bets (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    cell_id TEXT NOT NULL,
    amount INTEGER NOT NULL,
    status TEXT NOT NULL,  -- 'pending', 'confirmed', 'insufficient_funds'
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Partial index for fast pending queries
CREATE INDEX idx_bets_pending ON bets(user_id, status) WHERE status = 'pending';
```

### Batcher Implementation with Conditional Deduction

```go
const (
    StatusPending            = "pending"
    StatusConfirmed          = "confirmed"
    StatusInsufficientFunds  = "insufficient_funds"
)

type BetBatch struct {
    Bets []PendingBet
}

type PendingBet struct {
    BetID    string
    UserID   string
    CellID   string
    Amount   int64  // in cents
    Done     chan BetResult
}

type BetResult struct {
    Success bool
    Status  string
    Error   error
}

func (b *BetBatcher) flush(bets []PendingBet) {
    tx, err := b.db.BeginTx(context.Background(), &sql.TxOptions{
        Isolation: sql.LevelSerializable,
    })
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer tx.Rollback()

    // Prepare statements for efficiency
    deductStmt, _ := tx.Prepare(`
        UPDATE users 
        SET balance = balance - ?,
            version = version + 1
        WHERE id = ? 
        AND balance >= ?
    `)
    
    insertStmt, _ := tx.Prepare(`
        INSERT INTO bets (id, user_id, cell_id, amount, status)
        VALUES (?, ?, ?, ?, ?)
    `)

    results := make(map[string]BetResult)

    for _, bet := range bets {
        // Step 1: Attempt conditional balance deduction
        res, err := deductStmt.Exec(bet.Amount, bet.UserID, bet.Amount)
        if err != nil {
            results[bet.BetID] = BetResult{false, "", err}
            continue
        }

        rowsAffected, _ := res.RowsAffected()
        
        if rowsAffected == 0 {
            // Insufficient funds - record as failed
            insertStmt.Exec(bet.BetID, bet.UserID, bet.CellID, bet.Amount, StatusInsufficientFunds)
            results[bet.BetID] = BetResult{
                Success: false,
                Status:  StatusInsufficientFunds,
                Error:   ErrInsufficientFunds,
            }
            continue
        }

        // Step 2: Insert confirmed bet
        _, err = insertStmt.Exec(bet.BetID, bet.UserID, bet.CellID, bet.Amount, StatusConfirmed)
        if err != nil {
            // This shouldn't happen with proper constraints, but handle it
            results[bet.BetID] = BetResult{false, "", err}
            continue
        }

        results[bet.BetID] = BetResult{
            Success: true,
            Status:  StatusConfirmed,
            Error:   nil,
        }
    }

    if err := tx.Commit(); err != nil {
        b.failAll(bets, err)
        return
    }

    // Signal results to waiting goroutines
    for _, bet := range bets {
        bet.Done <- results[bet.BetID]
    }

    // Update Redis cache (best-effort, non-critical)
    b.syncRedisBalances(bets, results)
}

// syncRedisBalances updates Redis cache after successful DB commit
func (b *BetBatcher) syncRedisBalances(bets []PendingBet, results map[string]BetResult) {
    for _, bet := range bets {
        result := results[bet.BetID]
        if result.Success {
            // Decrement Redis balance (best effort)
            b.redis.DecrBy(context.Background(), 
                fmt.Sprintf("balance:%s", bet.UserID), 
                bet.Amount)
        }
    }
}
```

### API Flow with Database-First Validation

```go
func (h *BetHandler) PlaceBet(c echo.Context) error {
    var req BetRequest
    if err := c.Bind(&req); err != nil {
        return err
    }

    // 1. Validate signature (anti-replay)
    if err := h.validator.Validate(req); err != nil {
        return c.JSON(401, map[string]string{"error": "invalid_signature"})
    }

    // 2. Quick Redis balance check (early rejection, not authoritative)
    balanceKey := fmt.Sprintf("balance:%s", req.UserID)
    cachedBalance, err := h.redis.Get(c.Request().Context(), balanceKey).Int64()
    
    // Early rejection if definitely insufficient
    if err == nil && cachedBalance < req.Amount {
        return c.JSON(400, map[string]string{"error": "insufficient_funds"})
    }

    // 3. Check cell availability (Redis cache)
    if !h.grid.IsCellAvailable(req.CellID) {
        return c.JSON(400, map[string]string{"error": "cell_unavailable"})
    }

    // 4. Submit to batcher (database is authoritative)
    result := h.batcher.Submit(PendingBet{
        BetID:  generateUUID(),
        UserID: req.UserID,
        CellID: req.CellID,
        Amount: req.Amount,
        Done:   make(chan BetResult, 1),
    })

    // 5. Wait for batch result with timeout
    select {
    case result := <-result.Done:
        if !result.Success {
            if result.Status == StatusInsufficientFunds {
                return c.JSON(400, map[string]string{"error": "insufficient_funds"})
            }
            return c.JSON(500, map[string]string{"error": "internal_error"})
        }
        
        // Return 202 Accepted, WebSocket will confirm
        return c.JSON(202, map[string]string{
            "status": "accepted",
            "bet_id": result.BetID,
        })
        
    case <-time.After(5 * time.Second):
        // Batcher timeout - bet may or may not be processed
        return c.JSON(504, map[string]string{
            "error": "timeout",
            "message": "Bet status unknown, check history",
        })
    }
}
```

### Redis Cache Consistency

Redis balance is a **cache**, not authoritative:

| Scenario | Action |
|----------|--------|
| Cache hit | Use for early rejection only |
| Cache miss | Query SQLite, populate cache |
| After successful bet | Decrement cache (best effort) |
| After failed bet | No cache change |
| Startup/recovery | Rebuild Redis from SQLite: `SELECT id, balance FROM users` |

### Startup Recovery

```go
func (b *BetBatcher) RebuildRedisCache() error {
    rows, err := b.db.Query("SELECT id, balance FROM users")
    if err != nil {
        return err
    }
    defer rows.Close()

    pipe := b.redis.Pipeline()
    for rows.Next() {
        var userID string
        var balance int64
        rows.Scan(&userID, &balance)
        pipe.Set(context.Background(), 
            fmt.Sprintf("balance:%s", userID), 
            balance, 
            24*time.Hour)
    }
    _, err = pipe.Exec(context.Background())
    return err
}
```

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| Redis as truth (decrement there) | Redis not durable; crash = lost funds |
| Distributed locking (Redis Redlock) | Complex, slower, still has edge cases |
| Two-phase commit | Overkill for single VM; adds latency |
| Pessimistic locking (SELECT FOR UPDATE) | Doesn't work well with SQLite's single writer |

## NFR Compliance

| NFR | Compliance |
|-----|------------|
| Double-spend prevention | ‚úÖ Conditional SQL UPDATE enforces balance >= amount |
| Atomicity | ‚úÖ Single transaction per batch |
| Correctness | ‚úÖ Database is single source of truth |
| Performance | ‚úÖ Still batched; conditional check is fast (index on user_id) |
| ‚â§100ms latency | ‚úÖ Early rejection from cache; batch commit async |

## Decision Date
2026-02-03

## Author
System Architect (Hardening Response 002)
----- END FILE: docs/adr/009-conditional-balance-deduction.md -----


----- BEGIN FILE: docs/adr/010-hybrid-transaction-ledger.md -----
# ADR 010: Hybrid Transaction Ledger for Financial Auditability

## Status
Accepted (Hardening Response 003)

## Context
Hardening Audit #003 identified that ADR-009's mutable balance approach lacks financial auditability. Without a ledger, disputes ("Where did my money go?") are unanswerable.

## The Problem

### Mutable Balance Only (ADR-009)
```sql
UPDATE users SET balance = balance - 10 WHERE id = ?;
-- Previous state is LOST. No paper trail.
```

**Risk:** If a bug causes double-deduction or manual DB corruption, previous state is unrecoverable.

### Full Event Sourcing (Rejected)
```sql
-- Balance = SUM(transaction_ledger.amount) for user
-- Replay 100K events to check balance at 3K RPS = impossible
```

**Risk:** Too slow for our performance requirements.

## Decision
Implement a **Hybrid Ledger**: Mutable balance for speed, append-only ledger for auditability.

## Solution Design

### Schema

```sql
-- Users table: Mutable balance (performance cache)
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    wallet_address TEXT UNIQUE NOT NULL,
    balance INTEGER NOT NULL DEFAULT 0,  -- Materialized view of ledger
    version INTEGER NOT NULL DEFAULT 1,  -- Optimistic locking
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Transaction Ledger: Append-only, immutable audit trail
CREATE TABLE transaction_ledger (
    id TEXT PRIMARY KEY,                    -- ULID for sortability
    user_id TEXT NOT NULL,
    amount INTEGER NOT NULL,                -- Negative for debit, positive for credit
    balance_after INTEGER NOT NULL,         -- Snapshot for quick verification
    ref_type TEXT NOT NULL,                 -- 'BET', 'PAYOUT', 'DEPOSIT', 'WITHDRAWAL'
    ref_id TEXT NOT NULL,                   -- Foreign key to bets/deposits/etc
    metadata JSON,                          -- Flexible context
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_user_time (user_id, created_at),
    INDEX idx_ref (ref_type, ref_id)
) STRICT;

-- Bets table
CREATE TABLE bets (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    cell_id TEXT NOT NULL,
    amount INTEGER NOT NULL,
    reward_rate REAL NOT NULL,
    status TEXT NOT NULL,  -- 'pending', 'confirmed', 'won', 'lost', 'insufficient_funds'
    placed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP,
    payout_amount INTEGER,  -- NULL if lost
    
    FOREIGN KEY (user_id) REFERENCES users(id)
);
```

### Why `balance_after`?

The `balance_after` column in the ledger serves as a **checksum**:
```sql
-- Quick integrity verification per user
SELECT 
    l1.balance_after - l1.amount AS expected_balance_before,
    LAG(l1.balance_after) OVER (ORDER BY l1.created_at) AS actual_previous_balance
FROM transaction_ledger l1
WHERE l1.user_id = 'user_123'
ORDER BY l1.created_at;
```

### Batcher Implementation (Updated)

```go
type LedgerEntry struct {
    ID            string
    UserID        string
    Amount        int64   // Negative for bets
    BalanceAfter  int64   // New balance after this transaction
    RefType       string  // "BET"
    RefID         string  // Bet ID
    Metadata      map[string]interface{}
}

func (b *BetBatcher) flush(bets []PendingBet) {
    tx, err := b.db.BeginTx(context.Background(), &sql.TxOptions{
        Isolation: sql.LevelSerializable,
    })
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer tx.Rollback()

    // Prepare statements
    deductStmt, _ := tx.Prepare(`
        UPDATE users 
        SET balance = balance - ?, version = version + 1
        WHERE id = ? AND balance >= ?
        RETURNING balance
    `)
    
    ledgerStmt, _ := tx.Prepare(`
        INSERT INTO transaction_ledger 
        (id, user_id, amount, balance_after, ref_type, ref_id, metadata)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    `)
    
    betStmt, _ := tx.Prepare(`
        INSERT INTO bets (id, user_id, cell_id, amount, reward_rate, status)
        VALUES (?, ?, ?, ?, ?, ?)
    `)

    results := make(map[string]BetResult)

    for _, bet := range bets {
        // Step 1: Conditional deduction + get new balance
        var newBalance int64
        err := deductStmt.QueryRow(bet.Amount, bet.UserID, bet.BetID).Scan(&newBalance)
        
        if err == sql.ErrNoRows {
            // Insufficient funds
            results[bet.BetID] = BetResult{
                Success: false,
                Status:  StatusInsufficientFunds,
                Error:   ErrInsufficientFunds,
            }
            continue
        }
        if err != nil {
            results[bet.BetID] = BetResult{Success: false, Error: err}
            continue
        }

        // Step 2: Record in ledger (audit trail)
        ledgerID := generateULID()
        metadata := map[string]interface{}{
            "cell_id":     bet.CellID,
            "timestamp":   time.Now().Unix(),
        }
        metadataJSON, _ := json.Marshal(metadata)
        
        _, err = ledgerStmt.Exec(
            ledgerID,
            bet.UserID,
            -bet.Amount,        // Negative (debit)
            newBalance,         // Balance after deduction
            "BET",
            bet.BetID,
            metadataJSON,
        )
        if err != nil {
            results[bet.BetID] = BetResult{Success: false, Error: err}
            continue
        }

        // Step 3: Record bet
        _, err = betStmt.Exec(
            bet.BetID,
            bet.UserID,
            bet.CellID,
            bet.Amount,
            bet.RewardRate,
            StatusConfirmed,
        )
        if err != nil {
            results[bet.BetID] = BetResult{Success: false, Error: err}
            continue
        }

        results[bet.BetID] = BetResult{
            Success:    true,
            Status:     StatusConfirmed,
            NewBalance: newBalance,
        }
    }

    if err := tx.Commit(); err != nil {
        b.failAll(bets, err)
        return
    }

    // Signal results
    for _, bet := range bets {
        bet.Done <- results[bet.BetID]
    }

    // Best-effort Redis sync
    b.syncRedisBalances(bets, results)
}
```

## Performance Impact Analysis

| Aspect | Before (ADR-009) | After (ADR-010) | Impact |
|--------|------------------|-----------------|--------|
| Statements per bet | 2 (UPDATE + INSERT bets) | 3 (UPDATE + INSERT ledger + INSERT bets) | +1 INSERT |
| Batch of 100 | 200 statements | 300 statements | +50% statements |
| SQLite capacity | 10,000+ writes/sec | 10,000+ writes/sec | **Negligible** |
| Throughput @ 3K RPS | 30 batches/sec | 30 batches/sec | **Unchanged** |
| Storage per bet | ~200 bytes | ~350 bytes (+ledger) | +75% |
| Daily growth | 10 GB | 17.5 GB | Manageable with archival |

**Conclusion:** The ledger adds only 1 INSERT per bet. Since we're batching 100 bets per transaction, this is practically free performance-wise.

## Audit & Recovery Procedures

### 1. Daily Integrity Check

```sql
-- Verify users.balance matches SUM(ledger.amount)
WITH ledger_balances AS (
    SELECT 
        user_id,
        SUM(amount) as ledger_sum
    FROM transaction_ledger
    GROUP BY user_id
)
SELECT 
    u.id,
    u.balance as user_balance,
    lb.ledger_sum,
    u.balance - lb.ledger_sum as drift
FROM users u
LEFT JOIN ledger_balances lb ON u.id = lb.user_id
WHERE u.balance != lb.ledger_sum
   OR (u.balance = 0 AND lb.ledger_sum IS NULL AND u.id IN (SELECT user_id FROM transaction_ledger));
```

### 2. User Dispute Resolution

```sql
-- "Where did my money go?"
SELECT 
    created_at,
    ref_type,
    ref_id,
    amount,
    balance_after,
    metadata
FROM transaction_ledger
WHERE user_id = 'user_123'
ORDER BY created_at DESC
LIMIT 50;
```

### 3. Rebalance on Corruption

```sql
-- If drift detected, recalculate
UPDATE users 
SET balance = (
    SELECT COALESCE(SUM(amount), 0) 
    FROM transaction_ledger 
    WHERE user_id = users.id
)
WHERE id = 'user_123';
```

## Storage Growth Projection

| Timeframe | Ledger Size | With Archival Strategy |
|-----------|-------------|------------------------|
| 1 day | ~7.5 GB | Hot storage |
| 1 week | ~52 GB | Hot storage |
| 1 month | ~200 GB | S3 archival kicks in |
| Forever | Infinite | S3 STANDARD_IA |

**Note:** Ledger is archived with weekly partitions, same as bets table.

## Trade-off Analysis

| Alternative | Why Rejected |
|-------------|--------------|
| Mutable balance only | No audit trail, unanswerable disputes |
| Full event sourcing (no balance column) | Too slow for 3K RPS balance checks |
| Separate event store (Kafka) | Extra process, complexity, out of scope |
| Monthly ledger rotation | Too complex; weekly matches bet archival |

## NFR Compliance

| NFR | Compliance |
|-----|------------|
| 3K RPS | ‚úÖ Batching makes extra INSERT negligible |
| ‚â§100ms latency | ‚úÖ Unchanged (batch async) |
| Financial auditability | ‚úÖ Complete paper trail in ledger |
| Dispute resolution | ‚úÖ Query ledger by user/time/ref |
| Data integrity | ‚úÖ `balance_after` allows verification |
| Recovery | ‚úÖ Can rebuild balance from ledger |

## Decision Date
2026-02-03

## Author
System Architecture Guardian (Hardening Response 003)
----- END FILE: docs/adr/010-hybrid-transaction-ledger.md -----


----- BEGIN FILE: docs/diagrams/c4-container.md -----
# C4 Container Diagram (L2) ‚Äî BTC Tap Trading System

## Container View

```mermaid
C4Container
    title Container Diagram for BTC Tap Trading System

    Person(trader, "Trader", "Web3 wallet user placing directional bets")
    Person(admin, "Admin", "System operator monitoring health")

    System_Boundary(singleVM, "Single VM (4c/8GB/256GB)") {
        Container(nginx, "Nginx", "Nginx", "Reverse proxy, static file serving, TLS termination, rate limiting")
        
        Container(spa, "Frontend SPA", "React/Vue", "Grid-based betting interface, Web3 wallet integration")
        
        Container(api, "API Server", "Go + Echo/Fiber", "REST API for auth, bet placement, balances, history")
        Container(batcher, "Bet Batcher", "Go", "Channel-based batch writer with conditional SQL deduction + ledger write")
        Container(validator, "Signature Validator", "Go", "Web3 signature validation, replay protection")
        Container(cb, "Circuit Breaker", "Go", "Suspends betting on stale price feed")
        
        Container(ws, "WebSocket Server", "Go", "Real-time price streaming, bet confirmations")
        Container(wsAuth, "Session Manager", "Go", "Web3 session management")
        
        Container(priceIngest, "Price Ingestion", "Go", "External price feed consumer")
        Container(healthMon, "Price Health Monitor", "Go", "Tracks price freshness, triggers circuit breaker")
        Container(resolver, "Bet Resolver", "Go", "Background worker for bet resolution")
        Container(archiver, "Weekly Archiver", "Go + bash", "Automated S3 archival of old bets")
        
        ContainerDb(sqliteRW, "SQLite (Current Week)", "SQLite 3 (WAL mode)", "Bets + transaction_ledger, read-write")
        ContainerDb(sqliteRO, "SQLite (Historical)", "SQLite 3 (Read-Only)", "Previous 4 weeks bets + ledger, read replica")
        ContainerDb(redis, "Redis", "Redis 7", "Sessions, nonces, price cache, pub/sub, rate limiting")
    }
    
    System_Ext(priceFeed, "Crypto Price Feed", "Binance/Coinbase WebSocket API")
    System_Ext(s3, "S3 Cold Storage", "AWS S3 STANDARD_IA for archived bet history")

    %% User interactions
    Rel(trader, nginx, "Accesses web app", "HTTPS")
    Rel(admin, nginx, "Monitors system", "HTTPS")
    
    Rel(nginx, spa, "Serves static files", "HTTP")
    Rel(nginx, api, "Routes API requests", "HTTP :8080")
    Rel(nginx, ws, "Routes WebSocket", "WebSocket :8081")
    
    %% Frontend interactions
    Rel(spa, api, "Places bets via", "HTTPS/JSON")
    Rel(spa, ws, "Receives real-time updates via", "WebSocket")
    
    %% API flow
    Rel(api, validator, "Validates signatures via", "function call")
    Rel(api, cb, "Checks health status via", "function call")
    Rel(api, batcher, "Submits bets to", "channel")
    Rel(api, redis, "Checks sessions, nonces, balances", "Redis Protocol")
    
    Rel(validator, redis, "Checks nonce uniqueness", "SET NX")
    Rel(cb, healthMon, "Monitors", "function call")
    
    Rel(batcher, sqliteRW, "Conditional UPDATE + INSERT", "SQL/sqlite3")
    
    %% WebSocket flow
    Rel(ws, wsAuth, "Authenticates via", "function call")
    Rel(wsAuth, redis, "Validates sessions", "Redis Protocol")
    Rel(ws, redis, "Subscribes to price pub/sub", "Redis Pub/Sub")
    
    %% Price ingestion flow
    Rel(priceIngest, priceFeed, "Streams prices from", "WebSocket")
    Rel(priceIngest, redis, "Publishes prices to", "Redis Pub/Sub")
    Rel(priceIngest, healthMon, "Updates health status", "function call")
    
    %% Resolution flow
    Rel(resolver, redis, "Queries price history from", "Redis Protocol")
    Rel(resolver, sqliteRW, "Resolves bets via", "SQL/sqlite3")
    Rel(resolver, sqliteRO, "Queries historical data from", "SQL/sqlite3 (read-only)")
    
    %% Archival flow
    Rel(archiver, sqliteRW, "Exports old data from", "SQL/sqlite3")
    Rel(archiver, s3, "Uploads compressed archives to", "HTTPS/S3 API")
    Rel(archiver, sqliteRO, "Attaches as read replica", "SQL ATTACH")

    UpdateElementStyle(trader, $fontColor="#fff", $bgColor="#08427b")
    UpdateElementStyle(admin, $fontColor="#fff", $bgColor="#08427b")
    UpdateElementStyle(spa, $bgColor="#438dd5", $fontColor="#fff")
    UpdateElementStyle(api, $bgColor="#438dd5", $fontColor="#fff")
    UpdateElementStyle(ws, $bgColor="#438dd5", $fontColor="#fff")
    UpdateElementStyle(sqliteRW, $bgColor="#438dd5", $fontColor="#fff")
    UpdateElementStyle(sqliteRO, $bgColor="#438dd5", $fontColor="#fff")
    UpdateElementStyle(redis, $bgColor="#438dd5", $fontColor="#fff")
```

---

## Container Descriptions

| Container | Technology | Responsibility |
|-----------|------------|----------------|
| **Frontend SPA** | React or Vue | Grid-based betting UI, Web3 wallet integration, real-time price display |
| **Nginx** | Nginx | Static file serving, reverse proxy, TLS termination, rate limiting, connection limits |
| **API Server** | Go (Echo/Fiber) | REST API for authentication, bet placement, user balances, bet history queries |
| **Bet Batcher** | Go (channel-based) | Buffers bets, conditional SQL deduction, writes to transaction_ledger, flushes in batches |
| **Signature Validator** | Go | Validates EIP-191 Web3 signatures, checks timestamp freshness and nonce uniqueness |
| **Circuit Breaker** | Go middleware | Monitors price feed health, suspends betting when stale (>10s) |
| **WebSocket Server** | Go | Real-time price streaming to clients, bet confirmation events, system status broadcasts |
| **Session Manager** | Go | Web3 session validation and management |
| **Price Ingestion** | Go | Connects to external price feed, normalizes and publishes to Redis |
| **Price Health Monitor** | Go | Tracks price freshness, updates circuit breaker status |
| **Bet Resolver** | Go | Background worker scanning expired bets, resolving wins/losses using price history |
| **Weekly Archiver** | Go + bash | Exports old bet partitions, compresses with zstd, uploads to S3 |
| **SQLite (Current Week)** | SQLite 3 (WAL mode) | Active week: `bets`, `transaction_ledger`, `users` tables (read-write), ~18GB |
| **SQLite (Historical)** | SQLite 3 (read-only) | Previous 4 weeks (bets + ledger), attached read replica, ~72GB |
| **Redis** | Redis 7 | Sessions (24h TTL), nonce tracking (90s TTL), price cache (1h), price history (7d), pub/sub, rate limiting |

---

## Key Relationships

| Source | Destination | Description | Protocol/Mechanism |
|--------|-------------|-------------|-------------------|
| Trader ‚Üí Nginx | | Accesses web application | HTTPS |
| Nginx ‚Üí SPA | | Serves static assets | HTTP |
| Nginx ‚Üí API Server | | Routes API requests | HTTP :8080 |
| Nginx ‚Üí WebSocket Server | | Routes WebSocket connections | WebSocket :8081 |
| Frontend SPA ‚Üí API Server | | Places bets | HTTPS/JSON |
| Frontend SPA ‚Üí WebSocket Server | | Receives real-time updates | WebSocket |
| API Server ‚Üí Signature Validator | | Validates Web3 signatures | Function call |
| API Server ‚Üí Circuit Breaker | | Checks price health status | Function call |
| API Server ‚Üí Bet Batcher | | Submits bets for batching | Go channel |
| API Server ‚Üí Redis | | Session, nonce, balance checks | Redis Protocol |
| Bet Batcher ‚Üí SQLite (Current) | | UPDATE balance + INSERT ledger + INSERT bet | SQL/sqlite3 |
| WebSocket Server ‚Üí Redis | | Subscribes to price pub/sub | Redis Pub/Sub |
| Price Ingestion ‚Üí Price Feed | | Streams real-time prices | WebSocket |
| Price Ingestion ‚Üí Redis | | Publishes normalized prices | Redis Pub/Sub |
| Bet Resolver ‚Üí SQLite (Current) | | Resolves pending bets | SQL/sqlite3 |
| Bet Resolver ‚Üí SQLite (Historical) | | Queries historical bet data | SQL (read-only) |
| Weekly Archiver ‚Üí S3 | | Uploads compressed archives | HTTPS/S3 API |

---

## Hardening Measures

### 1. Write Batching (ADR-005)
- **Before:** 3,000 individual SQLite transactions/sec ‚Üí contention
- **After:** 30 batched transactions/sec (100 items or 100ms)
- **Impact:** Sustainable 3K RPS with <50ms latency

### 2. Time-Based Partitioning (ADR-006)
- Current week: `bets_yyyy_ww.db` (read-write, ~10GB)
- Historical: 4 weeks attached read-only (~40GB)
- Archived: zstd compressed to S3 STANDARD_IA (infinite retention)

### 3. Replay Protection (ADR-007)
- Timestamp freshness: 30-second window
- Nonce uniqueness: Redis SET NX with 90s TTL (was 24h, optimized in v1.2)
- Chain ID binding: Prevents cross-chain replay

### 4. Stale Price Circuit Breaker (ADR-008)
- Health threshold: 3s = DEGRADED, 10s = CRITICAL
- CRITICAL state: All bet placement rejected with 503
- Recovery: Only after 30s of stable updates (hysteresis, v1.2)

### 5. Read Replica for Resolution
- Bet Resolver uses read-only connection for historical queries
- Eliminates read-write contention during resolution
- Covering index on `(target_time, min_price, max_price)`

### 6. Conditional Balance Deduction (ADR-009) ‚Äî v1.2
- **Problem:** Redis balance check + SQLite batch deduction = double-spend race
- **Solution:** Database as single source of truth
- **Implementation:** `UPDATE users SET balance = balance - ? WHERE id = ? AND balance >= ?`
- **Impact:** RowsAffected = 0 means insufficient funds, even if Redis cache said otherwise

### 7. Optimized Nonce TTL ‚Äî v1.2
- **Before:** 24h TTL = ~260M keys = ~13GB RAM at 3K RPS
- **After:** 90s TTL (30s window + 60s buffer) = ~270K keys = ~13MB RAM
- **Impact:** 1000√ó memory reduction

### 8. Circuit Breaker Hysteresis ‚Äî v1.2
- **Enter CRITICAL:** Immediate at >10s stale
- **Recover to HEALTHY:** Only after 30s of stable updates
- **Impact:** Prevents flapping on unstable feed

### 9. UX Strictness ‚Äî v1.2
- **Pending state:** Pulsing animation until WebSocket confirmation
- **Confirmed state:** Solid color after `bet:confirmed` event
- **Unknown state:** Error if disconnect during pending
- **Impact:** Prevents phantom bet perception


### 10. Hybrid Transaction Ledger (ADR-010) ‚Äî v1.3
- **Problem:** Mutable balance has no audit trail; disputes unanswerable
- **Solution:** Append-only `transaction_ledger` table written in same transaction
- **Implementation:** 
  - `UPDATE users` (deduct)
  - `INSERT INTO transaction_ledger` (audit: amount, balance_after, ref_type, ref_id)
  - `INSERT INTO bets` (record)
- **Impact:** Complete paper trail; can rebuild balance from ledger
- **Performance:** +1 INSERT per bet (negligible with batching)
- **Storage:** ~75% increase (200B ‚Üí 350B per bet), manageable with archival
----- END FILE: docs/diagrams/c4-container.md -----


----- BEGIN FILE: docs/diagrams/c4-context.md -----
# C4 Context Diagram (L1) ‚Äî BTC Tap Trading System

## System Context

```mermaid
C4Context
    title System Context for BTC Tap Trading Application

    Person(trader, "Trader", "Web3 wallet user placing directional bets on BTC price movements")
    Person(admin, "Admin/Operator", "System monitor resolving disputes and monitoring health")
    
    System(tapTrading, "BTC Tap Trading System", "Allows users to place real-time directional bets on BTC price movements via a grid-based interface")
    
    System_Ext(priceFeed, "Crypto Price Feed", "External WebSocket API (Binance/Coinbase) providing real-time BTC/USD price updates")
    
    Rel(trader, tapTrading, "Places bets via Web UI", "HTTPS/WebSocket")
    Rel(trader, tapTrading, "Authenticates with", "Web3 Wallet Signature")
    Rel(admin, tapTrading, "Monitors metrics and health", "HTTPS")
    Rel(tapTrading, priceFeed, "Streams real-time BTC prices from", "WebSocket")
    
    UpdateElementStyle(trader, $fontColor="#fff", $bgColor="#08427b")
    UpdateElementStyle(admin, $fontColor="#fff", $bgColor="#08427b")
    UpdateElementStyle(tapTrading, $bgColor="#1168bd", $fontColor="#fff")
    UpdateElementStyle(priceFeed, $bgColor="#999999", $fontColor="#fff")
```

### Diagram Description

| Element | Type | Description |
|---------|------|-------------|
| **Trader** | Person | End-user with Web3 wallet placing directional bets on BTC price movements via grid interface |
| **Admin/Operator** | Person | Internal team member monitoring system health, resolving disputes |
| **BTC Tap Trading System** | System | The complete betting application including API, WebSocket, background workers, and data stores |
| **Crypto Price Feed** | External System | Third-party WebSocket API providing real-time BTC/USD price updates |

### Relationships

| Source | Target | Description | Protocol |
|--------|--------|-------------|----------|
| Trader | BTC Tap Trading System | Places bets via Web UI | HTTPS / WebSocket |
| Trader | BTC Tap Trading System | Authenticates with Web3 wallet | Web3 Signature |
| Admin | BTC Tap Trading System | Monitors metrics and system health | HTTPS |
| BTC Tap Trading System | Crypto Price Feed | Subscribes to real-time price stream | WebSocket |
----- END FILE: docs/diagrams/c4-context.md -----


----- BEGIN FILE: docs/hardening/001_initial_critique.md -----
# Architecture Hardening Audit: BTC Tap Trading

**Date:** 2026-02-03
**Reviewer:** System Architecture Guardian (Hardening Skill)
**Status:** CHALLENGING DESIGN

## Executive Summary
The current design (Single VM, SQLite, Go) is theoretically capable of meeting the 3,000 RPS target *if* the workload is read-heavy. However, the "Tap" nature implies a high write urgency. The primary risks are **SQLite write contention** at peak load and **disk space exhaustion** due to the "Forever" retention policy on a small SSD.

---

## üî® Risk Analysis & Critiques

### 1. Data & State Management (High Risk)

**Critical Vulnerability: SQLite Write Contention at 3k RPS**
- **Context:** SQLite allows only *one* writer at a time, even in WAL mode.
- **Attack Vector:** If 3,000 users "tap" simultaneously (3k writes/sec), distinct transactions will queue. If a transaction takes > 0.3ms (very easy with disk I/O, strict durability, or potential index updates), the queue backs up, latency spikes > 100ms (NFR violation), and the app may crash with `SQLITE_BUSY`.
- **NFR Impact:** Latency (p99), Availability.
- **Suggested Hardening:**
    - **Batching:** The API Server *must* buffer taps in memory (e.g., channel) and flush to SQLite in batches (100-500ms windows or 50-item batches). This reduces transaction overhead drastically.
    - **Optimistic UI:** Client assumes success; Server responds async via WebSocket if failed (though this conflicts with "No confirmation dialogs").
    - **Stress Test Request:** Must verify `fsync` time on the target SSD.

**Critical Vulnerability: "Forever" Retention vs. 256GB SSD**
- **Context:** PRD requires "Bet history retained indefinitely".
- **Attack Vector:**
    - Assumed object size: 200 bytes per bet (UUID, UserID, Timestamp, Price, Amount, Status, Metadata).
    - Traffic: 3,000 RPS * 20% write ratio (guess) = 600 bets/sec.
    - Daily Volume: 600 * 86400 * 200 bytes ‚âà 10 GB/day.
    - **Time to Death:** ~25 days. The system will fail before the first month is over.
- **NFR Impact:** Availability (Disk Full = Outage).
- **Suggested Hardening:**
    - **Pruning Policy:** Challenge the "Forever" requirement on the hot node.
    - **Cold Storage:** Implement daily `rsync` of older SQLite partitions or CSV exports to S3 (or external storage), then `DELETE` from main DB. SQLite `VACUUM` will be needed (which blocks operation!).
    - **Solution:** Use **auto-vacuum = incremental** or partition DBs by week.

### 2. Failure, Resilience & Degradation (Medium Risk)

**Vulnerability: Betting vs. Resolution Contention**
- **Context:** A "Bet Resolver" worker runs on the same VM/DB.
- **Attack Vector:** Resolution likely requires a "Range Scan" (Find all bets in Time Column X). Using the same SQLite file for aggressive reads (Resolution) + aggressive writes (Taps) creates lock contention.
- **NFR Impact:** Latency.
- **Suggested Hardening:**
    - **Read Replica:** Can we use a Read-Only connection for the Resolver? (SQLite WAL supports this well).
    - **Index Strategy:** Ensure covering index on `(target_time_column, min_price, max_price)` to avoid table scans during resolution.

**Vulnerability: Thundering Herd on WS Reconnect**
- **Context:** Single VM means deployments or crashes drop 100% of WebSocket connections.
- **Attack Vector:** 10,000 users reconnect simultaneously. Go server handles connections, but the validation logic (Session check in Redis) might spike Redis CPU.
- **Suggested Hardening:**
    - **Jitter:** Client *must* implement randomized exponential backoff (0-5s).
    - **Shedding:** Nginx must limit total concurrent connections if exceeding 4 core capacity.

### 3. Security & Abuse (Medium Risk)

**Vulnerability: Replay Attacks on Web3 Auth**
- **Context:** No password, signature only.
- **Attack Vector:** Attacker captures a valid "Place Bet" payload + signature and replays it 1000 times to drain user funds or grief the system.
- **Suggested Hardening:**
    - **Nonce/Timestamp:** Every signed message *must* include a timestamp (< 30s diff) and a nonce, tracked in Redis (with TTL) to ensure uniqueness.

### 4. Observability (The 3 A.M. Test)

**Missing: "Stale Price" Circuit Breaker**
- **What happens if the price feed stops?**
    - Do users keep betting on old prices?
    - Do resolutions happen with old data?
- **Suggested Hardening:** Application *must* assume UNHEALTHY if no price tick received in 2x expected interval (e.g., 2 seconds). All betting suspended automatically.

## Summary of Required Changes for Acceptance
1.  **Mandatory:** Implement Write Batching for Bets. Single-row inserts will not survive 3k RPS.
2.  **Mandatory:** Define an Archival Strategy immediately (256GB is insufficient).
3.  **Mandatory:** Add Replay Protection (Nonce/Timestamp) to signature payloads.
4.  **Mandatory:** "Stale Price" Circuit Breaker.
----- END FILE: docs/hardening/001_initial_critique.md -----


----- BEGIN FILE: docs/hardening/001_resolution.md -----
# Hardening Resolution: 001_initial_critique.md

**Date:** 2026-02-03  
**Status:** ALL CRITICAL ISSUES RESOLVED  
**Architecture Version:** v1.1 (Hardened)

---

## Summary

All four mandatory hardening requirements have been addressed with concrete implementations:

| # | Requirement | Status | ADR | Key Change |
|---|-------------|--------|-----|------------|
| 1 | Write Batching | ‚úÖ RESOLVED | ADR-005 | Channel-based batcher (100 items/100ms) |
| 2 | Archival Strategy | ‚úÖ RESOLVED | ADR-006 | Weekly partitions + S3 cold storage |
| 3 | Replay Protection | ‚úÖ RESOLVED | ADR-007 | Timestamp + Nonce + Redis |
| 4 | Stale Price CB | ‚úÖ RESOLVED | ADR-008 | Auto-suspend betting at 10s stale |

---

## Detailed Resolution

### 1. Write Batching for Bets (MANDATORY)

**Problem:** SQLite single-writer limitation causes queue buildup at 3K RPS.

**Solution:** In-memory batching with channel-based aggregation.

```
Before: 3,000 individual transactions/sec ‚Üí SQLite contention ‚Üí SQLITE_BUSY
After:  30 batched transactions/sec       ‚Üí Sustainable ‚Üí <50ms latency
```

**Implementation:**
- Buffered channel (10,000 items) for backpressure
- Flush triggers: 100 items OR 100ms timeout
- Optimistic UI: 202 Accepted immediately, WebSocket confirms
- Single writer goroutine eliminates lock contention

**Trade-off:** Max 100ms of unconfirmed bets at risk on crash (acceptable).

---

### 2. Archival Strategy (MANDATORY)

**Problem:** 256GB SSD fills in ~25 days at 10GB/day growth.

**Solution:** Time-based partitioning with automated S3 archival.

```
Storage Layout:
‚îú‚îÄ‚îÄ bets_current.db      (current week, read-write, ~10GB)
‚îú‚îÄ‚îÄ bets_2025_w05.db     (previous weeks, read-only, ~40GB total)
‚îú‚îÄ‚îÄ bets_2025_w04.db.zst (S3 STANDARD_IA, forever retention)
‚îî‚îÄ‚îÄ ...
```

**Weekly Archival Process:**
1. Sunday 00:00: VACUUM INTO new file
2. Sunday 00:30: zstd -19 compression
3. Sunday 01:00: Upload to S3 STANDARD_IA
4. Sunday 01:30: Attach as read-only for queries
5. +28 days: Purge local compressed file

**Result:** ~50GB hot storage, ~200GB headroom, infinite retention.

---

### 3. Replay Protection (MANDATORY)

**Problem:** Attacker can replay signed bet payloads to drain funds.

**Solution:** EIP-191 signed messages with timestamp + nonce.

**Payload Schema:**
```
Tap Trading Bet
Action: place_bet
Cell: {cell_id}
Amount: {amount}
Nonce: {uuid}
Timestamp: {unix_seconds}
Chain: {chain_id}
Version: 1
```

**Validation:**
1. Timestamp within 30-second window
2. Nonce uniqueness via Redis SET NX (24h TTL)
3. Signer address recovery and match
4. Chain ID binding (prevents cross-chain replay)

---

### 4. Stale Price Circuit Breaker (MANDATORY)

**Problem:** Price feed failure causes betting on stale data.

**Solution:** Automatic health monitoring with circuit breaker.

**Thresholds:**
| Age | Status | Action |
|-----|--------|--------|
| <3s | HEALTHY | Normal operation |
| 3-10s | DEGRADED | Warning banner, continue |
| >10s | CRITICAL | Reject all bets with 503 |

**Behavior:**
- Health monitor subscribed to price pub/sub
- Circuit breaker middleware on bet endpoints
- Bet resolution paused if unhealthy
- WebSocket broadcasts status to clients
- Automatic recovery when feed resumes

---

## Additional Improvements

### Read Replica for Resolution
- Bet Resolver uses separate read-only SQLite connection
- Eliminates read-write contention
- Covering index on `(target_time, min_price, max_price)`

### Thundering Herd Mitigation
- Client: Randomized exponential backoff (0-5s) on reconnect
- Nginx: `limit_conn` prevents connection exhaustion

---

## Updated Diagrams

- `docs/diagrams/c4-container.md` ‚Äî Updated with batcher, circuit breaker, archiver
- `current_state.md` ‚Äî Updated with hardened architecture

## New ADRs

- `docs/adr/005-write-batching-for-bet-ingestion.md`
- `docs/adr/006-archival-strategy-for-bet-history.md`
- `docs/adr/007-replay-protection-for-web3-auth.md`
- `docs/adr/008-stale-price-circuit-breaker.md`

---

## Sign-off

**Reviewer:** System Architecture Guardian  
**Verdict:** ‚úÖ **ACCEPTED FOR IMPLEMENTATION**

All critical vulnerabilities addressed. Architecture is now suitable for 3K RPS, 99.5% SLA, and indefinite retention within 256GB constraint.
----- END FILE: docs/hardening/001_resolution.md -----


----- BEGIN FILE: docs/hardening/002_followup_critique.md -----
# Architecture Hardening Audit: Round 002

**Date:** 2026-02-03
**Context:** Review of "Hardened v1.1" (Batching, Archival, Replay Protection)
**Status:** RISKS IDENTIFIED (OPERATIONAL)

## Executive Summary
The "Hardened v1.1" design successfully mitigates the catastrophic scalability flaws of v1.0. The system can now theoretically handle the 3,000 RPS load.
However, the introduction of **Batching** and **Archival** creates new *Operational Risks* and *Edge Cases* that must be handled to prevent "Silent Failures".

---

## üî® Remaining Risks & Edge Cases

### 1. Data Loss vs. User Trust (The "Optimistic UI" Trap)
**Risk:** High
**Context:** ADR-005 accepts "Max 100ms of unconfirmed bets at risk". The Client uses "Optimistic UI" (shows "Bet Placed" immediately).
**Failure Mode:**
1.  User Taps. UI shows "Success".
2.  Server receives HTTP request, puts in Channel.
3.  Server crashes (OOM/Panic) *before* the 100ms flush.
4.  **Result:** User thinks they have a bet. The system has no record of it. User wins, demands payout. System logs show nothing.
**Mitigation:**
-   **Client State Handling:** The Client **MUST NOT** show a permanent "Confirmed" state until the WebSocket sends the `OrderConfirmed` event.
-   **UX:** Use a "Pending" animation (e.g., pulsing border) for the first ~200ms. Only turn solid Green/Gold upon WS confirmation.
-   **Recovery:** If socket disconnects while "Pending", transition to "Unknown/Error" and prompt user to refresh.

### 2. Redis Memory Policy & Eviction
**Risk:** Medium
**Context:** Redis is now the Single Point of Failure for Auth (Sessions), Integrity (Nonces), and Data (Price History).
**Failure Mode:**
-   7 Days of Price History + millions of Nonces + Sessions fill the allocated Redis memory.
-   Redis Eviction kicks in or OOM killer kills it.
-   **Scenario:** Redis evicts `price_history` keys to make room for `nonces`.
-   **Result:** Bet Resolver fails to find historical prices ‚Üí Resolution stalls or errors.
**Mitigation:**
-   **Memory Sizing:** Explicit calculation needed. 7 days @ 1 tick/sec = ~60MB. Nonces @ 3k RPS * 24h is huge.
    -   *Correction:* Nonces are only needed for the 30s window? No, replay protection usually needs to track nonces for the validity window.
    -   *If* validity window is 30s (ADR-007), we only need to store nonces for 30s! Storing for 24h is wasteful unless the window is 24h.
    -   *Action:* **Reduce Nonce TTL to match the Timestamp Window (e.g., 60s safety buffer).** This saves GBs of RAM.

### 3. Archival Job "Silent Death"
**Risk:** Medium
**Context:** The "Infinite Retention" promise relies entirely on the `Weekly Archiver` script working perfectly every Sunday.
**Failure Mode:**
-   Script fails (e.g., S3 creds expired, disk full during compression).
-   System continues running.
-   Week 2: Fails again.
-   Week 4: Disk fills up with un-archived SQLite files.
-   **Result:** Hard outage.
**Mitigation:**
-   **Dead Man's Switch:** The Archiver must push a metric `last_successful_archive_timestamp`. Alert if `now - last_success > 8 days`.

### 4. Financial Integrity: The Double-Spend Race Condition
**Risk:** CRITICAL
**Context:** ADR-005 describes checking balances in Redis ("fast") but deducting in SQLite ("atomic").
**Failure Mode:**
-   User has $10.
-   User sends two simultaneous $10 bets.
-   **Request A:** Checks Redis ($10 > $10). OK. Enters Batch.
-   **Request B:** Checks Redis ($10 > $10). OK. Enters Batch.
-   **Batch Execution:** Inserts Bet A. Inserts Bet B.
-   **Result:** User spends $20. System assumes balances are updated, but if the Batcher doesn't *strictly* prevent the second deduction, we have a negative balance or data corruption.
**Mitigation:**
-   **Database as Truth:** The Batcher Transaction **MUST** perform the deduction: `UPDATE users SET balance = balance - diff WHERE id = ? AND balance >= diff`.
-   **Logic:** If `RowsAffected == 0`, the specific bet in the batch **MUST FAIL**, even if it passed the API check.
-   **Recovery:** On startup/crash recovery, Redis balances must be rebuilt/verified against the SQLite state.

### 5. Flapping Circuit Breaker
**Risk:** Low
**Context:** `Stale Price` check triggers at >10s. Recovery is "Automatic".
**Failure Mode:**
-   Feed is unstable (updates every 9s, then 11s, then 9s).
-   System toggles Healthy/Critical repeatedly.
-   User experience is jarring (Betting disabled... enabled... disabled).
**Mitigation:**
-   **Hysteresis:** Enter CRITICAL at >10s. Recover to HEALTHY only after **30s of stable** updates (Continuous Green).

---

## Recommendations for v1.2

1.  **UX Strictness:** Enforce "Pending" state in UI until WS Confirmation. logic.
2.  **Config Tuning:** Change Nonce TTL from 24h to **90 seconds** (Timestamp window + buffer).
3.  **Observability:** Add `last_archive_success` metric.
4.  **Stability:** Add 30s Hysteresis to Circuit Breaker recovery.

## Verdict
**Passed with Comments.**
The architecture is solid. These are implementation details but are critical for operational stability.
Proceed to Implementation Plan.
----- END FILE: docs/hardening/002_followup_critique.md -----


----- BEGIN FILE: docs/hardening/002_resolution.md -----
# Hardening Resolution: 002_followup_critique.md

**Date:** 2026-02-03  
**Status:** ALL RISKS ADDRESSED  
**Architecture Version:** v1.2 (Operationally Hardened)

---

## Summary

All 5 operational risks from Round 002 have been addressed:

| # | Risk | Severity | Solution | ADR/Config |
|---|------|----------|----------|------------|
| 1 | Optimistic UI Trust Gap | High | "Pending" UX until WS confirm | UX Spec |
| 2 | Redis Nonce Memory Bloat | Medium | TTL 24h ‚Üí 90s | Config |
| 3 | Archival Silent Death | Medium | Dead man's switch metric | Ops |
| 4 | **Double-Spend Race** | **CRITICAL** | **Conditional SQL deduction** | **ADR-009** |
| 5 | Circuit Breaker Flapping | Low | 30s hysteresis on recovery | Config |

---

## Detailed Resolution

### 1. Data Loss vs. User Trust (Optimistic UI)

**Problem:** User sees "Success" but server crashes before batch flush ‚Üí phantom bet.

**Solution:** Strict UX state machine

```
User Tap
    ‚Üì
Grid Cell: "PENDING" (pulsing animation)
    ‚Üì
WebSocket receives "bet:confirmed"
    ‚Üì
Grid Cell: "CONFIRMED" (solid color)
    ‚Üì
WebSocket disconnects during pending
    ‚Üì
Grid Cell: "UNKNOWN" (error state, prompt refresh)
```

**Implementation:** Client MUST NOT persist "confirmed" until WebSocket event received.

---

### 2. Redis Memory Policy (Nonce TTL)

**Problem:** 24h nonce TTL √ó 3K RPS = ~260M keys = ~13GB RAM.

**Solution:** Reduce TTL to match timestamp window + buffer.

```
Timestamp window: 30 seconds
Safety buffer:    60 seconds
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Nonce TTL:        90 seconds
```

**Impact:** ~270K keys max = ~13MB RAM (1000√ó reduction).

**Config:**
```bash
SET nonce:{uuid} {user_id} EX 90 NX
```

---

### 3. Archival Job Silent Death

**Problem:** Weekly archiver fails silently ‚Üí disk fills ‚Üí outage.

**Solution:** Dead man's switch metric.

**Implementation:**
```go
// In Weekly Archiver
func (a *Archiver) Run() error {
    // ... archival logic ...
    
    // Success: update timestamp
    a.metrics.GaugeSet("last_successful_archive_timestamp", time.Now().Unix())
    return nil
}
```

**Alert:**
```yaml
# Prometheus AlertManager
- alert: ArchiverStale
  expr: time() - last_successful_archive_timestamp > 8 * 24 * 3600
  severity: critical
  summary: Weekly archiver hasn't run in 8+ days
```

---

### 4. Double-Spend Race Condition (CRITICAL)

**Problem:** Redis balance check + SQLite batch deduction = race window.

**Solution:** Database as single source of truth.

```sql
-- Conditional deduction (atomic)
UPDATE users 
SET balance = balance - ?
WHERE id = ? 
AND balance >= ?;  -- <-- This prevents overdraft

-- Check RowsAffected
-- 0 = insufficient funds (reject)
-- 1 = success (insert bet)
```

**Flow:**
```
1. Redis check: Fast early rejection (cache, not authoritative)
2. Submit to batcher
3. Batcher transaction:
   a. UPDATE users (conditional)
   b. IF success ‚Üí INSERT bet
   c. IF fail ‚Üí INSERT bet with status='insufficient_funds'
4. Commit
5. Best-effort Redis cache update
```

**Recovery:** On startup, rebuild Redis cache from SQLite:
```go
SELECT id, balance FROM users;
// Pipeline SET to Redis
```

**See:** `docs/adr/009-conditional-balance-deduction.md`

---

### 5. Flapping Circuit Breaker

**Problem:** Unstable feed (9s, 11s, 9s) causes rapid Healthy/Critical toggling.

**Solution:** Hysteresis on recovery.

```go
const (
    DegradedThreshold  = 3 * time.Second
    CriticalThreshold  = 10 * time.Second
    RecoveryStability  = 30 * time.Second  // NEW
)

type CircuitBreaker struct {
    status          Status
    lastHealthyTime time.Time
}

func (cb *CircuitBreaker) Evaluate(age time.Duration) {
    switch {
    case age > CriticalThreshold:
        cb.status = CRITICAL
        cb.lastHealthyTime = time.Time{}  // Reset
        
    case age < DegradedThreshold && cb.status == CRITICAL:
        // Only recover after 30s of stable updates
        if cb.lastHealthyTime.IsZero() {
            cb.lastHealthyTime = time.Now()
        } else if time.Since(cb.lastHealthyTime) > RecoveryStability {
            cb.status = HEALTHY
        }
        
    case age > DegradedThreshold:
        cb.status = DEGRADED
        cb.lastHealthyTime = time.Time{}  // Reset
    }
}
```

**Behavior:**
| Transition | Delay |
|------------|-------|
| HEALTHY ‚Üí CRITICAL | Immediate (at 10s) |
| CRITICAL ‚Üí HEALTHY | 30s of stable updates |

---

## Verdict

**Reviewer:** System Architecture Guardian  
**Verdict:** ‚úÖ **ACCEPTED FOR IMPLEMENTATION**

All operational risks addressed. Architecture v1.2 is production-ready for the 2-week timeline.

### Final Checklist

- [ ] ADR-009 implemented (conditional balance deduction)
- [ ] Nonce TTL configured to 90 seconds
- [ ] Dead man's switch metric + alerting configured
- [ ] Circuit breaker hysteresis (30s) implemented
- [ ] UX "Pending" state spec delivered to frontend team
- [ ] Redis cache rebuild on startup implemented
----- END FILE: docs/hardening/002_resolution.md -----


----- BEGIN FILE: docs/hardening/003_followup_critique.md -----
# Architecture Hardening Audit: Round 003

**Date:** 2026-02-03
**Context:** Final Review of "Hardened v1.2" & Analysis of Financial Persistence Model
**Status:** RISKS IDENTIFIED (FINANCIAL AUDITABILITY)

## Executive Summary
The system is operationally solid (3K RPS, Anti-Double-Spend, Archival).
However, the **Financial Integrity Model** (ADR-009) relies on a **Mutable Balance Column** as the Source of Truth.
The User's critique is valid: In financial systems, *Events* (Ledger) should be the source of truth to allow reconstruction.
Relying solely on `UPDATE users SET balance = balance - x` risks "Drift" that cannot be debugged.

---

## üî® Risk Analysis: Mutable Balance vs. Event Ledger

### The Current Approach (ADR-009)
- **Mechanism:** `UPDATE users SET balance = balance - 10`.
- **Pros:** Extremely fast, simple, low storage.
- **Cons:** **Destructive Write.** If a bug causes a double-deduction or a manual DB edit happens, the previous state is lost forever. There is no "Paper Trail" to prove *why* a user has $50.

### The User's Suggestion (Event Sourcing)
- **Mechanism:** Store `BalanceDelta` (+10, -10). Replay to get Balance.
- **Pros:** Perfect auditability.
- **Cons:** **Scaling.** Replaying 100,000 bets to check if a user can afford a $1 bet at 3,000 RPS is impossible on SQLite without heavy snapshots/caching.

### The Hybrid "V2" Solution (The Missing Piece)
We can satisfy *both* constraints (Speed + Auditability) without full Event Sourcing complexity.

**Recommendation:** Introduce a **`transaction_ledger`** table.
1.  **Strict Rule:** *Every* balance change must have a corresponding Row in `transaction_ledger`.
2.  **Implementation:** The Batcher inserts *both* the Bet AND a Ledger Entry in the same transaction.
    ```sql
    BEGIN;
    -- 1. Deduct (Speed Constraint)
    UPDATE users SET balance = balance - 10 WHERE id = 1 AND balance >= 10;
    
    -- 2. Audit (Integrity Constraint)
    INSERT INTO transaction_ledger (user_id, amount, ref_type, ref_id) 
    VALUES (1, -10, 'BET', 'bet_uuid_123');
    
    -- 3. Record Bet
    INSERT INTO bets ...
    COMMIT;
    ```
3.  **Result:** `users.balance` is a "Cached Snapshot" of the Sum of `transaction_ledger`.
4.  **Recovery:** If data corruption is suspected, we can `SUM(amount)` from the ledger to verify/fix the `users` table.

---

## üîç Other Hardening Checks (Round 3)

### 1. UX "Pending" State
- **Status:** **PASS**. The new UX spec requiring WebSocket confirmation before "Solid Green" solves the trust gap.

### 2. Redis Memory (Nonce TTL)
- **Status:** **PASS**. Reducing TTL to 90s fixes the OOM risk.

### 3. Circuit Breaker Hysteresis
- **Status:** **PASS**. The 30s stability window prevents flapping.

---

## Final Recommendations for Implementation
1.  **Adopt the Hybrid Ledger:** Do not Rely on Mutable Balance alone. Add a `transaction_ledger` table.
    -   *Constraint Check:* Does this kill performance?
    -   *Analysis:* It adds 1 INSERT per Bet. SQLite can handle 3,000 inserts/sec easily *if batched*. Since we are already batching, this is practically free.
2.  **Schema Definition:** defining the `users` table without a `balance` column is suicide for performance. Keep the column, but treat it as a "Materialized View" of the ledger.

## Verdict
**Conditional Pass.**
The architecture is approved **ONLY IF** a Ledger Table is added to support the Mutable Balance.
Without a Ledger, a financial dispute ("Where did my money go?") is unanswerable.
----- END FILE: docs/hardening/003_followup_critique.md -----


----- BEGIN FILE: docs/hardening/003_resolution.md -----
# Hardening Resolution: 003_followup_critique.md

**Date:** 2026-02-03  
**Status:** ALL RISKS ADDRESSED  
**Architecture Version:** v1.3 (Financially Auditable)

---

## Summary

All critiques from Round 003 have been addressed:

| # | Risk | Severity | Solution | ADR |
|---|------|----------|----------|-----|
| 1 | Mutable Balance (no audit trail) | High | Hybrid Transaction Ledger | ADR-010 |
| 2 | Dispute resolution impossible | High | Query-able ledger with refs | ADR-010 |
| 3 | Data corruption undetected | Medium | Daily integrity check | ADR-010 |

---

## Detailed Resolution

### 1. Mutable Balance vs. Event Ledger

**Problem:** ADR-009's `UPDATE users SET balance = balance - x` loses previous state. No paper trail for disputes.

**Rejected Alternative:** Full Event Sourcing
```sql
-- Replay 100K events to check balance at 3K RPS = impossible
```

**Solution:** Hybrid Ledger (ADR-010)
```sql
-- Same transaction:
BEGIN;
  -- 1. Speed: Mutable balance update
  UPDATE users SET balance = balance - 10 
  WHERE id = 1 AND balance >= 10
  RETURNING balance;  -- new_balance = 90
  
  -- 2. Audit: Append-only ledger
  INSERT INTO transaction_ledger 
    (id, user_id, amount, balance_after, ref_type, ref_id)
  VALUES 
    ('uuid', 1, -10, 90, 'BET', 'bet_123');
    
  -- 3. Record bet
  INSERT INTO bets ...
COMMIT;
```

**Benefits:**
- ‚úÖ Performance: `users.balance` is fast read (materialized view)
- ‚úÖ Auditability: Every change recorded with `ref_type`, `ref_id`, `balance_after`
- ‚úÖ Recoverability: `SUM(amount)` from ledger can rebuild balance
- ‚úÖ Verifiability: `balance_after` column allows quick integrity checks

---

### 2. Ledger Schema

```sql
CREATE TABLE transaction_ledger (
    id TEXT PRIMARY KEY,              -- ULID (sortable)
    user_id TEXT NOT NULL,
    amount INTEGER NOT NULL,          -- -10 for bet, +20 for payout
    balance_after INTEGER NOT NULL,   -- Balance snapshot post-transaction
    ref_type TEXT NOT NULL,           -- 'BET', 'PAYOUT', 'DEPOSIT'
    ref_id TEXT NOT NULL,             -- Foreign key to source record
    metadata JSON,                    -- Flexible context
    created_at TIMESTAMP
);

-- Indexes
CREATE INDEX idx_user_time ON transaction_ledger(user_id, created_at);
CREATE INDEX idx_ref ON transaction_ledger(ref_type, ref_id);
```

---

### 3. Storage Impact

| Metric | Before (ADR-009) | After (ADR-010) |
|--------|------------------|-----------------|
| Statements/bet | 2 | 3 (+1 INSERT) |
| Storage/bet | ~200 bytes | ~350 bytes |
| Daily growth | 10 GB | 17.5 GB |
| Batch throughput | Unchanged | Unchanged (30 batches/sec) |

**Verdict:** Manageable with existing weekly archival strategy.

---

### 4. Audit Procedures

#### Daily Integrity Check
```sql
-- Verify no drift between balance and ledger
WITH ledger_sums AS (
    SELECT user_id, SUM(amount) as ledger_balance
    FROM transaction_ledger
    GROUP BY user_id
)
SELECT u.id, u.balance, ls.ledger_balance
FROM users u
LEFT JOIN ledger_sums ls ON u.id = ls.user_id
WHERE u.balance != COALESCE(ls.ledger_balance, 0);
```

#### User Dispute Query
```sql
-- "Where did my $50 go?"
SELECT created_at, ref_type, ref_id, amount, balance_after
FROM transaction_ledger
WHERE user_id = 'user_123'
ORDER BY created_at DESC
LIMIT 50;
```

#### Recovery from Corruption
```sql
-- Rebuild balance from ledger
UPDATE users 
SET balance = (
    SELECT COALESCE(SUM(amount), 0) 
    FROM transaction_ledger 
    WHERE user_id = users.id
);
```

---

### 5. Pass Checks from Round 003

| Check | Status |
|-------|--------|
| UX "Pending" State | ‚úÖ PASS (v1.2) |
| Redis Memory (Nonce TTL) | ‚úÖ PASS (v1.2, 90s) |
| Circuit Breaker Hysteresis | ‚úÖ PASS (v1.2, 30s) |

---

## Final Architecture State (v1.3)

### Data Flow ‚Äî Auditable Bet Placement

```
User Tap
    ‚Üì
Frontend: "PENDING" state
    ‚Üì
POST /api/bets
    ‚Üì
API: Validate signature + nonce (90s TTL)
    ‚Üì
API: Check circuit breaker
    ‚Üì
API: Submit to batcher channel
    ‚Üì
Return 202 Accepted
    ‚Üì
Batcher flush (every 100ms or 100 bets):
    BEGIN TRANSACTION
        UPDATE users SET balance = balance - ? WHERE balance >= ?
        INSERT INTO transaction_ledger (amount, balance_after, ref_type, ref_id)
        INSERT INTO bets
    COMMIT
    ‚Üì
WebSocket: "bet:confirmed" event
    ‚Üì
Frontend: "CONFIRMED" state
```

---

## Verdict

**Reviewer:** System Architecture Guardian  
**Verdict:** ‚úÖ **ACCEPTED FOR IMPLEMENTATION**

All financial auditability requirements satisfied. The hybrid ledger approach (mutable balance + append-only events) is the correct trade-off for this system's constraints (3K RPS, SQLite, 2-week timeline).

---

## Implementation Checklist

- [ ] Create `transaction_ledger` table with indexes
- [ ] Update `BetBatcher` to write ledger entries
- [ ] Include ledger in weekly archival partitions
- [ ] Implement daily integrity check (cron)
- [ ] Document dispute resolution queries for support team
----- END FILE: docs/hardening/003_resolution.md -----


----- BEGIN FILE: docs/implementation-plan/00-overview.md -----
# Implementation Plan: BTC Tap Trading Backend

## Project Context

| Attribute | Value |
|-----------|-------|
| **Architecture Version** | Hardened v1.3 |
| **Timeline** | 2 weeks |
| **Team** | 1 Senior Backend Engineer |
| **Target** | 3,000 RPS, ‚â§100ms latency, 99.5% uptime |
| **Infrastructure** | Single VM (4c/8GB/256GB SSD) |

## Implementation Strategy

This plan follows the **hybrid horizontal-vertical execution model**:

1. **Phase 1: Foundation** ‚Äî Establish infrastructure and shared components
2. **Phase 2: Skeleton** ‚Äî Deploy minimal versions of all containers with connectivity
3. **Phase 3: Vertical Modules** ‚Äî Implement module-by-module with full depth
4. **Phase 4: System Hardening** ‚Äî End-to-end validation and load testing

## Active ADRs (Source of Truth)

All 10 ADRs are `Accepted` and must be implemented:

| ADR | Title | Implementation In |
|-----|-------|-------------------|
| 001 | SQLite as Primary Database | Phase 1, 3 |
| 002 | Go as Backend Language | Phase 1 |
| 003 | Monolithic Single-VM Deployment | Phase 1, 4 |
| 004 | Redis for Cache and Pub/Sub | Phase 1, 3 |
| 005 | Write Batching for Bet Ingestion | Phase 3 (Bet Ingestion Module) |
| 006 | Archival Strategy for Bet History | Phase 3 (Archival Module) |
| 007 | Replay Protection for Web3 Auth | Phase 3 (Auth Module) |
| 008 | Stale Price Circuit Breaker | Phase 3 (Price Feed Module) |
| 009 | Conditional Balance Deduction | Phase 3 (Bet Ingestion Module) |
| 010 | Hybrid Transaction Ledger | Phase 3 (Bet Ingestion Module) |

> **Note:** ADR-010 extends ADR-009 ‚Äî both must be implemented together in the same transaction.

## C4 Container Inventory

All containers from the C4 diagram must be provisioned:

| Container | Technology | Phase |
|-----------|------------|-------|
| API Server | Go (Echo/Fiber) | Phase 2, 3 |
| Bet Batcher | Go (channel-based) | Phase 3 |
| Signature Validator | Go | Phase 3 |
| Circuit Breaker | Go middleware | Phase 3 |
| WebSocket Server | Go | Phase 2, 3 |
| Session Manager | Go | Phase 3 |
| Price Ingestion | Go | Phase 3 |
| Price Health Monitor | Go | Phase 3 |
| Bet Resolver | Go | Phase 3 |
| Weekly Archiver | Go + bash | Phase 3 |
| SQLite (Current Week) | SQLite 3 (WAL) | Phase 1 |
| SQLite (Historical) | SQLite 3 (Read-Only) | Phase 3 |
| Redis | Redis 7 | Phase 1 |
| Nginx | Nginx | Phase 1, 2 |

## Module Breakdown (Phase 3 Vertical Slices)

The backend is divided into 5 vertical modules for implementation:

1. **[Module 1: Core Infrastructure](./01-module-core-infrastructure.md)** ‚Äî Database, Redis, shared libraries
2. **[Module 2: Authentication & Security](./02-module-auth-security.md)** ‚Äî Web3 auth, replay protection, sessions
3. **[Module 3: Bet Ingestion](./03-module-bet-ingestion.md)** ‚Äî Write batching, conditional deduction, ledger
4. **[Module 4: Price Feed](./04-module-price-feed.md)** ‚Äî Ingestion, circuit breaker, WebSocket streaming
5. **[Module 5: Resolution & Archival](./05-module-resolution-archival.md)** ‚Äî Bet resolution, weekly archival, integrity checks

## Traceability Matrix

| ADR | Phase 1 | Phase 2 | Phase 3 Module | Phase 4 |
|-----|---------|---------|----------------|---------|
| 001 | ‚úÖ DB setup | ‚úÖ | All modules | ‚úÖ |
| 002 | ‚úÖ Go scaffold | ‚úÖ | All modules | ‚úÖ |
| 003 | ‚úÖ VM setup | ‚úÖ All containers | All modules | ‚úÖ |
| 004 | ‚úÖ Redis setup | ‚úÖ | All modules | ‚úÖ |
| 005 | | | Module 3 | ‚úÖ |
| 006 | | | Module 5 | ‚úÖ |
| 007 | | | Module 2 | ‚úÖ |
| 008 | | | Module 4 | ‚úÖ |
| 009 | | | Module 3 | ‚úÖ |
| 010 | | | Module 3 | ‚úÖ |

## Deliverables

| Phase | Output |
|-------|--------|
| Phase 1 | Infrastructure ready, CI/CD running, shared libraries published |
| Phase 2 | All containers deployable, health endpoints responding, connectivity verified |
| Phase 3 | 5 modules implemented with tests, integration passing |
| Phase 4 | 3K RPS sustained, 100ms p99 latency, all failure modes tested |

## Success Criteria

- [ ] All ADRs implemented per specification
- [ ] C4 container diagram fully realized
- [ ] 3,000 RPS sustained load test passing
- [ ] p99 latency ‚â§100ms
- [ ] Circuit breaker functional (verified by stale feed simulation)
- [ ] Replay attack prevention verified
- [ ] Double-spend prevention verified
- [ ] Weekly archival functional
- [ ] Transaction ledger integrity check passing
----- END FILE: docs/implementation-plan/00-overview.md -----


----- BEGIN FILE: docs/implementation-plan/01-phase-foundation.md -----
# Phase 1: Foundation (Horizontal)

**Goal:** Establish shared infrastructure, CI/CD, and scaffolding that enables all downstream work.

**Duration:** Days 1-2

---

## 1.1 VM Provisioning & System Setup

### Task 1.1.1: Provision VM and Base OS
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-003 (Monolithic Single-VM Deployment)  
**Source C4:** Single VM boundary

**Description:**
Provision the single VM with required specifications and configure base OS settings.

**DoD:**
- [ ] VM running with 4-core CPU, 8GB RAM, 256GB SSD
- [ ] Ubuntu 22.04 LTS (or compatible) installed
- [ ] SSH access configured with key-based auth
- [ ] Firewall configured (allow 22, 80, 443, 8080, 8081)
- [ ] Automatic security updates enabled

**‚ö†Ô∏è Constraints:**
- Single VM is the only deployment target (no scaling)
- 256GB disk must be monitored (alert at 70%, 85%, 95%)

---

### Task 1.1.2: Install System Dependencies
**Owner:** Backend Engineer  
**Estimated:** 1 hour  
**Source ADR:** ADR-002 (Go Backend)  
**Source C4:** Application Layer

**Description:**
Install Go, SQLite, Redis, Nginx, and supporting tools.

**DoD:**
- [ ] Go 1.21+ installed (`/usr/local/go`)
- [ ] SQLite 3.x installed with FTS5 support
- [ ] Redis 7.x installed
- [ ] Nginx installed
- [ ] zstd compression tools installed
- [ ] AWS CLI installed (for S3 archival)

**Commands:**
```bash
# Go installation
curl -L https://go.dev/dl/go1.21.5.linux-amd64.tar.gz | sudo tar -C /usr/local -xzf -
echo 'export PATH=$PATH:/usr/local/go/bin' | sudo tee /etc/profile.d/go.sh

# Other dependencies
sudo apt update
sudo apt install -y sqlite3 redis-server nginx zstd awscli
```

---

## 1.2 Database Setup (ADR-001)

### Task 1.2.1: Configure SQLite with WAL Mode
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-001 (SQLite as Primary Database)  
**Source C4:** SQLite (Current Week)

**Description:**
Set up SQLite with WAL mode for concurrent read/write, configure connection pooling parameters.

**DoD:**
- [ ] Data directory created: `/data`
- [ ] WAL mode enabled: `PRAGMA journal_mode = WAL`
- [ ] Synchronous mode set: `PRAGMA synchronous = NORMAL`
- [ ] Cache size configured: `PRAGMA cache_size = -64000` (64MB)
- [ ] Auto-vacuum incremental enabled
- [ ] Backup script created at `/opt/app/scripts/backup-hourly.sh`

**Configuration:**
```sql
-- Run on each new database
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = -64000;
PRAGMA auto_vacuum = INCREMENTAL;
PRAGMA temp_store = MEMORY;
PRAGMA mmap_size = 268435456; -- 256MB
```

**‚ö†Ô∏è Critical:**
- WAL mode is mandatory for concurrent reads during writes
- Backup script must use `sqlite3 .backup` (not file copy) for consistency

---

### Task 1.2.2: Create Database Schema Templates
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-009, ADR-010 (Conditional Deduction + Hybrid Ledger)  
**Source C4:** SQLite (Current Week)

**Description:**
Create initial schema for users, bets, and transaction_ledger tables.

**DoD:**
- [ ] Schema file created: `/opt/app/schema/001_initial.sql`
- [ ] `users` table with optimistic locking (version column)
- [ ] `bets` table with status tracking
- [ ] `transaction_ledger` table (ADR-010) with indexes
- [ ] Partial index on bets pending status
- [ ] Migration system in place (using golang-migrate or similar)

**Schema:**
```sql
-- Users table
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    wallet_address TEXT UNIQUE NOT NULL,
    balance INTEGER NOT NULL DEFAULT 0,
    version INTEGER NOT NULL DEFAULT 1,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Transaction Ledger (ADR-010)
CREATE TABLE transaction_ledger (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    amount INTEGER NOT NULL,
    balance_after INTEGER NOT NULL,
    ref_type TEXT NOT NULL, -- 'BET', 'PAYOUT', 'DEPOSIT', 'WITHDRAWAL'
    ref_id TEXT NOT NULL,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(id)
);
CREATE INDEX idx_ledger_user_time ON transaction_ledger(user_id, created_at);
CREATE INDEX idx_ledger_ref ON transaction_ledger(ref_type, ref_id);

-- Bets table
CREATE TABLE bets (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    cell_id TEXT NOT NULL,
    amount INTEGER NOT NULL,
    reward_rate REAL NOT NULL,
    status TEXT NOT NULL, -- 'pending', 'confirmed', 'won', 'lost', 'insufficient_funds'
    placed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP,
    payout_amount INTEGER,
    
    FOREIGN KEY (user_id) REFERENCES users(id)
);
CREATE INDEX idx_bets_pending ON bets(user_id, status) WHERE status = 'pending';
CREATE INDEX idx_bets_user ON bets(user_id, placed_at);
```

---

## 1.3 Redis Setup (ADR-004)

### Task 1.3.1: Configure Redis
**Owner:** Backend Engineer  
**Estimated:** 1.5 hours  
**Source ADR:** ADR-004 (Redis for Cache, Sessions, Pub/Sub)  
**Source C4:** Redis container

**Description:**
Configure Redis with memory limits, persistence, and proper key eviction.

**DoD:**
- [ ] Redis config at `/etc/redis/redis.conf`
- [ ] `maxmemory 512mb` set
- [ ] `maxmemory-policy allkeys-lru` set
- [ ] `appendonly yes` (AOF persistence)
- [ ] `appendfsync everysec` configured
- [ ] Redis started and responding on :6379

**Configuration:**
```conf
# /etc/redis/redis.conf
maxmemory 512mb
maxmemory-policy allkeys-lru
appendonly yes
appendfsync everysec
save 900 1
save 300 10
save 60 10000
```

**‚ö†Ô∏è Key TTLs:**
- Sessions: 24h
- Nonces: 90s (ADR-007 hardening fix)
- Prices: 1h (latest), 7d (history)
- Rate limits: 1m

---

## 1.4 Go Project Scaffolding (ADR-002)

### Task 1.4.1: Create Go Project Structure
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-002 (Go Backend Language)  
**Source C4:** All Go containers

**Description:**
Set up Go module structure, shared libraries, and build configuration.

**DoD:**
- [ ] Go module initialized: `github.com/tap-trading/backend`
- [ ] Directory structure created per layout below
- [ ] Shared packages: `pkg/config`, `pkg/db`, `pkg/redis`, `pkg/logger`
- [ ] Makefile with build targets
- [ ] Dockerfile for each service

**Project Layout:**
```
/opt/app/backend/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îú‚îÄ‚îÄ api/           # API Server
‚îÇ   ‚îú‚îÄ‚îÄ ws/            # WebSocket Server
‚îÇ   ‚îú‚îÄ‚îÄ price/         # Price Ingestion + Health Monitor
‚îÇ   ‚îú‚îÄ‚îÄ resolver/      # Bet Resolver
‚îÇ   ‚îî‚îÄ‚îÄ archiver/      # Weekly Archiver
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îú‚îÄ‚îÄ config/        # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ db/            # SQLite abstraction
‚îÇ   ‚îú‚îÄ‚îÄ redis/         # Redis client
‚îÇ   ‚îú‚îÄ‚îÄ logger/        # Structured logging (zerolog)
‚îÇ   ‚îú‚îÄ‚îÄ auth/          # Signature validation (shared)
‚îÇ   ‚îú‚îÄ‚îÄ batcher/       # Write batching logic
‚îÇ   ‚îú‚îÄ‚îÄ circuit/       # Circuit breaker
‚îÇ   ‚îî‚îÄ‚îÄ models/        # Domain models
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îî‚îÄ‚îÄ ...            # Private implementation
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ backup-hourly.sh
‚îÇ   ‚îî‚îÄ‚îÄ archive-weekly.sh
‚îú‚îÄ‚îÄ schema/
‚îÇ   ‚îî‚îÄ‚îÄ *.sql
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ go.mod
```

---

### Task 1.4.2: Implement Shared Libraries
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-001, ADR-004 (Database + Redis)  
**Source C4:** All containers

**Description:**
Implement reusable packages for database, Redis, configuration, and logging.

**DoD:**
- [ ] `pkg/config`: Environment-based config with validation
- [ ] `pkg/db`: SQLite connection pool with WAL, prepared statement cache
- [ ] `pkg/redis`: Redis client with connection pooling, pub/sub support
- [ ] `pkg/logger`: Structured JSON logging (zerolog)
- [ ] Unit tests for all packages

**Key Interfaces:**
```go
// pkg/db/db.go
type DB interface {
    QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error)
    QueryRowContext(ctx context.Context, query string, args ...interface{}) *sql.Row
    ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error)
    BeginTx(ctx context.Context, opts *sql.TxOptions) (*sql.Tx, error)
    Close() error
}

// pkg/redis/redis.go
type Client interface {
    Get(ctx context.Context, key string) *redis.StringCmd
    Set(ctx context.Context, key string, value interface{}, ttl time.Duration) *redis.StatusCmd
    SetNX(ctx context.Context, key string, value interface{}, ttl time.Duration) *redis.BoolCmd
    Subscribe(ctx context.Context, channels ...string) *redis.PubSub
    Publish(ctx context.Context, channel string, message interface{}) *redis.IntCmd
    Pipeline() redis.Pipeliner
}
```

---

## 1.5 CI/CD Pipeline and Deployment

### Task 1.5.1: CI/CD Pipeline Setup
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-003 (Monolithic Deployment)  
**Source C4:** Deployment flow

**Description:**
Implement CI/CD pipeline with build gates, test gates, and artifact versioning to ensure code quality and reproducible deployments.

---

#### Build Gate (Compilation & Quality)

**DoD:**
- [ ] Go compilation with `-ldflags` for version injection
- [ ] golangci-lint with custom config (security, performance, error handling)
- [ ] go vet, go fmt checks
- [ ] Static analysis for crypto/transaction code
- [ ] Build fails on any warning in critical paths

**Build Gate Configuration (`.golangci.yml`):**
```yaml
run:
  timeout: 5m
  issues-exit-code: 1

linters:
  enable:
    - gosec        # Security
    - staticcheck  # Static analysis
    - gosimple     # Simplification
    - ineffassign  # Ineffective assignments
    - errcheck     # Unchecked errors
    - govet        # Vet
    - typecheck    # Type checking
    - unused       # Unused code

linters-settings:
  gosec:
    excludes:
      - G104  # Allow unchecked errors in non-critical paths (checked in critical paths)
  
issues:
  exclude-rules:
    - path: _test\.go
      linters:
        - gosec
```

**Build Script (`scripts/build-gate.sh`):**
```bash
#!/bin/bash
set -euo pipefail

echo "=== Build Gate ==="

# Format check
echo "‚Üí Checking formatting..."
if [ -n "$(go fmt ./...)" ]; then
    echo "ERROR: Code is not formatted. Run 'go fmt ./...'"
    exit 1
fi

# Vet
echo "‚Üí Running go vet..."
go vet ./...

# Lint
echo "‚Üí Running golangci-lint..."
golangci-lint run --config .golangci.yml

# Security scan on critical paths
echo "‚Üí Security scan on transaction code..."
gosec -include=G104,G201,G202,G203,G301,G302,G303,G304,G305,G306,G307,G401,G402,G403,G404,G501,G502,G503,G504,G505,G601 ./pkg/batcher/... ./pkg/auth/...

echo "‚úì Build gate passed"
```

---

#### Test Gate (Quality Assurance)

**DoD:**
- [ ] Unit tests with race detector (`-race`)
- [ ] Integration tests with SQLite and Redis
- [ ] Coverage threshold: 80% for critical paths (batcher, auth)
- [ ] Benchmark tests for hot paths (3K RPS simulation)
- [ ] Test artifacts stored

**Test Gate Configuration:**
```bash
#!/bin/bash
# scripts/test-gate.sh
set -euo pipefail

echo "=== Test Gate ==="

# Unit tests with race detector
echo "‚Üí Running unit tests..."
go test -v -race -coverprofile=coverage.out ./...

# Coverage check for critical paths
echo "‚Üí Checking coverage..."
go tool cover -func=coverage.out | awk '
/^(pkg\/(batcher|auth)\/)/ { 
    if ($3 < 80) { 
        print "FAIL: Coverage below 80% in " $1 ": " $3 
        exit 1 
    }
}'

# Integration tests (requires running DB/Redis)
echo "‚Üí Running integration tests..."
go test -v -tags=integration ./pkg/db/... ./pkg/redis/...

# Benchmark tests
echo "‚Üí Running benchmarks..."
go test -bench=. -benchtime=10s ./pkg/batcher/... > benchmark.txt

echo "‚úì Test gate passed"
```

**Test Coverage Requirements:**

| Package | Minimum Coverage |
|---------|------------------|
| `pkg/batcher` | 80% |
| `pkg/auth` | 80% |
| `pkg/db` | 70% |
| `pkg/redis` | 70% |
| Other | 60% |

---

#### Artifact Versioning

**DoD:**
- [ ] Semantic versioning (v1.2.3-build+commit)
- [ ] Git commit SHA embedded in binary
- [ ] Build timestamp embedded
- [ ] Version endpoint in each service (`/version`)
- [ ] Versioned artifacts stored in `/opt/app/releases/`

**Versioning Strategy:**
```bash
# Version format: v{MAJOR}.{MINOR}.{PATCH}-{build}+{git_sha}
# Example: v1.3.0-42+7d2f1a3b

VERSION=$(git describe --tags --always --dirty)
BUILD_TIME=$(date -u +%Y%m%d%H%M%S)
GIT_COMMIT=$(git rev-parse --short HEAD)
GO_VERSION=$(go version | awk '{print $3}')

# Build with version injection
go build -ldflags "\
    -X main.Version=${VERSION} \
    -X main.BuildTime=${BUILD_TIME} \
    -X main.GitCommit=${GIT_COMMIT} \
    -X main.GoVersion=${GO_VERSION} \
    -s -w \
" -o bin/api ./cmd/api
```

**Version Endpoint:**
```go
// In each service
func (s *Server) VersionHandler(c echo.Context) error {
    return c.JSON(http.StatusOK, map[string]string{
        "version":    Version,
        "build_time": BuildTime,
        "git_commit": GitCommit,
        "go_version": GoVersion,
    })
}
```

**Artifact Storage:**
```
/opt/app/releases/
‚îú‚îÄ‚îÄ v1.3.0/
‚îÇ   ‚îú‚îÄ‚îÄ api
‚îÇ   ‚îú‚îÄ‚îÄ ws
‚îÇ   ‚îú‚îÄ‚îÄ price
‚îÇ   ‚îú‚îÄ‚îÄ resolver
‚îÇ   ‚îú‚îÄ‚îÄ archiver
‚îÇ   ‚îú‚îÄ‚îÄ checksums.sha256
‚îÇ   ‚îî‚îÄ‚îÄ manifest.json
‚îú‚îÄ‚îÄ v1.3.1/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ current -> v1.3.0  # Symlink to current version
```

**Manifest Format:**
```json
{
  "version": "v1.3.0-42+7d2f1a3b",
  "build_time": "20260203093045",
  "git_commit": "7d2f1a3b",
  "artifacts": {
    "api": {
      "path": "api",
      "sha256": "a1b2c3d4..."
    },
    "ws": {
      "path": "ws", 
      "sha256": "e5f6g7h8..."
    }
  },
  "built_by": "ci-server-01"
}
```

---

#### Deployment Pipeline

**DoD:**
- [ ] Blue-green or rolling deployment strategy
- [ ] Pre-deployment health check
- [ ] Automated rollback on failure
- [ ] Post-deployment smoke tests

**Deployment Script (`scripts/deploy.sh`):**
```bash
#!/bin/bash
set -euo pipefail

VERSION=${1:-current}
RELEASE_DIR="/opt/app/releases/${VERSION}"

echo "=== Deploying ${VERSION} ==="

# Verify artifacts exist
if [ ! -d "${RELEASE_DIR}" ]; then
    echo "ERROR: Release ${VERSION} not found"
    exit 1
fi

# Pre-deployment health check
echo "‚Üí Checking current health..."
if ! curl -sf http://localhost:8080/healthz > /dev/null; then
    echo "WARNING: Current deployment is unhealthy"
fi

# Backup current
BACKUP_DIR="/opt/app/releases/.backup-$(date +%Y%m%d%H%M%S)"
cp -r /opt/app/current "${BACKUP_DIR}"

# Update symlink
echo "‚Üí Updating to ${VERSION}..."
ln -sfn "${RELEASE_DIR}" /opt/app/current

# Restart services
echo "‚Üí Restarting services..."
systemctl restart tap-trading-api tap-trading-ws tap-trading-price

# Wait for services
echo "‚Üí Waiting for services..."
sleep 5

# Post-deployment smoke tests
echo "‚Üí Running smoke tests..."
for i in {1..10}; do
    if curl -sf http://localhost:8080/healthz > /dev/null; then
        echo "‚úì API healthy"
        break
    fi
    if [ $i -eq 10 ]; then
        echo "ERROR: Deployment failed health check"
        echo "‚Üí Rolling back..."
        ln -sfn "${BACKUP_DIR}" /opt/app/current
        systemctl restart tap-trading-api tap-trading-ws
        exit 1
    fi
    sleep 2
done

echo "‚úì Deployment successful"
```

---

### Task 1.5.2: Systemd Service Definitions
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-003  
**Source C4:** All Go containers

**Description:**
Create systemd service files for all Go binaries with proper dependency ordering.

**DoD:**
- [ ] Service files for api, ws, price, resolver, archiver
- [ ] Dependency ordering (api/ws after redis, price after network)
- [ ] Auto-restart with exponential backoff
- [ ] Structured logging to journald
- [ ] Resource limits (CPU: 50%, Memory: 1GB per service)

**Example Service:**
```ini
# /etc/systemd/system/tap-trading-api.service
[Unit]
Description=BTC Tap Trading API Server (v%i)
After=network.target redis.service sqlite.service
Requires=redis.service

[Service]
Type=simple
User=app
Group=app
WorkingDirectory=/opt/app/current
ExecStart=/opt/app/current/api
Restart=always
RestartSec=5
StartLimitInterval=60
StartLimitBurst=3

# Resource limits
CPUQuota=50%
MemoryLimit=1G

# Environment
Environment=LOG_LEVEL=info
Environment=DB_PATH=/data/bets.db
Environment=REDIS_ADDR=localhost:6379

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=tap-trading-api

[Install]
WantedBy=multi-user.target
```

---

### Task 1.5.2: Configure Nginx
**Owner:** Backend Engineer  
**Estimated:** 1.5 hours  
**Source ADR:** ADR-003 (Monolithic Deployment)  
**Source C4:** Nginx container

**Description:**
Configure Nginx as reverse proxy with rate limiting and connection limits.

**DoD:**
- [ ] Nginx config at `/etc/nginx/sites-available/tap-trading`
- [ ] Static file serving for frontend
- [ ] Reverse proxy to API (:8080) and WebSocket (:8081)
- [ ] Rate limiting: 100 req/min per IP
- [ ] Connection limiting: 1000 concurrent per IP
- [ ] WebSocket upgrade headers configured

**Configuration:**
```nginx
limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;
limit_conn_zone $binary_remote_addr zone=addr:10m;

server {
    listen 80;
    server_name _;
    
    # Static files
    location / {
        root /opt/app/frontend/dist;
        try_files $uri $uri/ /index.html;
    }
    
    # API
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        limit_conn addr 100;
        proxy_pass http://localhost:8080/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    # WebSocket
    location /ws {
        limit_req zone=api burst=50 nodelay;
        proxy_pass http://localhost:8081;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
    }
}
```

---

## 1.6 Observability Baseline

### Task 1.6.1: Set Up Logging and Metrics
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All (Observability requirement in PRD)  
**Source C4:** All containers

**Description:**
Implement structured logging and basic metrics collection.

**DoD:**
- [ ] zerolog configured for JSON output
- [ ] Log rotation configured (logrotate)
- [ ] Prometheus metrics endpoint scaffolded
- [ ] Request ID middleware for tracing
- [ ] Health check endpoint pattern defined

**Log Format:**
```json
{
  "level": "info",
  "time": "2026-02-03T12:00:00Z",
  "request_id": "req_abc123",
  "component": "api",
  "message": "bet placed",
  "user_id": "0x...",
  "bet_id": "bet_xyz",
  "amount": 1000
}
```

---

## Phase 1 Deliverables Checklist

### Infrastructure
- [ ] VM provisioned and accessible
- [ ] Go 1.21+, SQLite, Redis, Nginx installed
- [ ] SQLite WAL mode configured
- [ ] Database schema templates created
- [ ] Redis configured with 512MB limit and AOF
- [ ] Nginx configured with rate limiting

### Project Structure
- [ ] Go project structure established
- [ ] Shared libraries (db, redis, config, logger) implemented
- [ ] Logging and metrics baseline ready

### CI/CD Pipeline
- [ ] **Build Gate**: golangci-lint, go vet, go fmt, security scan configured
- [ ] **Test Gate**: Unit tests with race detector, coverage thresholds defined
- [ ] **Artifact Versioning**: Semantic versioning, version injection, `/version` endpoint
- [ ] **Deployment Pipeline**: Automated deploy with smoke tests and rollback
- [ ] Build, test, and deploy scripts created
- [ ] systemd service files with resource limits

---

## Exit Criteria for Phase 1

Phase 1 is complete when:
1. All infrastructure is provisioned and configurable
2. A new team member can clone the repo and run `make build` successfully
3. **Build gate passes** (lint, vet, fmt, security scan)
4. **Test gate passes** (unit tests with coverage threshold)
5. **Version endpoint returns** correct version info (`GET /version`)
6. Database connections can be established from a test program
7. Redis is accepting connections with proper persistence
8. Nginx serves static files and proxies to upstream (even if upstream doesn't exist yet)
9. **CI/CD pipeline can deploy a versioned artifact end-to-end**
----- END FILE: docs/implementation-plan/01-phase-foundation.md -----


----- BEGIN FILE: docs/implementation-plan/02-phase-skeleton.md -----
# Phase 2: Skeleton / Thin Thread (Horizontal)

**Goal:** Deploy minimal versions of all containers to validate connectivity and end-to-end flow.

**Duration:** Days 2-3

---

## 2.1 Minimal API Server

### Task 2.1.1: Skeleton API Server
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-002 (Go Backend)  
**Source C4:** API Server container

**Description:**
Create minimal API server with health endpoint, routing, and middleware scaffolding.

**DoD:**
- [ ] HTTP server on :8080 with graceful shutdown
- [ ] `/healthz` endpoint returning `{"status": "ok"}`
- [ ] Request logging middleware
- [ ] Panic recovery middleware
- [ ] CORS configured for frontend
- [ ] systemd service file working

**Code Skeleton:**
```go
// cmd/api/main.go
package main

import (
    "context"
    "net/http"
    "os"
    "os/signal"
    "syscall"
    "time"
    
    "github.com/labstack/echo/v4"
    "github.com/labstack/echo/v4/middleware"
    "github.com/tap-trading/backend/pkg/config"
    "github.com/tap-trading/backend/pkg/logger"
)

func main() {
    cfg := config.Load()
    log := logger.New(cfg.LogLevel)
    
    e := echo.New()
    e.Use(middleware.Recover())
    e.Use(middleware.RequestID())
    e.Use(logger.Middleware(log))
    
    e.GET("/healthz", func(c echo.Context) error {
        return c.JSON(http.StatusOK, map[string]string{
            "status": "ok",
            "component": "api",
        })
    })
    
    // Graceful shutdown
    go func() {
        if err := e.Start(":8080"); err != nil && err != http.ErrServerClosed {
            log.Fatal().Err(err).Msg("server error")
        }
    }()
    
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    <-quit
    
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    if err := e.Shutdown(ctx); err != nil {
        log.Fatal().Err(err).Msg("shutdown error")
    }
}
```

---

### Task 2.1.2: Skeleton Bet Endpoint
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-002  
**Source C4:** API Server ‚Üí Bet Batcher relationship

**Description:**
Create placeholder bet endpoint that validates request format but returns stub response.

**DoD:**
- [ ] `POST /api/bets` endpoint accepting bet request JSON
- [ ] Request validation (required fields)
- [ ] Returns stub 202 Accepted response
- [ ] Error responses following API contract

**Stub Response:**
```json
{
  "status": "accepted",
  "bet_id": "stub_bet_001",
  "message": "Stub implementation - not persisted"
}
```

---

## 2.2 Minimal WebSocket Server

### Task 2.2.1: Skeleton WebSocket Server
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-004 (Redis Pub/Sub)  
**Source C4:** WebSocket Server container

**Description:**
Create minimal WebSocket server with connection handling and basic message echo.

**DoD:**
- [ ] WebSocket server on :8081
- [ ] Connection upgrade handling
- [ ] Connection lifecycle logging
- [ ] Basic message echo for testing
- [ ] Heartbeat/ping-pong handling
- [ ] systemd service file working

**Code Skeleton:**
```go
// cmd/ws/main.go
package main

import (
    "net/http"
    "github.com/gorilla/websocket"
    "github.com/tap-trading/backend/pkg/logger"
)

var upgrader = websocket.Upgrader{
    CheckOrigin: func(r *http.Request) bool { return true },
}

func handleWebSocket(log logger.Logger) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        conn, err := upgrader.Upgrade(w, r, nil)
        if err != nil {
            log.Error().Err(err).Msg("upgrade failed")
            return
        }
        defer conn.Close()
        
        log.Info().Str("remote", r.RemoteAddr).Msg("client connected")
        
        for {
            mt, message, err := conn.ReadMessage()
            if err != nil {
                log.Info().Err(err).Msg("client disconnected")
                break
            }
            // Echo for now
            if err := conn.WriteMessage(mt, message); err != nil {
                log.Error().Err(err).Msg("write failed")
                break
            }
        }
    }
}
```

---

## 2.3 Minimal Price Ingestion

### Task 2.3.1: Skeleton Price Ingestion
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-004 (Redis Pub/Sub)  
**Source C4:** Price Ingestion container

**Description:**
Create minimal price ingestion that generates fake prices and publishes to Redis.

**DoD:**
- [ ] Connects to external price feed (or generates mock data)
- [ ] Publishes to Redis channel `prices:btc:usd`
- [ ] Logs each published price
- [ ] systemd service file working

**Mock Implementation:**
```go
// cmd/price/main.go - stub
func main() {
    redisClient := redis.NewClient(...)
    ticker := time.NewTicker(1 * time.Second)
    
    basePrice := 42000.00
    for range ticker.C {
        // Mock price movement
        change := (rand.Float64() - 0.5) * 100
        price := basePrice + change
        
        redisClient.Publish(ctx, "prices:btc:usd", fmt.Sprintf("%.2f", price))
        redisClient.Set(ctx, "price:btc:usd", price, 1*time.Hour)
        
        // Add to sorted set for history
        redisClient.ZAdd(ctx, "prices:btc:usd:ts", redis.Z{
            Score:  float64(time.Now().Unix()),
            Member: fmt.Sprintf("%.2f", price),
        })
    }
}
```

---

## 2.4 Connectivity Validation

### Task 2.4.1: Integration Smoke Tests
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-003 (Monolithic Deployment)  
**Source C4:** All container relationships

**Description:**
Write and run integration tests to verify all components connect properly.

**DoD:**
- [ ] Test: API server responds via Nginx
- [ ] Test: WebSocket accepts connections via Nginx
- [ ] Test: API can read/write SQLite
- [ ] Test: API can read/write Redis
- [ ] Test: WebSocket can subscribe to Redis pub/sub
- [ ] Test: Price ingestion publishes to Redis
- [ ] All services start via systemd without errors

**Test Commands:**
```bash
# Test Nginx ‚Üí API
curl http://localhost/api/healthz

# Test Nginx ‚Üí WebSocket
websocat ws://localhost/ws

# Test Redis connectivity
redis-cli ping

# Test SQLite
sqlite3 /data/test.db "SELECT 1;"

# Test systemd
sudo systemctl start tap-trading-api
curl http://localhost:8080/healthz
```

---

### Task 2.4.2: End-to-End Flow Test
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All  
**Source C4:** Full system

**Description:**
Verify the complete request flow from frontend through all layers.

**DoD:**
- [ ] Frontend can load via Nginx
- [ ] Frontend can place bet (stub)
- [ ] Bet request reaches API server
- [ ] Real-time prices flow to frontend via WebSocket

**Flow Verification:**
```
Frontend (Browser)
    ‚Üì HTTPS
Nginx (:80/443)
    ‚Üì HTTP :8080
API Server (responds)
    
Price Ingestion (:mock)
    ‚Üì Redis Pub/Sub
WebSocket Server (:8081)
    ‚Üì WebSocket
Frontend (displays price)
```

---

## 2.5 Service Definitions

### Task 2.5.1: Create systemd Services
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-003  
**Source C4:** All Go containers

**Description:**
Create systemd service files for all Go binaries.

**DoD:**
- [ ] `/etc/systemd/system/tap-trading-api.service`
- [ ] `/etc/systemd/system/tap-trading-ws.service`
- [ ] `/etc/systemd/system/tap-trading-price.service`
- [ ] Services auto-restart on failure
- [ ] Services start on boot

**Example Service:**
```ini
# /etc/systemd/system/tap-trading-api.service
[Unit]
Description=BTC Tap Trading API Server
After=network.target redis.service

[Service]
Type=simple
User=app
Group=app
WorkingDirectory=/opt/app
ExecStart=/opt/app/bin/api
Restart=always
RestartSec=5
Environment=LOG_LEVEL=info
Environment=DB_PATH=/data/bets.db
Environment=REDIS_ADDR=localhost:6379

[Install]
WantedBy=multi-user.target
```

---

## Phase 2 Deliverables Checklist

- [ ] API server running on :8080 with /healthz
- [ ] WebSocket server running on :8081
- [ ] Price ingestion running (mock or real)
- [ ] Nginx proxying to both servers
- [ ] SQLite connectivity verified
- [ ] Redis connectivity verified
- [ ] All systemd services defined and tested
- [ ] Integration smoke tests passing
- [ ] End-to-end flow manually verified

---

## Exit Criteria for Phase 2

Phase 2 is complete when:
1. All containers can be started via systemd
2. `curl http://localhost/api/healthz` returns 200
3. WebSocket connection via Nginx succeeds
4. A price update flows from ingestion ‚Üí Redis ‚Üí WebSocket
5. API can read/write to both SQLite and Redis
6. No container crashes on startup

---

## Troubleshooting Notes

| Issue | Solution |
|-------|----------|
| SQLite locked | Ensure WAL mode is enabled |
| Redis OOM | Check maxmemory-policy is allkeys-lru |
| Nginx 502 | Verify upstream servers are running |
| WebSocket disconnect | Check upgrade headers in Nginx config |
| Permission denied | Ensure /data is owned by app user |
----- END FILE: docs/implementation-plan/02-phase-skeleton.md -----


----- BEGIN FILE: docs/implementation-plan/03-module-core-infrastructure.md -----
# Module 1: Core Infrastructure (Vertical)

**Goal:** Implement the foundational data layer and shared components that all other modules depend on.

**Duration:** Days 3-4  
**Dependencies:** Phase 1 (Foundation), Phase 2 (Skeleton) complete  
**Priority:** CRITICAL (blocks all other modules)

---

## Module Scope

This module implements:
- Complete database layer with transaction support
- Redis caching and pub/sub with proper error handling
- Configuration management
- Structured logging with context propagation
- Domain models and shared types

**Source ADRs:** ADR-001 (SQLite), ADR-004 (Redis), ADR-002 (Go)  
**Source C4:** SQLite (RW/RO), Redis containers

---

## Task 1: Database Layer (ADR-001)

### Task 1.1: Implement SQLite Connection Pool
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-001 (SQLite as Primary Database)  
**Source C4:** SQLite (Current Week)

**Description:**
Implement production-ready SQLite client with connection pooling, WAL support, and prepared statement caching.

**DoD:**
- [ ] Connection pool with configurable max open connections (10-20)
- [ ] WAL mode enforcement on connection open
- [ ] Prepared statement cache for hot queries
- [ ] Connection health check
- [ ] Retry logic with exponential backoff for SQLITE_BUSY
- [ ] Context cancellation support

**Implementation:**
```go
// pkg/db/sqlite.go
package db

import (
    "context"
    "database/sql"
    "fmt"
    "time"
    
    _ "github.com/mattn/go-sqlite3"
)

type Config struct {
    Path           string
    MaxOpenConns   int
    MaxIdleConns   int
    ConnMaxLifetime time.Duration
    BusyTimeout    time.Duration
}

type Client struct {
    db *sql.DB
}

func New(cfg Config) (*Client, error) {
    dsn := fmt.Sprintf("%s?_journal_mode=WAL&_synchronous=NORMAL&_busy_timeout=%d",
        cfg.Path, cfg.BusyTimeout.Milliseconds())
    
    db, err := sql.Open("sqlite3", dsn)
    if err != nil {
        return nil, fmt.Errorf("open database: %w", err)
    }
    
    db.SetMaxOpenConns(cfg.MaxOpenConns)
    db.SetMaxIdleConns(cfg.MaxIdleConns)
    db.SetConnMaxLifetime(cfg.ConnMaxLifetime)
    
    // Verify WAL mode
    var journalMode string
    if err := db.QueryRow("PRAGMA journal_mode").Scan(&journalMode); err != nil {
        return nil, fmt.Errorf("check journal mode: %w", err)
    }
    if journalMode != "wal" {
        return nil, fmt.Errorf("WAL mode not enabled, got: %s", journalMode)
    }
    
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    if err := db.PingContext(ctx); err != nil {
        return nil, fmt.Errorf("ping database: %w", err)
    }
    
    return &Client{db: db}, nil
}

func (c *Client) Close() error {
    return c.db.Close()
}

func (c *Client) ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error) {
    return c.db.ExecContext(ctx, query, args...)
}

func (c *Client) QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error) {
    return c.db.QueryContext(ctx, query, args...)
}

func (c *Client) QueryRowContext(ctx context.Context, query string, args ...interface{}) *sql.Row {
    return c.db.QueryRowContext(ctx, query, args...)
}

func (c *Client) BeginTx(ctx context.Context, opts *sql.TxOptions) (*sql.Tx, error) {
    return c.db.BeginTx(ctx, opts)
}
```

**Tests:**
- [ ] Connection pool limits enforced
- [ ] WAL mode verified
- [ ] SQLITE_BUSY triggers retry
- [ ] Context cancellation interrupts query

---

### Task 1.2: Implement Database Migrations
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-001, ADR-009, ADR-010  
**Source C4:** SQLite schema

**Description:**
Implement migration system for schema versioning and evolution.

**DoD:**
- [ ] Migration runner using `golang-migrate` or custom
- [ ] All schema files in `/opt/app/backend/schema/`
- [ ] Migration version table in database
- [ ] Up/down migrations for each version
- [ ] Migration command in Makefile

**Migration Files:**
```
schema/
‚îú‚îÄ‚îÄ 001_initial.up.sql
‚îú‚îÄ‚îÄ 001_initial.down.sql
‚îú‚îÄ‚îÄ 002_indexes.up.sql
‚îú‚îÄ‚îÄ 002_indexes.down.sql
‚îî‚îÄ‚îÄ ...
```

**Migration Runner:**
```go
// pkg/db/migrate.go
func Migrate(db *sql.DB, direction string) error {
    // Implementation using golang-migrate/migrate
    // or custom lightweight version
}
```

---

### Task 1.3: Implement Partition Manager (ADR-006)
**Owner:** Backend Engineer  
**Estimated:** 5 hours  
**Source ADR:** ADR-006 (Archival Strategy)  
**Source C4:** SQLite (Current Week + Historical)

**Description:**
Implement partition management for weekly database rotation.

**DoD:**
- [ ] `PartitionManager` struct tracking current week
- [ ] `ATTACH DATABASE` for historical partitions
- [ ] Query routing based on timestamp ranges
- [ ] Automatic partition discovery on startup
- [ ] Error handling for archived data queries

**Interface:**
```go
// pkg/db/partition.go
type PartitionManager struct {
    currentDB *sql.DB
    attached  map[string]*sql.DB // week -> read-only connection
    currentWeek string
}

func (pm *PartitionManager) QueryBets(ctx context.Context, userID string, from, to time.Time) ([]models.Bet, error)
func (pm *PartitionManager) AttachPartition(week string, dbPath string) error
func (pm *PartitionManager) DetachPartition(week string) error
func (pm *PartitionManager) RotateToNewWeek(newWeek string) error
```

---

## Task 2: Redis Layer (ADR-004)

### Task 2.1: Implement Redis Client
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-004 (Redis for Cache, Sessions, Pub/Sub)  
**Source C4:** Redis container

**Description:**
Implement production-ready Redis client with connection pooling and pub/sub support.

**DoD:**
- [ ] Connection pool configuration
- [ ] Health check and connection retry
- [ ] Pub/Sub subscriber management
- [ ] Pipeline support for batch operations
- [ ] Context cancellation support

**Implementation:**
```go
// pkg/redis/client.go
package redis

import (
    "context"
    "fmt"
    "time"
    
    "github.com/redis/go-redis/v9"
)

type Config struct {
    Addr         string
    Password     string
    DB           int
    PoolSize     int
    MinIdleConns int
    MaxRetries   int
}

type Client struct {
    client *redis.Client
}

func New(cfg Config) (*Client, error) {
    rdb := redis.NewClient(&redis.Options{
        Addr:         cfg.Addr,
        Password:     cfg.Password,
        DB:           cfg.DB,
        PoolSize:     cfg.PoolSize,
        MinIdleConns: cfg.MinIdleConns,
        MaxRetries:   cfg.MaxRetries,
    })
    
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    if err := rdb.Ping(ctx).Err(); err != nil {
        return nil, fmt.Errorf("redis ping: %w", err)
    }
    
    return &Client{client: rdb}, nil
}

func (c *Client) Close() error {
    return c.client.Close()
}

func (c *Client) Get(ctx context.Context, key string) *redis.StringCmd {
    return c.client.Get(ctx, key)
}

func (c *Client) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) *redis.StatusCmd {
    return c.client.Set(ctx, key, value, ttl)
}

func (c *Client) SetNX(ctx context.Context, key string, value interface{}, ttl time.Duration) *redis.BoolCmd {
    return c.client.SetNX(ctx, key, value, ttl)
}

func (c *Client) Publish(ctx context.Context, channel string, message interface{}) *redis.IntCmd {
    return c.client.Publish(ctx, channel, message)
}

func (c *Client) Subscribe(ctx context.Context, channels ...string) *redis.PubSub {
    return c.client.Subscribe(ctx, channels...)
}

func (c *Client) Pipeline() redis.Pipeliner {
    return c.client.Pipeline()
}
```

---

### Task 2.2: Implement Price Cache
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-004  
**Source C4:** Redis (price storage)

**Description:**
Implement price caching with time-series storage in Redis sorted sets.

**DoD:**
- [ ] Store latest price with TTL
- [ ] Append price to sorted set with timestamp
- [ ] Query price history by time range
- [ ] Automatic pruning (Redis TTL handles expiration)

**Interface:**
```go
// pkg/redis/price.go
func (c *Client) SetPrice(ctx context.Context, symbol string, price float64) error
func (c *Client) GetLatestPrice(ctx context.Context, symbol string) (float64, error)
func (c *Client) GetPriceHistory(ctx context.Context, symbol string, from, to time.Time) ([]PricePoint, error)
```

---

## Task 3: Shared Components

### Task 3.1: Configuration Management
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All  
**Source C4:** All containers

**Description:**
Implement environment-based configuration with validation.

**DoD:**
- [ ] Struct-based config with env tags
- [ ] Validation for required fields
- [ ] Sensible defaults
- [ ] Documentation of all config options

**Config Struct:**
```go
// pkg/config/config.go
type Config struct {
    LogLevel string `env:"LOG_LEVEL" envDefault:"info"`
    
    Database struct {
        Path           string        `env:"DB_PATH" envDefault:"/data/bets.db"`
        MaxOpenConns   int           `env:"DB_MAX_OPEN_CONNS" envDefault:"20"`
        MaxIdleConns   int           `env:"DB_MAX_IDLE_CONNS" envDefault:"5"`
        ConnMaxLifetime time.Duration `env:"DB_CONN_MAX_LIFETIME" envDefault:"1h"`
        BusyTimeout    time.Duration `env:"DB_BUSY_TIMEOUT" envDefault:"10s"`
    }
    
    Redis struct {
        Addr         string `env:"REDIS_ADDR" envDefault:"localhost:6379"`
        Password     string `env:"REDIS_PASSWORD"`
        DB           int    `env:"REDIS_DB" envDefault:"0"`
        PoolSize     int    `env:"REDIS_POOL_SIZE" envDefault:"10"`
        MinIdleConns int    `env:"REDIS_MIN_IDLE_CONNS" envDefault:"2"`
    }
    
    // ... other config sections
}

func Load() (*Config, error)
```

---

### Task 3.2: Structured Logging
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All (Observability)  
**Source C4:** All containers

**Description:**
Implement structured JSON logging with request context propagation.

**DoD:**
- [ ] zerolog setup with configurable level
- [ ] Request ID injection middleware
- [ ] Component field for all logs
- [ ] Error stack trace capture

**Implementation:**
```go
// pkg/logger/logger.go
package logger

import (
    "os"
    "github.com/rs/zerolog"
)

type Logger struct {
    logger zerolog.Logger
}

func New(level string) Logger {
    l := zerolog.New(os.Stdout).With().Timestamp().Logger()
    // Set level...
    return Logger{logger: l}
}

func (l Logger) WithComponent(name string) Logger {
    return Logger{logger: l.logger.With().Str("component", name).Logger()}
}

func (l Logger) WithRequestID(id string) Logger {
    return Logger{logger: l.logger.With().Str("request_id", id).Logger()}
}

// Middleware for Echo
func Middleware(log Logger) echo.MiddlewareFunc
```

---

### Task 3.3: Domain Models
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-009, ADR-010  
**Source C4:** Data models

**Description:**
Define domain models for users, bets, and ledger entries.

**DoD:**
- [ ] User model with balance
- [ ] Bet model with status enum
- [ ] LedgerEntry model
- [ ] Validation methods on models
- [ ] JSON marshaling/unmarshaling

**Models:**
```go
// pkg/models/models.go
package models

import "time"

type User struct {
    ID            string    `json:"id" db:"id"`
    WalletAddress string    `json:"wallet_address" db:"wallet_address"`
    Balance       int64     `json:"balance" db:"balance"`
    Version       int       `json:"version" db:"version"`
    CreatedAt     time.Time `json:"created_at" db:"created_at"`
    UpdatedAt     time.Time `json:"updated_at" db:"updated_at"`
}

type BetStatus string

const (
    BetStatusPending           BetStatus = "pending"
    BetStatusConfirmed         BetStatus = "confirmed"
    BetStatusWon               BetStatus = "won"
    BetStatusLost              BetStatus = "lost"
    BetStatusInsufficientFunds BetStatus = "insufficient_funds"
)

type Bet struct {
    ID           string    `json:"id" db:"id"`
    UserID       string    `json:"user_id" db:"user_id"`
    CellID       string    `json:"cell_id" db:"cell_id"`
    Amount       int64     `json:"amount" db:"amount"`
    RewardRate   float64   `json:"reward_rate" db:"reward_rate"`
    Status       BetStatus `json:"status" db:"status"`
    PlacedAt     time.Time `json:"placed_at" db:"placed_at"`
    ResolvedAt   *time.Time `json:"resolved_at,omitempty" db:"resolved_at"`
    PayoutAmount *int64     `json:"payout_amount,omitempty" db:"payout_amount"`
}

type LedgerEntry struct {
    ID           string                 `json:"id" db:"id"`
    UserID       string                 `json:"user_id" db:"user_id"`
    Amount       int64                  `json:"amount" db:"amount"` // Negative for debit
    BalanceAfter int64                  `json:"balance_after" db:"balance_after"`
    RefType      string                 `json:"ref_type" db:"ref_type"`
    RefID        string                 `json:"ref_id" db:"ref_id"`
    Metadata     map[string]interface{} `json:"metadata,omitempty" db:"metadata"`
    CreatedAt    time.Time              `json:"created_at" db:"created_at"`
}
```

---

## Task 4: Integration Testing

### Task 4.1: Module Integration Tests
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** All  
**Source C4:** All containers

**Description:**
Write integration tests for the complete module.

**DoD:**
- [ ] SQLite client integration tests
- [ ] Redis client integration tests
- [ ] Partition manager tests
- [ ] Config loading tests
- [ ] All tests pass in CI

**Test Commands:**
```bash
cd /opt/app/backend
go test -v ./pkg/db/... -tags=integration
go test -v ./pkg/redis/... -tags=integration
go test -v ./pkg/config/...
```

---

## Module Deliverables

- [ ] SQLite client with WAL, pooling, retry logic
- [ ] Migration system with all schema files
- [ ] Partition manager for weekly rotation
- [ ] Redis client with pub/sub support
- [ ] Configuration management
- [ ] Structured logging
- [ ] Domain models (User, Bet, LedgerEntry)
- [ ] Integration tests passing

---

## Exit Criteria

This module is complete when:
1. Database layer handles concurrent access without SQLITE_BUSY errors
2. Redis pub/sub can broadcast messages across services
3. All models serialize/deserialize correctly
4. Integration tests pass reliably
5. Other modules can import and use these packages
----- END FILE: docs/implementation-plan/03-module-core-infrastructure.md -----


----- BEGIN FILE: docs/implementation-plan/04-module-auth-security.md -----
# Module 2: Authentication & Security (Vertical)

**Goal:** Implement Web3 signature-based authentication with replay protection.

**Duration:** Days 4-5  
**Dependencies:** Module 1 (Core Infrastructure)  
**Priority:** HIGH (required for bet placement)

---

## Module Scope

This module implements:
- EIP-191 signature validation
- Timestamp + nonce replay protection
- Session management
- Rate limiting

**Source ADRs:** ADR-007 (Replay Protection)  
**Source C4:** Signature Validator, Session Manager containers

---

## Task 1: Signature Validation (ADR-007)

### Task 1.1: Implement EIP-191 Signature Recovery
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-007 (Replay Protection for Web3 Auth)  
**Source C4:** Signature Validator

**Description:**
Implement Ethereum signature verification using EIP-191 personal_sign standard.

**DoD:**
- [ ] Recover address from signature
- [ ] Verify signature format (65 bytes: r, s, v)
- [ ] EIP-191 prefix validation
- [ ] Address normalization (checksum vs lowercase)
- [ ] Unit tests with known good/bad signatures

**Implementation:**
```go
// pkg/auth/signature.go
package auth

import (
    "crypto/ecdsa"
    "fmt"
    "strings"
    
    "github.com/ethereum/go-ethereum/common"
    "github.com/ethereum/go-ethereum/crypto"
)

type SignatureValidator struct {
    // dependencies
}

type BetSignaturePayload struct {
    Action          string `json:"action"`
    CellID          string `json:"cell_id"`
    Amount          string `json:"amount"`
    Nonce           string `json:"nonce"`
    Timestamp       int64  `json:"timestamp"`
    ChainID         int    `json:"chain_id"`
    ContractVersion string `json:"contract_version"`
}

func (v *SignatureValidator) FormatMessage(payload BetSignaturePayload) string {
    return fmt.Sprintf(`Tap Trading Bet
Action: %s
Cell: %s
Amount: %s
Nonce: %s
Timestamp: %d
Chain: %d
Version: %s`,
        payload.Action,
        payload.CellID,
        payload.Amount,
        payload.Nonce,
        payload.Timestamp,
        payload.ChainID,
        payload.ContractVersion,
    )
}

func (v *SignatureValidator) RecoverAddress(message string, signature []byte) (string, error) {
    // EIP-191 message prefix
    prefixed := fmt.Sprintf("\x19Ethereum Signed Message:\n%d%s", len(message), message)
    msgHash := crypto.Keccak256Hash([]byte(prefixed))
    
    // Handle signature malleability (v can be 27/28 or 0/1)
    if len(signature) != 65 {
        return "", fmt.Errorf("invalid signature length: %d", len(signature))
    }
    
    sig := make([]byte, 65)
    copy(sig, signature)
    
    // Adjust v value
    if sig[64] >= 27 {
        sig[64] -= 27
    }
    
    pubKey, err := crypto.SigToPub(msgHash.Bytes(), sig)
    if err != nil {
        return "", fmt.Errorf("recover public key: %w", err)
    }
    
    address := crypto.PubkeyToAddress(*pubKey)
    return address.Hex(), nil
}

func (v *SignatureValidator) ValidateSignature(payload BetSignaturePayload, signature string, expectedAddress string) error {
    // 1. Reconstruct message
    message := v.FormatMessage(payload)
    
    // 2. Decode signature (hex)
    sigBytes, err := hexutil.Decode(signature)
    if err != nil {
        return fmt.Errorf("decode signature: %w", err)
    }
    
    // 3. Recover address
    recovered, err := v.RecoverAddress(message, sigBytes)
    if err != nil {
        return fmt.Errorf("recover address: %w", err)
    }
    
    // 4. Compare addresses (case-insensitive)
    if !strings.EqualFold(recovered, expectedAddress) {
        return fmt.Errorf("signature mismatch: recovered %s, expected %s", recovered, expectedAddress)
    }
    
    return nil
}
```

**Tests:**
- [ ] Valid signature verification
- [ ] Invalid signature rejection
- [ ] Wrong address rejection
- [ ] Signature malleability handling

---

## Task 2: Replay Protection (ADR-007)

### Task 2.1: Implement Timestamp Validation
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-007  
**Source C4:** Signature Validator

**Description:**
Validate request timestamp freshness to prevent replay of old signatures.

**DoD:**
- [ ] 30-second window (clock drift tolerance)
- [ ] Reject requests with old timestamps
- [ ] Reject requests with future timestamps
- [ ] Configurable clock skew tolerance

**Implementation:**
```go
// pkg/auth/replay.go
const (
    TimestampWindow = 30 * time.Second
)

func (v *SignatureValidator) ValidateTimestamp(timestamp int64) error {
    now := time.Now().Unix()
    diff := abs(now - timestamp)
    
    if diff > int64(TimestampWindow.Seconds()) {
        return fmt.Errorf("timestamp expired: diff=%ds", diff)
    }
    
    if timestamp > now+5 { // Small future tolerance
        return fmt.Errorf("timestamp in future")
    }
    
    return nil
}
```

---

### Task 2.2: Implement Nonce Tracking
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-007  
**Source C4:** Signature Validator ‚Üí Redis

**Description:**
Implement nonce uniqueness check using Redis SET NX.

**DoD:**
- [ ] Redis SET NX for atomic check-and-set
- [ ] 90-second TTL (ADR-007 hardening fix, not 24h)
- [ ] Nonce key format: `nonce:{uuid}`
- [ ] Duplicate nonce rejection

**Implementation:**
```go
// pkg/auth/nonce.go
package auth

import (
    "context"
    "fmt"
    "time"
    
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    NonceTTL = 90 * time.Second // Hardened: was 24h, now 90s
)

type NonceStore struct {
    redis *redis.Client
}

func NewNonceStore(redis *redis.Client) *NonceStore {
    return &NonceStore{redis: redis}
}

func (s *NonceStore) CheckAndStore(ctx context.Context, nonce string, userID string) error {
    key := fmt.Sprintf("nonce:%s", nonce)
    
    // SET key value NX EX ttl
    ok, err := s.redis.SetNX(ctx, key, userID, NonceTTL).Result()
    if err != nil {
        return fmt.Errorf("redis error: %w", err)
    }
    
    if !ok {
        return fmt.Errorf("nonce already used: %s", nonce)
    }
    
    return nil
}
```

**‚ö†Ô∏è Critical:**
- TTL is 90 seconds (30s window + 60s buffer)
- This reduces memory from ~13GB to ~13MB at 3K RPS

---

## Task 3: Session Management

### Task 3.1: Implement Session Store
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-004 (Redis for Sessions)  
**Source C4:** Session Manager

**Description:**
Implement session management for authenticated users.

**DoD:**
- [ ] Session creation on successful auth
- [ ] 24-hour session TTL
- [ ] Session validation middleware
- [ ] Session revocation (logout)

**Implementation:**
```go
// pkg/auth/session.go
package auth

import (
    "context"
    "crypto/rand"
    "encoding/hex"
    "fmt"
    "time"
    
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    SessionTTL = 24 * time.Hour
)

type Session struct {
    ID        string    `json:"id"`
    UserID    string    `json:"user_id"`
    Wallet    string    `json:"wallet"`
    CreatedAt time.Time `json:"created_at"`
}

type SessionStore struct {
    redis *redis.Client
}

func NewSessionStore(redis *redis.Client) *SessionStore {
    return &SessionStore{redis: redis}
}

func (s *SessionStore) Create(ctx context.Context, userID, wallet string) (*Session, error) {
    sessionID := generateSessionID()
    
    session := &Session{
        ID:        sessionID,
        UserID:    userID,
        Wallet:    wallet,
        CreatedAt: time.Now(),
    }
    
    key := fmt.Sprintf("session:%s", sessionID)
    // Store session in Redis
    err := s.redis.Set(ctx, key, session, SessionTTL).Err()
    if err != nil {
        return nil, fmt.Errorf("store session: %w", err)
    }
    
    // Also index by wallet for lookup
    walletKey := fmt.Sprintf("session:wallet:%s", wallet)
    s.redis.Set(ctx, walletKey, sessionID, SessionTTL)
    
    return session, nil
}

func (s *SessionStore) Get(ctx context.Context, sessionID string) (*Session, error) {
    key := fmt.Sprintf("session:%s", sessionID)
    
    var session Session
    err := s.redis.Get(ctx, key).Scan(&session)
    if err != nil {
        return nil, fmt.Errorf("get session: %w", err)
    }
    
    return &session, nil
}

func (s *SessionStore) Delete(ctx context.Context, sessionID string) error {
    key := fmt.Sprintf("session:%s", sessionID)
    
    // Get wallet first for index cleanup
    var session Session
    err := s.redis.Get(ctx, key).Scan(&session)
    if err == nil {
        walletKey := fmt.Sprintf("session:wallet:%s", session.Wallet)
        s.redis.Del(ctx, walletKey)
    }
    
    return s.redis.Del(ctx, key).Err()
}

func generateSessionID() string {
    b := make([]byte, 32)
    rand.Read(b)
    return hex.EncodeToString(b)
}
```

---

### Task 3.2: Auth Middleware
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-007  
**Source C4:** API Server

**Description:**
Implement Echo middleware for session validation.

**DoD:**
- [ ] Extract session from header/cookie
- [ ] Validate session exists and not expired
- [ ] Inject user context into request
- [ ] Return 401 for invalid sessions

**Implementation:**
```go
// pkg/auth/middleware.go
package auth

import (
    "net/http"
    "strings"
    
    "github.com/labstack/echo/v4"
)

func (s *SessionStore) Middleware() echo.MiddlewareFunc {
    return func(next echo.HandlerFunc) echo.HandlerFunc {
        return func(c echo.Context) error {
            // Extract session from Authorization header
            auth := c.Request().Header.Get("Authorization")
            if auth == "" {
                return c.JSON(http.StatusUnauthorized, map[string]string{
                    "error": "missing authorization header",
                })
            }
            
            parts := strings.SplitN(auth, " ", 2)
            if len(parts) != 2 || strings.ToLower(parts[0]) != "bearer" {
                return c.JSON(http.StatusUnauthorized, map[string]string{
                    "error": "invalid authorization format",
                })
            }
            
            sessionID := parts[1]
            session, err := s.Get(c.Request().Context(), sessionID)
            if err != nil {
                return c.JSON(http.StatusUnauthorized, map[string]string{
                    "error": "invalid or expired session",
                })
            }
            
            // Inject user into context
            c.Set("user_id", session.UserID)
            c.Set("wallet", session.Wallet)
            
            return next(c)
        }
    }
}
```

---

## Task 4: Rate Limiting

### Task 4.1: Implement Rate Limiter
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-004 (Redis for rate limiting)  
**Source C4:** API Server

**Description:**
Implement per-wallet rate limiting for bet placement.

**DoD:**
- [ ] Redis-based sliding window or token bucket
- [ ] Configurable rate limit (e.g., 10 bets/minute)
- [ ] Return 429 with Retry-After header when limited

**Implementation:**
```go
// pkg/auth/ratelimit.go
package auth

import (
    "context"
    "fmt"
    "time"
    
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    RateLimitRequests = 10
    RateLimitWindow   = 60 * time.Second
)

type RateLimiter struct {
    redis *redis.Client
}

func NewRateLimiter(redis *redis.Client) *RateLimiter {
    return &RateLimiter{redis: redis}
}

func (r *RateLimiter) Allow(ctx context.Context, wallet string) (bool, error) {
    key := fmt.Sprintf("rate_limit:%s", wallet)
    
    pipe := r.redis.Pipeline()
    incr := pipe.Incr(ctx, key)
    pipe.Expire(ctx, key, RateLimitWindow)
    
    _, err := pipe.Exec(ctx)
    if err != nil {
        return false, err
    }
    
    count := incr.Val()
    return count <= RateLimitRequests, nil
}
```

---

## Task 5: Complete Auth Flow

### Task 5.1: Implement Login Endpoint
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-007  
**Source C4:** API Server

**Description:**
Create login endpoint that validates Web3 signature and creates session.

**DoD:**
- [ ] `POST /api/auth/login` endpoint
- [ ] Validate signature
- [ ] Validate timestamp + nonce
- [ ] Create or get user record
- [ ] Create session
- [ ] Return session token

**Request/Response:**
```json
// POST /api/auth/login
{
  "wallet": "0x...",
  "payload": { /* BetSignaturePayload */ },
  "signature": "0x..."
}

// Response
{
  "session_id": "sess_abc123",
  "user": {
    "id": "...",
    "wallet": "0x...",
    "balance": 10000
  },
  "expires_at": "2026-02-04T12:00:00Z"
}
```

---

### Task 5.2: Auth Integration Tests
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-007  
**Source C4:** All auth components

**Description:**
Write comprehensive tests for auth flow.

**DoD:**
- [ ] Valid login test
- [ ] Invalid signature rejection
- [ ] Expired timestamp rejection
- [ ] Reused nonce rejection
- [ ] Rate limit enforcement
- [ ] Session validation
- [ ] Logout test

---

## Module Deliverables

- [ ] EIP-191 signature validation
- [ ] Timestamp freshness validation (30s window)
- [ ] Nonce tracking with 90s TTL
- [ ] Session management (24h TTL)
- [ ] Rate limiting per wallet
- [ ] Login endpoint
- [ ] Auth middleware
- [ ] Integration tests

---

## Exit Criteria

This module is complete when:
1. Valid Web3 signatures are accepted
2. Replayed signatures (old timestamp or used nonce) are rejected
3. Sessions are created and can be validated
4. Rate limits are enforced
5. All auth integration tests pass
6. Bet placement can use auth middleware

---

## Security Checklist

- [ ] Signature uses EIP-191 standard
- [ ] Nonce TTL is 90s (not 24h)
- [ ] Timestamp window is 30s
- [ ] Rate limits are enforced
- [ ] Sessions use cryptographically random IDs
- [ ] No sensitive data in logs
----- END FILE: docs/implementation-plan/04-module-auth-security.md -----


----- BEGIN FILE: docs/implementation-plan/05-module-bet-ingestion.md -----
# Module 3: Bet Ingestion (Vertical)

**Goal:** Implement the complete bet placement flow with write batching, conditional balance deduction, and hybrid ledger.

**Duration:** Days 5-7  
**Dependencies:** Module 1 (Core), Module 2 (Auth)  
**Priority:** CRITICAL (core business logic)

---

## Module Scope

This module implements:
- Write batching channel with flush logic
- Conditional SQL balance deduction (ADR-009)
- Hybrid transaction ledger (ADR-010)
- Bet placement API endpoint
- WebSocket confirmation

**Source ADRs:** ADR-005 (Write Batching), ADR-009 (Conditional Deduction), ADR-010 (Hybrid Ledger)  
**Source C4:** Bet Batcher, API Server, WebSocket Server

> **‚ö†Ô∏è Critical:** ADR-010 EXTENDS ADR-009 ‚Äî both must be implemented together in the same transaction.

---

## Task 1: Write Batching (ADR-005)

### Task 1.1: Implement Bet Batcher Core
**Owner:** Backend Engineer  
**Estimated:** 5 hours  
**Source ADR:** ADR-005 (Write Batching for Bet Ingestion)  
**Source C4:** Bet Batcher container

**Description:**
Implement channel-based bet batching with size and time triggers.

**DoD:**
- [ ] Buffered channel for bet submission (10K capacity)
- [ ] Batch flusher goroutine
- [ ] Size trigger: 100 items
- [ ] Time trigger: 100ms timeout
- [ ] Per-bet result signaling via channel
- [ ] Graceful shutdown with drain

**Implementation:**
```go
// pkg/batcher/batcher.go
package batcher

import (
    "context"
    "database/sql"
    "sync"
    "time"
    
    "github.com/tap-trading/backend/pkg/db"
    "github.com/tap-trading/backend/pkg/models"
)

const (
    BatchSize     = 100
    BatchTimeout  = 100 * time.Millisecond
    ChannelBuffer = 10000
)

type PendingBet struct {
    BetID       string
    UserID      string
    CellID      string
    Amount      int64
    RewardRate  float64
    ResultChan  chan BetResult
}

type BetResult struct {
    Success    bool
    Status     string
    NewBalance int64
    Error      error
}

type Batcher struct {
    db       *db.Client
    redis    *redis.Client
    channel  chan PendingBet
    shutdown chan struct{}
    wg       sync.WaitGroup
}

func New(db *db.Client, redis *redis.Client) *Batcher {
    return &Batcher{
        db:       db,
        redis:    redis,
        channel:  make(chan PendingBet, ChannelBuffer),
        shutdown: make(chan struct{}),
    }
}

func (b *Batcher) Start() {
    b.wg.Add(1)
    go b.run()
}

func (b *Batcher) Stop() {
    close(b.shutdown)
    b.wg.Wait()
}

func (b *Batcher) Submit(bet PendingBet) {
    select {
    case b.channel <- bet:
        // Submitted successfully
    default:
        // Channel full - apply backpressure
        bet.ResultChan <- BetResult{
            Success: false,
            Error:   fmt.Errorf("batcher overloaded"),
        }
    }
}

func (b *Batcher) run() {
    defer b.wg.Done()
    
    ticker := time.NewTicker(BatchTimeout)
    defer ticker.Stop()
    
    batch := make([]PendingBet, 0, BatchSize)
    
    for {
        select {
        case bet := <-b.channel:
            batch = append(batch, bet)
            
            if len(batch) >= BatchSize {
                b.flush(batch)
                batch = batch[:0]
                ticker.Reset(BatchTimeout)
            }
            
        case <-ticker.C:
            if len(batch) > 0 {
                b.flush(batch)
                batch = batch[:0]
            }
            
        case <-b.shutdown:
            // Drain remaining
            for len(b.channel) > 0 && len(batch) < BatchSize {
                select {
                case bet := <-b.channel:
                    batch = append(batch, bet)
                default:
                }
            }
            if len(batch) > 0 {
                b.flush(batch)
            }
            return
        }
    }
}

func (b *Batcher) flush(bets []PendingBet) {
    // Implementation in Task 1.2
}
```

---

### Task 1.2: Implement Flush with Conditional Deduction + Ledger (ADR-009 + ADR-010)
**Owner:** Backend Engineer  
**Estimated:** 6 hours  
**Source ADR:** ADR-009 (Conditional Deduction), ADR-010 (Hybrid Ledger)  
**Source C4:** Bet Batcher ‚Üí SQLite

**Description:**
Implement the batched flush with conditional SQL balance deduction and hybrid ledger write. This is the critical financial transaction.

**DoD:**
- [ ] Single transaction for entire batch
- [ ] Conditional UPDATE: `balance >= amount` check
- [ ] Ledger INSERT with `balance_after` snapshot
- [ ] Bet INSERT with status
- [ ] RowsAffected check for insufficient funds
- [ ] Best-effort Redis balance sync (post-commit)

**Implementation:**
```go
// pkg/batcher/flush.go
package batcher

import (
    "context"
    "database/sql"
    "encoding/json"
    "fmt"
    "time"
    
    "github.com/oklog/ulid/v2"
)

const (
    StatusConfirmed         = "confirmed"
    StatusInsufficientFunds = "insufficient_funds"
)

func (b *Batcher) flush(bets []PendingBet) {
    ctx := context.Background()
    
    tx, err := b.db.BeginTx(ctx, &sql.TxOptions{
        Isolation: sql.LevelSerializable,
    })
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer tx.Rollback()

    // Prepare statements
    deductStmt, err := tx.PrepareContext(ctx, `
        UPDATE users 
        SET balance = balance - ?,
            version = version + 1,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = ? 
        AND balance >= ?
        RETURNING balance
    `)
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer deductStmt.Close()
    
    ledgerStmt, err := tx.PrepareContext(ctx, `
        INSERT INTO transaction_ledger 
        (id, user_id, amount, balance_after, ref_type, ref_id, metadata, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `)
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer ledgerStmt.Close()
    
    betStmt, err := tx.PrepareContext(ctx, `
        INSERT INTO bets 
        (id, user_id, cell_id, amount, reward_rate, status, placed_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    `)
    if err != nil {
        b.failAll(bets, err)
        return
    }
    defer betStmt.Close()

    results := make(map[string]BetResult)
    now := time.Now()

    for _, bet := range bets {
        // Step 1: Conditional deduction with RETURNING
        var newBalance int64
        err := deductStmt.QueryRowContext(ctx, bet.Amount, bet.UserID, bet.Amount).Scan(&newBalance)
        
        if err == sql.ErrNoRows {
            // Insufficient funds - record failed bet
            _, _ = betStmt.ExecContext(ctx,
                bet.BetID, bet.UserID, bet.CellID, bet.Amount,
                bet.RewardRate, StatusInsufficientFunds, now,
            )
            
            results[bet.BetID] = BetResult{
                Success: false,
                Status:  StatusInsufficientFunds,
                Error:   fmt.Errorf("insufficient funds"),
            }
            continue
        }
        
        if err != nil {
            results[bet.BetID] = BetResult{
                Success: false,
                Error:   fmt.Errorf("deduction failed: %w", err),
            }
            continue
        }

        // Step 2: Record in ledger (ADR-010)
        ledgerID := ulid.Make().String()
        metadata := map[string]interface{}{
            "cell_id":   bet.CellID,
            "timestamp": now.Unix(),
        }
        metadataJSON, _ := json.Marshal(metadata)
        
        _, err = ledgerStmt.ExecContext(ctx,
            ledgerID,
            bet.UserID,
            -bet.Amount,  // Negative for debit
            newBalance,   // Balance after deduction
            "BET",
            bet.BetID,
            metadataJSON,
            now,
        )
        if err != nil {
            results[bet.BetID] = BetResult{
                Success: false,
                Error:   fmt.Errorf("ledger insert failed: %w", err),
            }
            continue
        }

        // Step 3: Record bet
        _, err = betStmt.ExecContext(ctx,
            bet.BetID,
            bet.UserID,
            bet.CellID,
            bet.Amount,
            bet.RewardRate,
            StatusConfirmed,
            now,
        )
        if err != nil {
            results[bet.BetID] = BetResult{
                Success: false,
                Error:   fmt.Errorf("bet insert failed: %w", err),
            }
            continue
        }

        results[bet.BetID] = BetResult{
            Success:    true,
            Status:     StatusConfirmed,
            NewBalance: newBalance,
        }
    }

    // Commit transaction
    if err := tx.Commit(); err != nil {
        b.failAll(bets, err)
        return
    }

    // Signal results to waiting goroutines
    for _, bet := range bets {
        select {
        case bet.ResultChan <- results[bet.BetID]:
        case <-time.After(time.Second):
            // Client not listening, log warning
        }
    }

    // Best-effort Redis sync (non-critical)
    b.syncRedisBalances(bets, results)
}

func (b *Batcher) failAll(bets []PendingBet, err error) {
    for _, bet := range bets {
        select {
        case bet.ResultChan <- BetResult{Success: false, Error: err}:
        default:
        }
    }
}

func (b *Batcher) syncRedisBalances(bets []PendingBet, results map[string]BetResult) {
    ctx := context.Background()
    pipe := b.redis.Pipeline()
    
    for _, bet := range bets {
        result := results[bet.BetID]
        if result.Success {
            key := fmt.Sprintf("balance:%s", bet.UserID)
            pipe.Set(ctx, key, result.NewBalance, 24*time.Hour)
        }
    }
    
    // Fire and forget - Redis is cache, not source of truth
    _, _ = pipe.Exec(ctx)
}
```

**‚ö†Ô∏è Critical Implementation Notes:**
1. **Single Transaction:** All three operations (UPDATE, INSERT ledger, INSERT bet) must be in one transaction
2. **Conditional UPDATE:** `RETURNING balance` gives us new balance atomically
3. **Ledger Always Written:** Even on failure, we could log a ledger entry (optional)
4. **Redis is Cache Only:** Redis sync happens AFTER commit and is best-effort

---

## Task 2: Bet Placement API

### Task 2.1: Implement Bet Handler
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-005, ADR-009, ADR-010  
**Source C4:** API Server

**Description:**
Implement the complete bet placement endpoint with all validations.

**DoD:**
- [ ] `POST /api/bets` endpoint
- [ ] Auth middleware integration
- [ ] Request validation (cell_id, amount)
- [ ] Grid availability check
- [ ] Balance check (Redis cache - early rejection)
- [ ] Submit to batcher
- [ ] Wait for result with timeout
- [ ] Return 202 Accepted or error

**Implementation:**
```go
// cmd/api/handlers/bet.go
package handlers

import (
    "net/http"
    "time"
    
    "github.com/labstack/echo/v4"
    "github.com/tap-trading/backend/pkg/batcher"
    "github.com/tap-trading/backend/pkg/redis"
)

type BetRequest struct {
    CellID     string  `json:"cell_id" validate:"required"`
    Amount     int64   `json:"amount" validate:"required,min=1"`
    RewardRate float64 `json:"reward_rate"`
}

type BetHandler struct {
    batcher *batcher.Batcher
    redis   *redis.Client
}

func NewBetHandler(batcher *batcher.Batcher, redis *redis.Client) *BetHandler {
    return &BetHandler{batcher: batcher, redis: redis}
}

func (h *BetHandler) PlaceBet(c echo.Context) error {
    ctx := c.Request().Context()
    userID := c.Get("user_id").(string)
    
    var req BetRequest
    if err := c.Bind(&req); err != nil {
        return c.JSON(http.StatusBadRequest, map[string]string{
            "error": "invalid request body",
        })
    }
    
    // Validate request
    if err := c.Validate(&req); err != nil {
        return c.JSON(http.StatusBadRequest, map[string]string{
            "error": err.Error(),
        })
    }
    
    // Check cell availability (via Redis cache)
    // This would check grid state - implementation depends on grid module
    
    // Early balance check via Redis cache (not authoritative)
    balanceKey := fmt.Sprintf("balance:%s", userID)
    cachedBalance, err := h.redis.Get(ctx, balanceKey).Int64()
    if err == nil && cachedBalance < req.Amount {
        return c.JSON(http.StatusBadRequest, map[string]string{
            "error": "insufficient_funds",
        })
    }
    
    // Generate bet ID
    betID := ulid.Make().String()
    
    // Submit to batcher
    resultChan := make(chan batcher.BetResult, 1)
    h.batcher.Submit(batcher.PendingBet{
        BetID:      betID,
        UserID:     userID,
        CellID:     req.CellID,
        Amount:     req.Amount,
        RewardRate: req.RewardRate,
        ResultChan: resultChan,
    })
    
    // Wait for result with timeout
    select {
    case result := <-resultChan:
        if !result.Success {
            if result.Status == batcher.StatusInsufficientFunds {
                return c.JSON(http.StatusBadRequest, map[string]string{
                    "error": "insufficient_funds",
                })
            }
            return c.JSON(http.StatusInternalServerError, map[string]string{
                "error": "internal_error",
                "message": result.Error.Error(),
            })
        }
        
        // Return 202 Accepted
        return c.JSON(http.StatusAccepted, map[string]interface{}{
            "status":      "accepted",
            "bet_id":      betID,
            "new_balance": result.NewBalance,
        })
        
    case <-time.After(5 * time.Second):
        // Timeout - bet may or may not be processed
        return c.JSON(http.StatusGatewayTimeout, map[string]string{
            "error":   "timeout",
            "message": "Bet status unknown, check history",
            "bet_id":  betID,
        })
    }
}
```

---

### Task 2.2: WebSocket Confirmation
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-005 (Optimistic UI)  
**Source C4:** WebSocket Server

**Description:**
Implement WebSocket notification when bet is confirmed.

**DoD:**
- [ ] WebSocket server subscribes to bet confirmations
- [ ] Channel: `user:{user_id}:bets`
- [ ] Broadcast `bet:confirmed` event
- [ ] Frontend updates grid state

**Implementation:**
```go
// cmd/ws/hub.go
package main

// After batcher confirms bet, publish to Redis
type BetConfirmation struct {
    BetID      string `json:"bet_id"`
    UserID     string `json:"user_id"`
    CellID     string `json:"cell_id"`
    Status     string `json:"status"`
    NewBalance int64  `json:"new_balance"`
}

func (b *Batcher) notifyConfirmation(bet PendingBet, result BetResult) {
    if !result.Success {
        return
    }
    
    confirmation := BetConfirmation{
        BetID:      bet.BetID,
        UserID:     bet.UserID,
        CellID:     bet.CellID,
        Status:     result.Status,
        NewBalance: result.NewBalance,
    }
    
    data, _ := json.Marshal(confirmation)
    channel := fmt.Sprintf("user:%s:bets", bet.UserID)
    b.redis.Publish(context.Background(), channel, data)
}
```

---

## Task 3: Grid State Management

### Task 3.1: Implement Grid Cache
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** PRD (Grid betting rules)  
**Source C4:** Redis (grid cache)

**Description:**
Cache grid state in Redis for fast cell availability checks.

**DoD:**
- [ ] Store occupied cells per user
- [ ] Check cell availability before bet submission
- [ ] TTL-based expiration for past columns

**Implementation:**
```go
// pkg/grid/grid.go
package grid

type GridCache struct {
    redis *redis.Client
}

func (g *GridCache) IsCellAvailable(ctx context.Context, userID, cellID string) (bool, error) {
    key := fmt.Sprintf("user_cells:%s", userID)
    // Check if cell is in user's occupied set
    exists, err := g.redis.SIsMember(ctx, key, cellID).Result()
    return !exists, err
}

func (g *GridCache) MarkCellOccupied(ctx context.Context, userID, cellID string, ttl time.Duration) error {
    key := fmt.Sprintf("user_cells:%s", userID)
    pipe := g.redis.Pipeline()
    pipe.SAdd(ctx, key, cellID)
    pipe.Expire(ctx, key, ttl)
    _, err := pipe.Exec(ctx)
    return err
}
```

---

## Task 4: Testing

### Task 4.1: Batcher Unit Tests
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-005, ADR-009, ADR-010  
**Source C4:** Bet Batcher

**Description:**
Write comprehensive tests for batcher logic.

**DoD:**
- [ ] Batch flush on size trigger
- [ ] Batch flush on timeout
- [ ] Insufficient funds handling
- [ ] Concurrent bet submission
- [ ] Transaction rollback on error
- [ ] Ledger entry creation verified

---

### Task 4.2: Double-Spend Prevention Test
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-009  
**Source C4:** Bet Batcher

**Description:**
Verify double-spend cannot occur with concurrent requests.

**DoD:**
- [ ] Simulate 100 concurrent bets from same user
- [ ] Verify balance never goes negative
- [ ] Verify sum of bets <= initial balance

**Test:**
```go
func TestDoubleSpendPrevention(t *testing.T) {
    // Setup: User with $100 balance
    // Launch 100 goroutines trying to bet $10 each
    // Only 10 should succeed
    // Balance should be exactly $0 after
}
```

---

### Task 4.3: Integration Load Test
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-005 (3K RPS target)  
**Source C4:** Full flow

**Description:**
Verify system handles 3K RPS with batching.

**DoD:**
- [ ] 3,000 concurrent requests/second
- [ ] p99 latency < 100ms
- [ ] No SQLITE_BUSY errors
- [ ] All bets processed or correctly rejected

---

## Module Deliverables

- [ ] Bet batcher with size/time triggers
- [ ] Conditional SQL balance deduction
- [ ] Hybrid transaction ledger (same transaction)
- [ ] Bet placement API endpoint
- [ ] WebSocket confirmation publishing
- [ ] Grid state caching
- [ ] Unit and integration tests
- [ ] Double-spend prevention verified
- [ ] 3K RPS load test passing

---

## Exit Criteria

This module is complete when:
1. Bets are batched and flushed correctly
2. Conditional deduction prevents overdrafts
3. Ledger entries are created atomically with bets
4. WebSocket confirmations are sent
5. Double-spend is impossible (verified by test)
6. 3K RPS sustained with <100ms p99 latency
7. Zero SQLITE_BUSY errors

---

## Financial Integrity Checklist

- [ ] Conditional UPDATE with `balance >= amount`
- [ ] RowsAffected checked
- [ ] Ledger written in same transaction
- [ ] `balance_after` snapshot recorded
- [ ] Redis sync is post-commit and best-effort
- [ ] Daily integrity check query exists
----- END FILE: docs/implementation-plan/05-module-bet-ingestion.md -----


----- BEGIN FILE: docs/implementation-plan/06-module-price-feed.md -----
# Module 4: Price Feed (Vertical)

**Goal:** Implement real-time price ingestion, circuit breaker, and WebSocket distribution.

**Duration:** Days 7-8  
**Dependencies:** Module 1 (Core)  
**Priority:** HIGH (required for fair betting)

---

## Module Scope

This module implements:
- External price feed ingestion
- Redis pub/sub price distribution
- Circuit breaker for stale prices (ADR-008)
- Price health monitoring
- WebSocket price streaming

**Source ADRs:** ADR-004 (Redis Pub/Sub), ADR-008 (Circuit Breaker)  
**Source C4:** Price Ingestion, Price Health Monitor, WebSocket Server

---

## Task 1: Price Ingestion

### Task 1.1: Implement Price Feed Client
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** PRD (Real-time price streaming)  
**Source C4:** Price Ingestion container

**Description:**
Implement WebSocket client for external price feed (Binance/Coinbase).

**DoD:**
- [ ] Connect to Binance/Coinbase WebSocket
- [ ] Handle connection drops with auto-reconnect
- [ ] Parse price messages
- [ ] Publish to Redis pub/sub
- [ ] Store in Redis sorted set for history
- [ ] Connection health logging

**Implementation:**
```go
// cmd/price/ingestion.go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "net/url"
    "time"

    "github.com/gorilla/websocket"
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    BinanceWSURL = "wss://stream.binance.com:9443/ws/btcusdt@trade"
    ReconnectDelay = 5 * time.Second
)

type PriceFeed struct {
    redis    *redis.Client
    health   *HealthMonitor
    shutdown chan struct{}
}

type TradeMessage struct {
    Symbol    string `json:"s"`
    Price     string `json:"p"`
    Time      int64  `json:"T"`
}

func (f *PriceFeed) Start() {
    for {
        select {
        case <-f.shutdown:
            return
        default:
        }
        
        err := f.connectAndStream()
        if err != nil {
            log.Error().Err(err).Msg("price feed error, reconnecting...")
        }
        
        select {
        case <-f.shutdown:
            return
        case <-time.After(ReconnectDelay):
        }
    }
}

func (f *PriceFeed) connectAndStream() error {
    u, _ := url.Parse(BinanceWSURL)
    
    conn, _, err := websocket.DefaultDialer.Dial(u.String(), nil)
    if err != nil {
        return fmt.Errorf("dial: %w", err)
    }
    defer conn.Close()
    
    log.Info().Msg("price feed connected")
    
    for {
        select {
        case <-f.shutdown:
            return nil
        default:
        }
        
        conn.SetReadDeadline(time.Now().Add(30 * time.Second))
        
        _, message, err := conn.ReadMessage()
        if err != nil {
            return fmt.Errorf("read: %w", err)
        }
        
        var trade TradeMessage
        if err := json.Unmarshal(message, &trade); err != nil {
            log.Error().Err(err).RawJSON("message", message).Msg("unmarshal failed")
            continue
        }
        
        // Parse and publish
        price := parsePrice(trade.Price)
        timestamp := time.UnixMilli(trade.Time)
        
        if err := f.publishPrice(price, timestamp); err != nil {
            log.Error().Err(err).Msg("publish failed")
        }
    }
}

func (f *PriceFeed) publishPrice(price float64, timestamp time.Time) error {
    ctx := context.Background()
    priceStr := fmt.Sprintf("%.2f", price)
    
    pipe := f.redis.Pipeline()
    
    // Publish to pub/sub for real-time distribution
    pipe.Publish(ctx, "prices:btc:usd", priceStr)
    
    // Store latest price
    pipe.Set(ctx, "price:btc:usd", priceStr, 1*time.Hour)
    
    // Add to sorted set for history (7-day TTL via Redis config)
    pipe.ZAdd(ctx, "prices:btc:usd:ts", redis.Z{
        Score:  float64(timestamp.Unix()),
        Member: priceStr,
    })
    
    // Trim old entries (keep 7 days worth)
    weekAgo := time.Now().Add(-7 * 24 * time.Hour).Unix()
    pipe.ZRemRangeByScore(ctx, "prices:btc:usd:ts", "0", fmt.Sprintf("%d", weekAgo))
    
    _, err := pipe.Exec(ctx)
    
    // Update health monitor
    f.health.RecordPrice(timestamp)
    
    return err
}
```

---

## Task 2: Circuit Breaker (ADR-008)

### Task 2.1: Implement Price Health Monitor
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-008 (Stale Price Circuit Breaker)  
**Source C4:** Price Health Monitor

**Description:**
Implement health monitoring that tracks price freshness.

**DoD:**
- [ ] Subscribe to price updates
- [ ] Track last update timestamp
- [ ] Evaluate health status (Healthy/Degraded/Critical)
- [ ] Thresholds: 3s = Degraded, 10s = Critical

**Implementation:**
```go
// pkg/circuit/health.go
package circuit

import (
    "context"
    "sync/atomic"
    "time"
    
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    StaleThreshold    = 3 * time.Second
    CriticalThreshold = 10 * time.Second
)

type HealthStatus int32

const (
    Healthy HealthStatus = iota
    Degraded
    Critical
)

type HealthMonitor struct {
    redis       *redis.Client
    lastUpdated atomic.Int64 // Unix nano
    status      atomic.Int32 // HealthStatus
}

func NewHealthMonitor(redis *redis.Client) *HealthMonitor {
    return &HealthMonitor{redis: redis}
}

func (m *HealthMonitor) Start(ctx context.Context) {
    // Subscribe to price updates
    pubsub := m.redis.Subscribe(ctx, "prices:btc:usd")
    
    go func() {
        for msg := range pubsub.Channel() {
            m.RecordPrice(time.Now())
        }
    }()
    
    // Periodic health evaluation
    ticker := time.NewTicker(1 * time.Second)
    go func() {
        defer ticker.Stop()
        for {
            select {
            case <-ctx.Done():
                return
            case <-ticker.C:
                m.evaluate()
            }
        }
    }()
}

func (m *HealthMonitor) RecordPrice(t time.Time) {
    m.lastUpdated.Store(t.UnixNano())
    m.status.Store(int32(Healthy))
}

func (m *HealthMonitor) evaluate() {
    last := time.Unix(0, m.lastUpdated.Load())
    age := time.Since(last)
    
    var status HealthStatus
    switch {
    case age > CriticalThreshold:
        status = Critical
        log.Error().
            Dur("stale_for", age).
            Str("status", "CRITICAL").
            Msg("PRICE_FEED_CRITICAL")
    case age > StaleThreshold:
        status = Degraded
        log.Warn().
            Dur("stale_for", age).
            Str("status", "DEGRADED").
            Msg("PRICE_FEED_DEGRADED")
    default:
        status = Healthy
    }
    
    m.status.Store(int32(status))
}

func (m *HealthMonitor) GetStatus() HealthStatus {
    return HealthStatus(m.status.Load())
}

func (m *HealthMonitor) IsHealthy() bool {
    return m.GetStatus() != Critical
}

func (m *HealthMonitor) GetLastUpdated() time.Time {
    return time.Unix(0, m.lastUpdated.Load())
}
```

---

### Task 2.2: Implement Circuit Breaker Middleware
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-008  
**Source C4:** API Server

**Description:**
Implement middleware that blocks bet placement when price feed is critical.

**DoD:**
- [ ] Middleware checks health status
- [ ] Returns 503 when Critical
- [ ] Includes Retry-After header
- [ ] Applies only to bet placement routes
- [ ] Recovery with 30s hysteresis (Hardening v1.2)

**Implementation:**
```go
// pkg/circuit/middleware.go
package circuit

import (
    "net/http"
    "time"
    
    "github.com/labstack/echo/v4"
)

const RecoveryHysteresis = 30 * time.Second

type CircuitBreaker struct {
    health      *HealthMonitor
    recoveredAt atomic.Int64 // Unix nano
}

func NewCircuitBreaker(health *HealthMonitor) *CircuitBreaker {
    return &CircuitBreaker{health: health}
}

func (cb *CircuitBreaker) Middleware() echo.MiddlewareFunc {
    return func(next echo.HandlerFunc) echo.HandlerFunc {
        return func(c echo.Context) error {
            status := cb.health.GetStatus()
            
            if status == Critical {
                return c.JSON(http.StatusServiceUnavailable, map[string]interface{}{
                    "error":        "PRICE_FEED_STALE",
                    "message":      "Betting temporarily suspended due to stale price data",
                    "retry_after":  10,
                })
            }
            
            // Hysteresis for recovery (Hardening v1.2)
            if status == Healthy {
                lastHealthy := cb.health.GetLastUpdated()
                recoveredAt := time.Unix(0, cb.recoveredAt.Load())
                
                if time.Since(lastHealthy) < RecoveryHysteresis && time.Since(recoveredAt) < RecoveryHysteresis {
                    // Still in recovery window
                    log.Info().Msg("circuit breaker: in recovery hysteresis")
                }
            }
            
            return next(c)
        }
    }
}
```

---

## Task 3: WebSocket Price Streaming

### Task 3.1: Implement Price Subscriber
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-004 (Redis Pub/Sub)  
**Source C4:** WebSocket Server

**Description:**
Subscribe to Redis price channel and broadcast to WebSocket clients.

**DoD:**
- [ ] Subscribe to `prices:btc:usd` channel
- [ ] Broadcast to all connected clients
- [ ] JSON message format
- [ ] Handle client connect/disconnect

**Implementation:**
```go
// cmd/ws/price.go
package main

import (
    "context"
    "encoding/json"
    
    "github.com/tap-trading/backend/pkg/redis"
)

type PriceUpdate struct {
    Symbol    string  `json:"symbol"`
    Price     float64 `json:"price"`
    Timestamp int64   `json:"timestamp"`
}

func (s *WebSocketServer) startPriceSubscriber() {
    ctx := context.Background()
    pubsub := s.redis.Subscribe(ctx, "prices:btc:usd")
    defer pubsub.Close()
    
    for msg := range pubsub.Channel() {
        price, _ := strconv.ParseFloat(msg.Payload, 64)
        
        update := PriceUpdate{
            Symbol:    "BTC/USD",
            Price:     price,
            Timestamp: time.Now().Unix(),
        }
        
        data, _ := json.Marshal(update)
        
        // Broadcast to all clients
        s.broadcast(data)
    }
}

func (s *WebSocketServer) broadcast(data []byte) {
    s.clientsMutex.RLock()
    defer s.clientsMutex.RUnlock()
    
    for client := range s.clients {
        select {
        case client.send <- data:
        default:
            // Client slow, drop message
            close(client.send)
            delete(s.clients, client)
        }
    }
}
```

---

### Task 3.2: System Status Broadcasting
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-008  
**Source C4:** WebSocket Server

**Description:**
Broadcast system status (price health) to clients for UI feedback.

**DoD:**
- [ ] Broadcast `system:status` message
- [ ] Include price health status
- [ ] Frontend disables grid when Critical

**Implementation:**
```go
type SystemStatus struct {
    PriceHealth string `json:"price_health"` // "healthy", "degraded", "critical"
    Timestamp   int64  `json:"timestamp"`
}

func (s *WebSocketServer) broadcastStatus() {
    ticker := time.NewTicker(5 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        status := SystemStatus{
            PriceHealth: s.health.GetStatus().String(),
            Timestamp:   time.Now().Unix(),
        }
        
        data, _ := json.Marshal(map[string]interface{}{
            "type":   "system:status",
            "data":   status,
        })
        
        s.broadcast(data)
    }
}
```

---

## Task 4: Price History Query

### Task 4.1: Implement Price History Service
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** PRD (Bet resolution needs price history)  
**Source C4:** Redis (price history)

**Description:**
Service to query price history for bet resolution.

**DoD:**
- [ ] Query prices by time range
- [ ] Check if price hit target range
- [ ] Handle gaps in price data

**Implementation:**
```go
// pkg/price/history.go
package price

import (
    "context"
    "strconv"
    "time"
    
    "github.com/tap-trading/backend/pkg/redis"
)

type HistoryService struct {
    redis *redis.Client
}

func (h *HistoryService) GetPricesInRange(ctx context.Context, from, to time.Time) ([]PricePoint, error) {
    scores, err := h.redis.ZRangeByScoreWithScores(ctx, "prices:btc:usd:ts", &redis.ZRangeBy{
        Min: fmt.Sprintf("%d", from.Unix()),
        Max: fmt.Sprintf("%d", to.Unix()),
    }).Result()
    
    if err != nil {
        return nil, err
    }
    
    points := make([]PricePoint, len(scores))
    for i, z := range scores {
        price, _ := strconv.ParseFloat(z.Member.(string), 64)
        points[i] = PricePoint{
            Price:     price,
            Timestamp: time.Unix(int64(z.Score), 0),
        }
    }
    
    return points, nil
}

func (h *HistoryService) DidPriceHitRange(ctx context.Context, from, to time.Time, minPrice, maxPrice float64) (bool, error) {
    prices, err := h.GetPricesInRange(ctx, from, to)
    if err != nil {
        return false, err
    }
    
    for _, p := range prices {
        if p.Price >= minPrice && p.Price <= maxPrice {
            return true, nil
        }
    }
    
    return false, nil
}
```

---

## Task 5: Testing

### Task 5.1: Circuit Breaker Tests
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-008  
**Source C4:** Circuit Breaker

**DoD:**
- [ ] Healthy ‚Üí Degraded ‚Üí Critical transition
- [ ] Critical blocks bet placement
- [ ] Recovery with hysteresis
- [ ] Stale threshold triggers correctly

---

### Task 5.2: Price Feed Resilience Tests
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-004  
**Source C4:** Price Ingestion

**DoD:**
- [ ] Reconnection after disconnect
- [ ] Price buffering during disconnect
- [ ] Graceful degradation with mock data

---

## Module Deliverables

- [ ] External price feed ingestion
- [ ] Redis pub/sub price distribution
- [ ] Price health monitoring
- [ ] Circuit breaker middleware
- [ ] WebSocket price streaming
- [ ] System status broadcasting
- [ ] Price history query service
- [ ] Circuit breaker tests

---

## Exit Criteria

This module is complete when:
1. Real-time prices flow from exchange ‚Üí Redis ‚Üí WebSocket ‚Üí Frontend
2. Circuit breaker blocks bets when price is stale >10s
3. Recovery requires 30s of stable prices (hysteresis)
4. Price history can be queried for bet resolution
5. System status is broadcast to all clients
6. Reconnection happens automatically after disconnect
----- END FILE: docs/implementation-plan/06-module-price-feed.md -----


----- BEGIN FILE: docs/implementation-plan/07-module-resolution-archival.md -----
# Module 5: Resolution & Archival (Vertical)

**Goal:** Implement bet resolution worker and automated weekly archival.

**Duration:** Days 8-9  
**Dependencies:** Module 1 (Core), Module 4 (Price Feed)  
**Priority:** MEDIUM (background processes)

---

## Module Scope

This module implements:
- Bet resolution worker (scans expired bets, determines win/loss)
- Payout processing with ledger entries
- Weekly archival to S3 (ADR-006)
- Data integrity checking

**Source ADRs:** ADR-006 (Archival Strategy), ADR-010 (Ledger for payouts)  
**Source C4:** Bet Resolver, Weekly Archiver, SQLite, S3

---

## Task 1: Bet Resolution

### Task 1.1: Implement Resolution Worker
**Owner:** Backend Engineer  
**Estimated:** 5 hours  
**Source ADR:** PRD (Bet resolution logic)  
**Source C4:** Bet Resolver container

**Description:**
Implement background worker that scans and resolves expired bets.

**DoD:**
- [ ] Poll for pending bets past their target time
- [ ] Query price history for resolution window
- [ ] Determine win/loss based on price range
- [ ] Update bet status and payout
- [ ] Create ledger entry for payouts
- [ ] Increment user balance for wins

**Implementation:**
```go
// cmd/resolver/resolver.go
package main

import (
    "context"
    "database/sql"
    "fmt"
    "time"
    
    "github.com/oklog/ulid/v2"
    "github.com/tap-trading/backend/pkg/db"
    "github.com/tap-trading/backend/pkg/price"
    "github.com/tap-trading/backend/pkg/redis"
)

const (
    PollInterval = 10 * time.Second
    BatchSize    = 100
)

type Resolver struct {
    db      *db.Client
    redis   *redis.Client
    price   *price.HistoryService
    health  *price.HealthMonitor
}

func (r *Resolver) Start(ctx context.Context) {
    ticker := time.NewTicker(PollInterval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            if err := r.resolveBatch(ctx); err != nil {
                log.Error().Err(err).Msg("resolution batch failed")
            }
        }
    }
}

func (r *Resolver) resolveBatch(ctx context.Context) error {
    // Query pending bets that have passed their target time
    // Note: This is simplified - actual query needs to consider cell time windows
    query := `
        SELECT id, user_id, cell_id, amount, reward_rate, placed_at
        FROM bets
        WHERE status = 'confirmed'
        AND placed_at < datetime('now', '-5 minutes')
        LIMIT ?
    `
    
    rows, err := r.db.QueryContext(ctx, query, BatchSize)
    if err != nil {
        return fmt.Errorf("query bets: %w", err)
    }
    defer rows.Close()
    
    var bets []models.Bet
    for rows.Next() {
        var bet models.Bet
        if err := rows.Scan(&bet.ID, &bet.UserID, &bet.CellID, &bet.Amount, &bet.RewardRate, &bet.PlacedAt); err != nil {
            continue
        }
        bets = append(bets, bet)
    }
    
    for _, bet := range bets {
        if err := r.resolveBet(ctx, bet); err != nil {
            log.Error().
                Err(err).
                Str("bet_id", bet.ID).
                Msg("resolve bet failed")
        }
    }
    
    return nil
}

func (r *Resolver) resolveBet(ctx context.Context, bet models.Bet) error {
    // Don't resolve if price feed is unhealthy
    if !r.health.IsHealthy() {
        return fmt.Errorf("price feed unhealthy, skipping resolution")
    }
    
    // Parse cell_id to get time window and price range
    // Cell format: "{time_col}_{min_price}_{max_price}"
    cellInfo, err := parseCellID(bet.CellID)
    if err != nil {
        return fmt.Errorf("parse cell: %w", err)
    }
    
    // Query price history for the cell's time window
    hit, err := r.price.DidPriceHitRange(
        ctx,
        cellInfo.TimeStart,
        cellInfo.TimeEnd,
        cellInfo.MinPrice,
        cellInfo.MaxPrice,
    )
    if err != nil {
        return fmt.Errorf("query price history: %w", err)
    }
    
    tx, err := r.db.BeginTx(ctx, &sql.TxOptions{
        Isolation: sql.LevelSerializable,
    })
    if err != nil {
        return fmt.Errorf("begin tx: %w", err)
    }
    defer tx.Rollback()
    
    now := time.Now()
    
    if hit {
        // WIN: Calculate payout and credit user
        payout := int64(float64(bet.Amount) * bet.RewardRate)
        
        // Update user balance
        _, err := tx.ExecContext(ctx, `
            UPDATE users 
            SET balance = balance + ?, updated_at = ?
            WHERE id = ?
        `, payout, now, bet.UserID)
        if err != nil {
            return fmt.Errorf("credit balance: %w", err)
        }
        
        // Get new balance for ledger
        var newBalance int64
        err = tx.QueryRowContext(ctx, `SELECT balance FROM users WHERE id = ?`, bet.UserID).Scan(&newBalance)
        if err != nil {
            return fmt.Errorf("get balance: %w", err)
        }
        
        // Create payout ledger entry
        ledgerID := ulid.Make().String()
        _, err = tx.ExecContext(ctx, `
            INSERT INTO transaction_ledger 
            (id, user_id, amount, balance_after, ref_type, ref_id, metadata, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        `, ledgerID, bet.UserID, payout, newBalance, "PAYOUT", bet.ID, nil, now)
        if err != nil {
            return fmt.Errorf("insert ledger: %w", err)
        }
        
        // Update bet status
        _, err = tx.ExecContext(ctx, `
            UPDATE bets 
            SET status = ?, resolved_at = ?, payout_amount = ?
            WHERE id = ?
        `, models.BetStatusWon, now, payout, bet.ID)
        if err != nil {
            return fmt.Errorf("update bet: %w", err)
        }
        
        log.Info().
            Str("bet_id", bet.ID).
            Str("user_id", bet.UserID).
            Int64("payout", payout).
            Msg("bet won")
    } else {
        // LOSS
        _, err := tx.ExecContext(ctx, `
            UPDATE bets 
            SET status = ?, resolved_at = ?
            WHERE id = ?
        `, models.BetStatusLost, now, bet.ID)
        if err != nil {
            return fmt.Errorf("update bet: %w", err)
        }
        
        log.Info().
            Str("bet_id", bet.ID).
            Str("user_id", bet.UserID).
            Msg("bet lost")
    }
    
    if err := tx.Commit(); err != nil {
        return fmt.Errorf("commit: %w", err)
    }
    
    // Update Redis cache (best effort)
    if hit {
        r.redis.IncrBy(ctx, fmt.Sprintf("balance:%s", bet.UserID), payout)
    }
    
    return nil
}

type CellInfo struct {
    TimeStart time.Time
    TimeEnd   time.Time
    MinPrice  float64
    MaxPrice  float64
}

func parseCellID(cellID string) (*CellInfo, error) {
    // Parse "{col}_{min}_{max}" format
    // Implementation depends on grid definition
    return nil, fmt.Errorf("not implemented")
}
```

---

### Task 1.2: Resolution Read Replica
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-006 (Historical queries)  
**Source C4:** SQLite (Historical)

**Description:**
Use read-only replica for historical bet queries during resolution.

**DoD:**
- [ ] Query historical bets from attached read-only DB
- [ ] Covering index on bets table
- [ ] Eliminate read-write contention

**Index:**
```sql
CREATE INDEX idx_bets_resolution ON bets(target_time, status) WHERE status = 'confirmed';
```

---

## Task 2: Weekly Archival (ADR-006)

### Task 2.1: Implement Archiver
**Owner:** Backend Engineer  
**Estimated:** 5 hours  
**Source ADR:** ADR-006 (Archival Strategy)  
**Source C4:** Weekly Archiver, S3

**Description:**
Implement automated weekly archival to S3 with compression.

**DoD:**
- [ ] Identify week to archive
- [ ] VACUUM INTO optimized copy
- [ ] zstd compress with -19 level
- [ ] Upload to S3 STANDARD_IA
- [ ] Verify upload
- [ ] Update metadata
- [ ] Attach as read-only
- [ ] Purge old local files (>4 weeks)

**Implementation:**
```go
// cmd/archiver/archiver.go
package main

import (
    "context"
    "fmt"
    "os"
    "os/exec"
    "path/filepath"
    "time"
    
    "github.com/aws/aws-sdk-go-v2/aws"
    "github.com/aws/aws-sdk-go-v2/config"
    "github.com/aws/aws-sdk-go-v2/service/s3"
)

const (
    DataDir     = "/data"
    ArchiveDir  = "/data/archives"
    S3Bucket    = "tap-trading-archives"
    S3Prefix    = "bets/"
    RetainWeeks = 4
)

type Archiver struct {
    s3Client *s3.Client
}

func (a *Archiver) Run(ctx context.Context, week string) error {
    // 1. Freeze: VACUUM INTO
    sourceDB := filepath.Join(DataDir, "bets_current.db")
    weekDB := filepath.Join(DataDir, fmt.Sprintf("bets_%s.db", week))
    
    log.Info().Str("week", week).Msg("starting archival")
    
    cmd := exec.CommandContext(ctx, "sqlite3", sourceDB, fmt.Sprintf("VACUUM INTO '%s'", weekDB))
    if err := cmd.Run(); err != nil {
        return fmt.Errorf("vacuum: %w", err)
    }
    
    // 2. Compress
    archiveFile := filepath.Join(ArchiveDir, fmt.Sprintf("bets_%s.db.zst", week))
    cmd = exec.CommandContext(ctx, "zstd", "-19", "-T4", "-o", archiveFile, weekDB)
    if err := cmd.Run(); err != nil {
        return fmt.Errorf("compress: %w", err)
    }
    
    // 3. Upload to S3
    file, err := os.Open(archiveFile)
    if err != nil {
        return fmt.Errorf("open archive: %w", err)
    }
    defer file.Close()
    
    stat, _ := file.Stat()
    key := fmt.Sprintf("%sbets_%s.db.zst", S3Prefix, week)
    
    _, err = a.s3Client.PutObject(ctx, &s3.PutObjectInput{
        Bucket:       aws.String(S3Bucket),
        Key:          aws.String(key),
        Body:         file,
        ContentLength: aws.Int64(stat.Size()),
        StorageClass: types.StorageClassStandardIa,
    })
    if err != nil {
        return fmt.Errorf("s3 upload: %w", err)
    }
    
    // 4. Verify
    head, err := a.s3Client.HeadObject(ctx, &s3.HeadObjectInput{
        Bucket: aws.String(S3Bucket),
        Key:    aws.String(key),
    })
    if err != nil {
        return fmt.Errorf("s3 verify: %w", err)
    }
    
    if *head.ContentLength != stat.Size() {
        return fmt.Errorf("upload size mismatch")
    }
    
    // 5. Update metadata
    if err := a.recordArchive(ctx, week, key, stat.Size()); err != nil {
        return fmt.Errorf("record metadata: %w", err)
    }
    
    // 6. Attach as read-only (via partition manager)
    // This would be done through the app's partition manager
    
    // 7. Cleanup old files
    if err := a.cleanupOldFiles(ctx); err != nil {
        log.Error().Err(err).Msg("cleanup failed")
    }
    
    log.Info().
        Str("week", week).
        Int64("size", stat.Size()).
        Msg("archival complete")
    
    return nil
}

func (a *Archiver) recordArchive(ctx context.Context, week, s3Path string, size int64) error {
    // Insert into metadata DB
    query := `
        INSERT INTO archives (week, s3_path, size_bytes, archived_at, status)
        VALUES (?, ?, ?, ?, ?)
    `
    _, err := a.db.ExecContext(ctx, query, week, s3Path, size, time.Now(), "complete")
    return err
}

func (a *Archiver) cleanupOldFiles(ctx context.Context) error {
    cutoff := time.Now().Add(-RetainWeeks * 7 * 24 * time.Hour)
    
    entries, err := os.ReadDir(ArchiveDir)
    if err != nil {
        return err
    }
    
    for _, entry := range entries {
        info, _ := entry.Info()
        if info.ModTime().Before(cutoff) {
            path := filepath.Join(ArchiveDir, entry.Name())
            if err := os.Remove(path); err != nil {
                log.Error().Err(err).Str("file", path).Msg("remove failed")
            } else {
                log.Info().Str("file", entry.Name()).Msg("purged old archive")
            }
        }
    }
    
    return nil
}
```

---

### Task 2.2: Cron Scheduling
**Owner:** Backend Engineer  
**Estimated:** 1 hour  
**Source ADR:** ADR-006  
**Source C4:** Weekly Archiver

**Description:**
Schedule weekly archival via cron.

**DoD:**
- [ ] Cron job at Sunday 00:00 UTC
- [ ] Script wrapper for archiver binary
- [ ] Dead man's switch metric

**Cron:**
```bash
# /etc/cron.d/tap-trading-archiver
0 0 * * 0 root /opt/app/scripts/archive-weekly.sh >> /var/log/archiver.log 2>&1
```

**Script:**
```bash
#!/bin/bash
# /opt/app/scripts/archive-weekly.sh

WEEK=$(date -d 'last week' +%Y_w%U)
/opt/app/bin/archiver -week="$WEEK"
```

---

## Task 3: Data Integrity

### Task 3.1: Implement Daily Integrity Check
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** ADR-010 (Financial auditability)  
**Source C4:** SQLite

**Description:**
Daily check that users.balance matches SUM(ledger.amount).

**DoD:**
- [ ] Query comparing balance to ledger sum
- [ ] Report any drift
- [ ] Auto-correct option (emergency only)
- [ ] Metric export for alerting

**Implementation:**
```go
// cmd/archiver/integrity.go
package main

import (
    "context"
    "database/sql"
)

func (a *Archiver) RunIntegrityCheck(ctx context.Context) (*IntegrityReport, error) {
    query := `
        WITH ledger_balances AS (
            SELECT 
                user_id,
                COALESCE(SUM(amount), 0) as ledger_sum
            FROM transaction_ledger
            GROUP BY user_id
        )
        SELECT 
            u.id,
            u.balance as user_balance,
            lb.ledger_sum,
            u.balance - lb.ledger_sum as drift
        FROM users u
        LEFT JOIN ledger_balances lb ON u.id = lb.user_id
        WHERE u.balance != lb.ledger_sum
           OR (lb.ledger_sum IS NULL AND u.balance != 0)
    `
    
    rows, err := a.db.QueryContext(ctx, query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var drifts []BalanceDrift
    for rows.Next() {
        var d BalanceDrift
        if err := rows.Scan(&d.UserID, &d.UserBalance, &d.LedgerSum, &d.Drift); err != nil {
            continue
        }
        drifts = append(drifts, d)
    }
    
    report := &IntegrityReport{
        CheckedAt: time.Now(),
        Drifts:    drifts,
        Valid:     len(drifts) == 0,
    }
    
    // Export metric
    metrics.RecordDriftCount(len(drifts))
    
    return report, nil
}

type BalanceDrift struct {
    UserID      string
    UserBalance int64
    LedgerSum   int64
    Drift       int64
}

type IntegrityReport struct {
    CheckedAt time.Time
    Drifts    []BalanceDrift
    Valid     bool
}
```

**Cron:**
```bash
# Daily at 02:00 UTC
0 2 * * * root /opt/app/bin/archiver -integrity-check >> /var/log/integrity.log 2>&1
```

---

## Task 4: Dead Man's Switch

### Task 4.1: Archive Health Metric
**Owner:** Backend Engineer  
**Estimated:** 1 hour  
**Source ADR:** ADR-006 (Operational monitoring)  
**Source C4:** Archiver

**Description:**
Export metric for last successful archive timestamp.

**DoD:**
- [ ] Gauge: `last_successful_archive_timestamp`
- [ ] Alert if >8 days old
- [ ] Prometheus metric export

**Implementation:**
```go
var (
    lastArchiveTimestamp = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "last_successful_archive_timestamp",
        Help: "Unix timestamp of last successful archive",
    })
)

func init() {
    prometheus.MustRegister(lastArchiveTimestamp)
}

func (a *Archiver) updateArchiveMetric() {
    lastArchiveTimestamp.SetToCurrentTime()
}
```

**Alert Rule:**
```yaml
- alert: ArchiveStale
  expr: time() - last_successful_archive_timestamp > 8 * 24 * 3600
  for: 1h
  labels:
    severity: critical
  annotations:
    summary: "Weekly archive has not run in 8+ days"
```

---

## Module Deliverables

- [ ] Bet resolution worker
- [ ] Win/loss determination with price history
- [ ] Payout processing with ledger entries
- [ ] Weekly archival to S3 (VACUUM, zstd, upload)
- [ ] Read-only attachment for historical data
- [ ] Daily integrity check (balance vs ledger)
- [ ] Dead man's switch metric for archival
- [ ] Cron scheduling for all jobs

---

## Exit Criteria

This module is complete when:
1. Bets are automatically resolved based on price history
2. Winners receive payouts with ledger entries
3. Weekly archival runs successfully end-to-end
4. Integrity check passes daily (zero drift)
5. Dead man's switch metric is exported
6. Historical queries work via read replica
7. Disk usage stays within 256GB (with archival)
----- END FILE: docs/implementation-plan/07-module-resolution-archival.md -----


----- BEGIN FILE: docs/implementation-plan/08-phase-hardening.md -----
# Phase 4: System-Level Hardening (Horizontal)

**Goal:** Validate emergent system behavior, failure modes, and operational readiness.

**Duration:** Days 9-10  
**Dependencies:** All Phase 3 modules complete  
**Priority:** CRITICAL (production readiness)

---

## Phase Scope

This phase validates:
- End-to-end workflows across modules
- Load testing at 3K RPS
- Failure mode testing (circuit breaker, reconnection, etc.)
- Operational readiness (monitoring, alerting, runbooks)

**Source ADRs:** All (system-wide validation)  
**Source C4:** Full system integration

---

## Task 1: Integration Testing

### Task 1.1: End-to-End Workflow Tests
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** All  
**Source C4:** Full system

**Description:**
Test complete user journeys through the system.

**DoD:**
- [ ] User registration ‚Üí deposit ‚Üí bet ‚Üí resolution ‚Üí withdrawal flow
- [ ] WebSocket price updates reach frontend
- [ ] Bet placement returns 202, confirms via WebSocket
- [ ] Circuit breaker blocks bets during stale price
- [ ] Recovery allows bets after 30s stable prices

**Test Scenarios:**
```go
// Test: Complete Bet Flow
func TestCompleteBetFlow(t *testing.T) {
    // 1. User authenticates
    // 2. Price feed streams to WebSocket
    // 3. User places bet
    // 4. Bet confirmed via WebSocket
    // 5. Wait for resolution time
    // 6. Bet resolved, balance updated
}

// Test: Circuit Breaker Flow
func TestCircuitBreakerFlow(t *testing.T) {
    // 1. Normal operation - bets accepted
    // 2. Stop price feed
    // 3. After 10s, bets rejected with 503
    // 4. Resume price feed
    // 5. After 30s hysteresis, bets accepted again
}
```

---

### Task 1.2: Failure Mode Testing
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-008 (Circuit Breaker)  
**Source C4:** All containers

**Description:**
Test system behavior under various failure conditions.

**DoD:**
- [ ] Redis failure: API degrades gracefully
- [ ] SQLite BUSY: Batcher retries correctly
- [ ] Price feed disconnect: Auto-reconnects
- [ ] Price feed stale: Circuit breaker activates
- [ ] Network partition: Timeouts handled correctly

**Failure Scenarios:**

| Scenario | Expected Behavior |
|----------|-------------------|
| Redis down | API returns 503, logs error |
| SQLite locked | Batcher retries with backoff |
| Price feed 500 | Reconnects with exponential backoff |
| Stale prices >10s | Circuit breaker blocks bets |
| WebSocket disconnect | Client reconnects with jitter |

---

## Task 2: Load Testing

### Task 2.1: 3K RPS Load Test
**Owner:** Backend Engineer  
**Estimated:** 4 hours  
**Source ADR:** ADR-005 (Write Batching)  
**Source C4:** Full system

**Description:**
Verify system sustains 3,000 requests per second.

**DoD:**
- [ ] 3,000 concurrent requests/second for 10 minutes
- [ ] p99 latency < 100ms
- [ ] p95 latency < 50ms
- [ ] Zero 5xx errors
- [ ] No SQLITE_BUSY errors
- [ ] Memory usage stable (no leaks)
- [ ] CPU usage < 80%

**Load Test Configuration:**
```yaml
# k6 or similar load test
declare
stages:
  - duration: 2m
    target: 1000  # Ramp up
  - duration: 5m
    target: 3000  # Sustained load
  - duration: 2m
    target: 1000  # Ramp down
  - duration: 1m
    target: 0

thresholds:
  http_req_duration{p(95)}: ['p(95)<50']
  http_req_duration{p(99)}: ['p(99)<100']
  http_req_failed: ['rate<0.001']  # <0.1% errors
```

**Monitoring During Test:**
```bash
# Watch key metrics
watch -n 1 'echo "=== SQLite ===" && sqlite3 /data/bets.db "PRAGMA wal_checkpoint;" && \
echo "=== Redis Memory ===" && redis-cli INFO memory | grep used_memory_human && \
echo "=== Disk ===" && df -h /data && \
echo "=== Load ===" && uptime'
```

---

### Task 2.2: Batcher Throughput Test
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-005  
**Source C4:** Bet Batcher

**Description:**
Verify batcher handles 3K RPS without queue buildup.

**DoD:**
- [ ] Channel utilization < 50% at 3K RPS
- [ ] Batch flush rate ~30/second
- [ ] No bets timeout waiting for result
- [ ] Channel backpressure triggers when full

---

## Task 3: Stress Testing

### Task 3.1: Spike Test
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All  
**Source C4:** Full system

**Description:**
Test system response to sudden traffic spikes.

**DoD:**
- [ ] 0 to 5K RPS in 10 seconds
- [ ] No crashes
- [ ] Graceful degradation (503 with retry-after)
- [ ] Recovery to normal latency after spike

---

### Task 3.2: Soak Test
**Owner:** Backend Engineer  
**Estimated:** 4 hours (automated)  
**Source ADR:** ADR-006 (Archival)  
**Source C4:** Full system

**Description:**
Run sustained load to catch resource leaks.

**DoD:**
- [ ] 1K RPS sustained for 4 hours
- [ ] Memory usage stable (no growth trend)
- [ ] File descriptors stable
- [ ] Goroutine count stable
- [ ] Disk space growth matches projection

---

## Task 4: Operational Readiness

### Task 4.1: Health Endpoint Verification
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All  
**Source C4:** All containers

**Description:**
Verify all health endpoints report correct status.

**DoD:**
- [ ] `GET /healthz` returns 200 when healthy
- [ ] Returns 503 when dependency unhealthy
- [ ] Includes sub-system status (DB, Redis, price feed)
- [ ] Circuit breaker state included

**Expected Response:**
```json
GET /healthz
{
  "status": "healthy",
  "timestamp": "2026-02-03T12:00:00Z",
  "checks": {
    "database": {
      "status": "healthy",
      "connections": 15,
      "wal_size_mb": 12
    },
    "redis": {
      "status": "healthy",
      "latency_ms": 0.5,
      "memory_used_mb": 45
    },
    "price_feed": {
      "status": "healthy",
      "last_update": "2026-02-03T11:59:59Z",
      "stale_for_ms": 120
    },
    "circuit_breaker": {
      "status": "closed"
    }
  }
}
```

---

### Task 4.2: Metrics and Alerting
**Owner:** Backend Engineer  
**Estimated:** 3 hours  
**Source ADR:** All  
**Source C4:** All containers

**Description:**
Set up Prometheus metrics and alerting rules.

**DoD:**
- [ ] Request rate, latency, error rate metrics
- [ ] Bet placement counter (by outcome)
- [ ] Resolution counter (by win/loss)
- [ ] Circuit breaker state gauge
- [ ] Price feed staleness gauge
- [ ] Archive timestamp gauge
- [ ] Alert rules documented

**Key Metrics:**
```go
// pkg/metrics/metrics.go
var (
    RequestsTotal = prometheus.NewCounterVec(prometheus.CounterOpts{
        Name: "http_requests_total",
        Help: "Total HTTP requests",
    }, []string{"method", "endpoint", "status"})
    
    RequestDuration = prometheus.NewHistogramVec(prometheus.HistogramOpts{
        Name:    "http_request_duration_seconds",
        Help:    "HTTP request duration",
        Buckets: prometheus.DefBuckets,
    }, []string{"method", "endpoint"})
    
    BetsPlacedTotal = prometheus.NewCounterVec(prometheus.CounterOpts{
        Name: "bets_placed_total",
        Help: "Total bets placed",
    }, []string{"status"}) // confirmed, rejected, timeout
    
    BetsResolvedTotal = prometheus.NewCounterVec(prometheus.CounterOpts{
        Name: "bets_resolved_total",
        Help: "Total bets resolved",
    }, []string{"outcome"}) // won, lost
    
    PriceFeedStaleness = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "price_feed_staleness_seconds",
        Help: "Seconds since last price update",
    })
    
    CircuitBreakerState = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "circuit_breaker_state",
        Help: "Circuit breaker state (0=closed, 1=open)",
    })
    
    LastArchiveTimestamp = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "last_successful_archive_timestamp",
        Help: "Unix timestamp of last archive",
    })
)
```

**Alert Rules:**
```yaml
groups:
  - name: tap-trading
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.01
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          
      - alert: HighLatency
        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "p99 latency > 100ms"
          
      - alert: PriceFeedStale
        expr: price_feed_staleness_seconds > 10
        for: 10s
        labels:
          severity: critical
        annotations:
          summary: "Price feed is stale"
          
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state == 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Circuit breaker is open"
          
      - alert: ArchiveStale
        expr: time() - last_successful_archive_timestamp > 8 * 24 * 3600
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Weekly archive has not run in 8+ days"
          
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/data"} / node_filesystem_size_bytes{mountpoint="/data"}) < 0.15
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disk space < 15%"
```

---

### Task 4.3: Runbook Documentation
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** All  
**Source C4:** Operations

**Description:**
Create operational runbooks for common scenarios.

**DoD:**
- [ ] Restart procedure for each service
- [ ] Database backup/restore procedure
- [ ] Circuit breaker manual override
- [ ] Price feed failover procedure
- [ ] Disk space emergency procedures

**Example Runbook:**
```markdown
# Runbook: Circuit Breaker Manual Override

## Scenario
Circuit breaker is stuck open due to price feed issues.

## Procedure
1. Check price feed status: `curl /healthz | jq .checks.price_feed`
2. Verify external feed is healthy
3. Manual override: `redis-cli PUBLISH circuit:override "close"`
4. Monitor for 5 minutes
5. Remove override: `redis-cli PUBLISH circuit:override "auto"`

## Rollback
Set back to auto mode if issues persist.
```

---

## Task 5: Security Validation

### Task 5.1: Replay Attack Test
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-007 (Replay Protection)  
**Source C4:** API Server

**Description:**
Verify replay protection is effective.

**DoD:**
- [ ] Replay same request with same nonce ‚Üí rejected
- [ ] Replay with old timestamp ‚Üí rejected
- [ ] Replay with future timestamp ‚Üí rejected
- [ ] Cross-chain replay ‚Üí rejected

---

### Task 5.2: Double-Spend Prevention Test
**Owner:** Backend Engineer  
**Estimated:** 2 hours  
**Source ADR:** ADR-009 (Conditional Deduction)  
**Source C4:** Bet Batcher

**Description:**
Verify double-spend is impossible under concurrent load.

**DoD:**
- [ ] 100 concurrent bets from same user
- [ ] Total spent never exceeds balance
- [ ] No negative balances in database
- [ ] Ledger balances match user balances

---

## Phase 4 Deliverables

- [ ] End-to-end workflow tests passing
- [ ] Failure mode tests passing
- [ ] 3K RPS load test passing (p99 < 100ms)
- [ ] Spike test passing
- [ ] Soak test passing (4 hours, no leaks)
- [ ] Health endpoint verified
- [ ] Prometheus metrics exported
- [ ] Alert rules configured
- [ ] Runbooks created
- [ ] Replay attack test passing
- [ ] Double-spend prevention verified

---

## Exit Criteria for Phase 4

Phase 4 (and the entire implementation) is complete when:

1. **Functional:** All end-to-end workflows complete successfully
2. **Performance:** 3,000 RPS sustained with p99 < 100ms
3. **Reliability:** All failure modes handled gracefully
4. **Security:** Replay attacks blocked, double-spend impossible
5. **Observability:** Metrics, logs, and alerts operational
6. **Operations:** Runbooks exist for common scenarios
7. **Data Integrity:** Daily integrity check passes
8. **Archival:** Weekly archival functional with dead man's switch

---

## Final Checklist

- [ ] SQLite WAL mode enabled
- [ ] Redis AOF + RDB persistence configured
- [ ] Nginx rate limiting active
- [ ] S3 bucket with STANDARD_IA lifecycle
- [ ] Circuit breaker thresholds configured
- [ ] Nonce TTL set to 90 seconds
- [ ] Circuit breaker hysteresis set to 30s
- [ ] Pending UX state implemented
- [ ] Redis balance cache rebuild on startup
- [ ] Transaction ledger table created
- [ ] Daily integrity check cron installed
- [ ] Archiver includes ledger table
- [ ] Dead man's switch metric alerting
----- END FILE: docs/implementation-plan/08-phase-hardening.md -----


----- BEGIN FILE: docs/implementation-plan/README.md -----
# Backend Implementation Plan

## Overview

This implementation plan translates the **BTC Tap Trading Application (Hardened v1.3)** architecture into an actionable, dependency-aware technical roadmap.

**Architecture Reference:**
- `prd.md` ‚Äî Product requirements
- `current_state.md` ‚Äî Current system state (Hardened v1.3)
- `docs/adr/` ‚Äî 10 Accepted ADRs
- `docs/diagrams/` ‚Äî C4 Context and Container diagrams

---

## Plan Structure

This plan follows the **hybrid horizontal-vertical execution model**:

### Phase 1: Foundation (Horizontal)
**File:** [`01-phase-foundation.md`](./01-phase-foundation.md)

Establish shared infrastructure and scaffolding:
- VM provisioning and system setup
- SQLite with WAL mode (ADR-001)
- Redis configuration (ADR-004)
- Go project scaffolding (ADR-002)
- CI/CD and deployment scripts
- Nginx reverse proxy
- Observability baseline

**Duration:** Days 1-2  
**Dependencies:** None

---

### Phase 2: Skeleton (Horizontal)
**File:** [`02-phase-skeleton.md`](./02-phase-skeleton.md)

Deploy minimal versions of all containers to validate connectivity:
- Minimal API server with health endpoint
- Minimal WebSocket server
- Minimal price ingestion
- Connectivity validation
- systemd service definitions

**Duration:** Days 2-3  
**Dependencies:** Phase 1

---

### Phase 3: Vertical Modules (Iterative)

Five independent vertical slices, prioritized by dependency order:

| Order | Module | File | Duration | Dependencies |
|-------|--------|------|----------|--------------|
| 1 | Core Infrastructure | [`03-module-core-infrastructure.md`](./03-module-core-infrastructure.md) | Days 3-4 | Phase 2 |
| 2 | Auth & Security | [`04-module-auth-security.md`](./04-module-auth-security.md) | Days 4-5 | Module 1 |
| 3 | Bet Ingestion | [`05-module-bet-ingestion.md`](./05-module-bet-ingestion.md) | Days 5-7 | Modules 1, 2 |
| 4 | Price Feed | [`06-module-price-feed.md`](./06-module-price-feed.md) | Days 7-8 | Module 1 |
| 5 | Resolution & Archival | [`07-module-resolution-archival.md`](./07-module-resolution-archival.md) | Days 8-9 | Modules 1, 4 |

---

### Phase 4: System Hardening (Horizontal)
**File:** [`08-phase-hardening.md`](./08-phase-hardening.md)

Validate emergent system behavior and operational readiness:
- End-to-end workflow testing
- Load testing (3K RPS target)
- Failure mode testing
- Stress testing (spike and soak)
- Operational readiness (metrics, alerts, runbooks)
- Security validation

**Duration:** Days 9-10  
**Dependencies:** All Phase 3 modules

---

## ADR to Module Mapping

| ADR | Title | Implemented In |
|-----|-------|----------------|
| 001 | SQLite as Primary Database | Phase 1, Module 1 |
| 002 | Go as Backend Language | Phase 1 |
| 003 | Monolithic Single-VM Deployment | Phase 1, 2, 4 |
| 004 | Redis for Cache and Pub/Sub | Phase 1, Modules 1, 4 |
| 005 | Write Batching for Bet Ingestion | Module 3 |
| 006 | Archival Strategy for Bet History | Module 5 |
| 007 | Replay Protection for Web3 Auth | Module 2 |
| 008 | Stale Price Circuit Breaker | Module 4 |
| 009 | Conditional Balance Deduction | Module 3 |
| 010 | Hybrid Transaction Ledger | Module 3 |

> **Note:** ADR-010 **extends** ADR-009 ‚Äî both must be implemented together in the same transaction.

---

## Critical Path

```
Phase 1 ‚îÄ‚îÄ‚ñ∫ Phase 2 ‚îÄ‚îÄ‚ñ∫ Module 1 ‚îÄ‚îÄ‚ñ∫ Module 2 ‚îÄ‚îÄ‚ñ∫ Module 3
                          ‚îÇ                       ‚ñ≤
                          ‚ñº                       ‚îÇ
                        Module 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
                        Module 5 ‚îÄ‚îÄ‚ñ∫ Phase 4
```

**Critical Path:** Phase 1 ‚Üí Phase 2 ‚Üí Module 1 ‚Üí Module 2 ‚Üí Module 3 ‚Üí Phase 4

Modules 4 and 5 can be worked in parallel with Module 3 once Module 1 is complete.

---

## Task Summary

| Phase/Module | Tasks | Est. Hours | Owner |
|--------------|-------|------------|-------|
| Phase 1: Foundation | 7 | 20 | Backend Engineer |
| Phase 2: Skeleton | 5 | 12 | Backend Engineer |
| Module 1: Core Infrastructure | 4 | 16 | Backend Engineer |
| Module 2: Auth & Security | 5 | 17 | Backend Engineer |
| Module 3: Bet Ingestion | 4 | 18 | Backend Engineer |
| Module 4: Price Feed | 5 | 14 | Backend Engineer |
| Module 5: Resolution & Archival | 4 | 14 | Backend Engineer |
| Phase 4: System Hardening | 5 | 20 | Backend Engineer |
| **Total** | **40** | **131** | |

---

## Key Implementation Notes

### ADR-009 + ADR-010 Coordination
The bet batcher must implement both conditional deduction AND hybrid ledger in the **same transaction**:

```sql
BEGIN;
  -- ADR-009: Conditional deduction
  UPDATE users SET balance = balance - ? WHERE id = ? AND balance >= ? RETURNING balance;
  
  -- ADR-010: Ledger entry
  INSERT INTO transaction_ledger (user_id, amount, balance_after, ref_type, ref_id) VALUES (...);
  
  -- Bet record
  INSERT INTO bets (...) VALUES (...);
COMMIT;
```

### Configuration Values (from Hardening)

| Parameter | Value | ADR |
|-----------|-------|-----|
| Nonce TTL | 90 seconds | ADR-007 (hardened) |
| Circuit breaker enter | 10s stale | ADR-008 |
| Circuit breaker recovery | 30s hysteresis | ADR-008 (hardened) |
| Batch size | 100 bets | ADR-005 |
| Batch timeout | 100ms | ADR-005 |
| Redis maxmemory | 512MB | ADR-004 |

### CI/CD Quality Gates

Per the architecture-to-implementation guidelines, all changes must pass:

| Gate | Checks | Failure Action |
|------|--------|----------------|
| **Build Gate** | `gofmt`, `go vet`, `golangci-lint`, `gosec` on critical paths | Block merge |
| **Test Gate** | Unit tests with race detector, 80% coverage on `pkg/batcher` and `pkg/auth` | Block merge |
| **Artifact Versioning** | Semantic version (v1.2.3-build+sha), version endpoint | Block deploy |

### Testing Requirements

Each module must include:
- Unit tests for domain logic
- Integration tests against real dependencies (SQLite, Redis)
- Module exit criteria verification

Phase 4 includes:
- End-to-end workflow tests
- 3K RPS load test
- Failure mode tests
- Security validation (replay, double-spend)

---

## Deliverables by Phase

### Phase 1
- [ ] Infrastructure provisioned
- [ ] Database configured (WAL mode)
- [ ] Redis running with persistence
- [ ] Go project builds successfully
- [ ] Nginx serving static files
- [ ] **CI/CD pipeline with build gates, test gates, and artifact versioning**

### Phase 2
- [ ] All containers deployable via systemd
- [ ] Health endpoints responding
- [ ] Connectivity verified end-to-end

### Phase 3
- [ ] 5 modules with tests passing
- [ ] Auth flow functional
- [ ] Bet placement with batching
- [ ] Price feed streaming
- [ ] Resolution worker running

### Phase 4
- [ ] 3K RPS sustained
- [ ] p99 latency < 100ms
- [ ] All failure modes tested
- [ ] Metrics and alerts operational
- [ ] Runbooks documented

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| SQLite contention at 3K RPS | Write batching (ADR-005), verified in Phase 4 |
| Double-spend race condition | Conditional SQL update (ADR-009), tested in Module 3 |
| Redis OOM | 90s nonce TTL (hardened), 512MB limit with LRU |
| Disk exhaustion | Weekly archival (ADR-006), integrity check |
| Stale price exploitation | Circuit breaker (ADR-008), 30s hysteresis |

---

## Navigation

- [Overview](./00-overview.md)
- [Phase 1: Foundation](./01-phase-foundation.md)
- [Phase 2: Skeleton](./02-phase-skeleton.md)
- [Module 1: Core Infrastructure](./03-module-core-infrastructure.md)
- [Module 2: Auth & Security](./04-module-auth-security.md)
- [Module 3: Bet Ingestion](./05-module-bet-ingestion.md)
- [Module 4: Price Feed](./06-module-price-feed.md)
- [Module 5: Resolution & Archival](./07-module-resolution-archival.md)
- [Phase 4: System Hardening](./08-phase-hardening.md)
----- END FILE: docs/implementation-plan/README.md -----


----- BEGIN FILE: IMPLEMENTATION_AGENTS.md -----
# Vibe System Implementation Rules

## Purpose

This file defines **mandatory rules** for any agent participating in the Vibe System implementation.

It exists to:
- prevent architectural drift
- enforce execution discipline
- ensure progress is traceable and restart-safe
- standardize agent behavior across phases

Failure to follow this file invalidates implementation work.

---

## 1. Mandatory Context Loading (Before Any Work)

Before executing any phase, the agent MUST:

1. Load architecture context using  
   `.agent/skills/architecture-context-loader`
2. Load the implementation execution model by reading:
   - `docs/implementation-plan/README.md`
   - `docs/implementation-plan/00-overview.md`

No code, tasks, or plans may be produced before this completes.

---

## 2. Required MCPs (Tooling)

The agent MUST identify and use appropriate MCPs when needed.

### Mandatory MCP Categories

- **Third-party / API Integration MCP**  
  For interacting with external services, infrastructure APIs, and system tools.

- **Context7 MCP**  
  For tasks requiring web search, specification lookup, or verification.

- **Dependency Knowledge MCPs**  
  Used to select correct libraries and versions:
  - `docs.rs` (Rust)
  - `pkg.go.dev` (Go)
  - ecosystem equivalents when applicable

Dependency selection MUST prioritize:
- stable releases
- active maintenance
- compatibility with ADR constraints

---

## 3. Mandatory Skills

The agent should look for skills that are relevant to the task before performing it. Examples:

- **Testing Skill**  
  Unit, integration, and end-to-end tests aligned with phase and module exit criteria.

- **Benchmarking Skill**  
  Performance tests on declared hot paths with recorded results.

- **Pull Request Writing Skill**  
  Pull requests MUST include:
  - scope summary
  - referenced ADRs
  - test and benchmark evidence

Skipping any required skill is considered a process failure.

---

## 4. Progress Logging and State Tracking (Non-Negotiable)

For every meaningful unit of work, the agent MUST:

- Append to  
  `progress-logs/YYYY-MM-DD-<phase-or-module>.md`
- Update  
  `implementation_state.md`

Rules:
- Logs are append-only
- Logs must reflect completed work, not intent
- `implementation_state.md` must always be consistent with logs
- Phases or modules may not be marked complete without evidence

Silent progress is considered invalid.

---

## 5. Phase Discipline

The agent MUST respect phase boundaries:

- Phase 0 / 0.5: context and knowledge preparation only
- Phase 1‚Äì4: execute strictly according to the implementation plan
- No phase skipping
- No scope leakage between phases

All work MUST reference the corresponding file in `docs/implementation-plan/`.

---

## 6. Phase 3 Execution Model (Vertical Modules)

During **Phase 3**, modules SHOULD be implemented using the **Ralph Wiggum Loop**:

----- END FILE: IMPLEMENTATION_AGENTS.md -----


----- BEGIN FILE: prd.md -----
# Product Requirements Document (PRD)

## 1. Functional Requirements

The product is a web-based tap trading application that streams the real-time price of BTC and allows users to place directional bets on future price movements using a grid-based interface.

The core interaction model is a 2D grid where:

* The x-axis represents future timestamps divided into discrete columns.
* The y-axis represents BTC price ranges divided into discrete rows.
* Each grid cell corresponds to a bounded price range at a specific future time window.

Users can place a bet by tapping on a single grid cell, subject to the following rules:

* A user may place at most one bet per grid cell.
* Users select a bet size using predefined amount presets.
* Cells that are in the past, in the current column, or within one column ahead of the current price column are not placeable.
* No secondary confirmation step is required; a valid tap immediately places the bet.

Bet resolution logic:

* A cell becomes eligible for resolution only after the market price has passed its associated time column.
* A bet is considered a win if, during or after the column is passed, the BTC price enters the cell‚Äôs defined price range (price >= lower bound AND price <= upper bound).
* If the bet wins, the user receives: bet size √ó reward rate of that cell.
* If the bet loses, the user loses 100% of the bet size.

Reward rates:

* Each cell has an associated reward rate.
* Reward rates are computed and provided by an internal R&D-owned algorithm.
* Reward rates may vary across cells in both the time and price dimensions.

Account and custody model:

* The system is fully custodial.
* Users register and log in via Web3 wallet signature (no password-based authentication).
* User balances, bets, and payouts are managed off-chain within the application backend.

Market data:

* The application streams real-time BTC price data.
* Price updates must propagate to the UI fast enough to maintain a coherent and responsive grid state.

## 2. Non-Functional Requirements

Availability:

* Target SLA: 99.5% uptime.
* Scheduled maintenance is acceptable during low-trading hours.

Performance and latency:

* End-to-end perceived latency per user interaction must be ‚â§ 100 ms.
* Tap interactions must be recognized instantly with no blocking confirmation dialogs.
* Price updates should feel real-time and visually continuous.

Scalability:

* The system must sustain approximately 3,000 requests per second at peak load.
* Graceful degradation is acceptable beyond this target, but correctness must be preserved.

Reliability and correctness:

* Bet placement must be atomic and idempotent.
* A bet must never be placed twice for the same user-cell pair.
* Bet resolution must be deterministic and reproducible from market price history.

Security:

* Web3 signature-based authentication must prevent replay attacks.
* Custodial balances must be protected against double-spend and race conditions.
* Server-side validation must enforce all betting constraints regardless of client behavior.

Observability:

* The system must expose basic metrics for request rate, latency, error rate, and bet resolution outcomes.
* Logging must be sufficient to reconstruct disputes around bet placement and resolution.

Data retention:

* Bet history must be retained indefinitely (forever).
* Market price data retained for the last 7 days only.

## 3. Constraints

Time-to-market:

* The initial production-ready version must be delivered within 2 weeks.

Infrastructure:

* Deployment is constrained to a single virtual machine with:

  * 4-core CPU (2.5 GHz)
  * 8 GB RAM
  * 256 GB SSD

Team capacity:

* Development resources are limited to:

  * 1 senior backend engineer
  * 1 senior frontend engineer

Technology scope implications:

* The architecture must be simple, operationally lightweight, and fast to implement.
* Complex distributed systems, multi-region deployments, or heavy data pipelines are out of scope for the initial release.

## 4. Explicit Non-Goals

The following are explicitly out of scope for this product phase:

* Ultra-low-latency engineering targeting ‚â§ 10 ms per user interaction.
* Scaling to extreme throughput levels such as 10,000+ RPS.
* Supporting assets other than BTC.
* On-chain settlement, non-custodial wallets, or smart contract‚Äìbased bet resolution.
* Advanced trading features such as hedging, cash-out before resolution, or secondary markets for bets.
* Mobile native applications (iOS/Android) in the initial phase.
----- END FILE: prd.md -----
